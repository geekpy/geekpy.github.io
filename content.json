{"meta":{"title":"Geekpy's Blog","subtitle":null,"description":null,"author":"geekpy","url":"http://yoursite.com"},"pages":[{"title":"Geekpy","date":"2018-08-12T03:45:00.000Z","updated":"2018-08-12T09:15:33.363Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"我是geekpy，后端开发工程师，精通Python, 和各种数据库，热衷于机器学习和人工智能的研究。喜欢阅读，喜欢深度思考，热衷于研究各种思维模型和复杂系统。非常希望跟志趣相投的人沟通交流。"},{"title":"tags","date":"2018-10-05T14:01:31.000Z","updated":"2018-10-05T14:02:53.698Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"史上最详解Python日期和时间处理（下）","slug":"python_time2","date":"2018-08-25T03:52:56.000Z","updated":"2018-10-05T14:28:36.106Z","comments":true,"path":"2018/08/25/python_time2/","link":"","permalink":"http://yoursite.com/2018/08/25/python_time2/","excerpt":"此下篇主要讲解跟时区相关的概念和程序中经常使用的场景，希望通过此文大家可以搞定所有时区相关的编程问题（如果还有不明白的地方，请联系我，我将进一步补充）。","text":"此下篇主要讲解跟时区相关的概念和程序中经常使用的场景，希望通过此文大家可以搞定所有时区相关的编程问题（如果还有不明白的地方，请联系我，我将进一步补充）。 本文的目录结构如下： 时区基本概念 时区 GMT和UTC 时区偏移(Offset) 夏令时(DST) 模糊时间(Ambiguous Time) 设置时区 tzinfo dateutil pytz 时区处理的最佳实践 时区基本概念 时区 由于地球自转导致不同地区的人看到太阳升起和落下的时间不同，于是人们就定义了时区的概念，将全球分为24个时区，其中位于英国的本初子午线作为零时区中线，然后向东划分出十二个时区（分别为+1， +2....+12)，向西也划分成十二个时区(分别为-1, -2 .....-12)。其中最早进入新的一天的是+12时区，当+12时区为中午12点时，正好零时区进入第二天（它们相差12小时，所以+12） GMT和UTC GMT(Greenwich Mean Time)，即格林尼治标准时间，也就是本初子午线所在的时区。UTC(Universal Time Coordinated)，即标准世界时间。GMT和UTC虽然表示的时间相同，但是两个是不同的概念，大家注意区分，实践过程中，我们通常使用UTC时间作为标准时间。 时区偏移Offset 时区偏移(Offset)是指所处时区时间相对于UTC时间的偏移量，比如中国的CST时间其偏移量就是+8，即相对于UTC时间需要+8小时。有些程序会使用秒或者分钟来替代小时，所以使用的偏移量计算时间的时候需要注意具体使用的时间单位。具体可以参考wiki时区偏移 夏令时(DST) 关于夏令时我觉得这篇文章已经讲解比较详细了, 大家可以直接参考，在此不再赘述。但是夏令时进一步增加了复杂度，这意味着即使同一个时区，一年中也会随着夏令时和非夏令时而导致offset的变化。 模糊时间(Ambiguous Time) 指的是在夏令时转换过程中的一段时间，在夏令时转换时，会有两个正确的时间，那么到底应该如何显示呢，所以要让程序知道到底选择哪个时间，就必须要有一个参数来确定这件事情。关于模糊时间的操作，Python2和Python3是不同的，具体可以参考Paul Ganssle的这篇文章pytz: The Fastest Footgun in the West 设置时区 tzinfo 在《上篇》中我们已经说过Python用于表示时间的对象会分为原始的(naive)和有知的(aware)两种，而要表示有知的时间，就必须给相应的对象传递tzinfo参数。tzinfo参数主要用在datetime.datetime对象和datetime.time对象，其类初始化函数定义如下： 12345# datetime.datetimeclass datetime.datetime(year, month, day[, hour[, minute[, second[, microsecond[, tzinfo]]]]])# datetime.timeclass datetime.time([hour[, minute[, second[, microsecond[, tzinfo]]]]]) 可以看到初始化datetime和time对象时，都有一个tzinfo参数，当我们传递一个tzinfo对象给这个参数的时候我们就可以初始化一个有知的时间对象。 那么这个tzinfo对象到底是怎么来的呢？ 先来看下Python官网的定义： This is an abstract base class, meaning that this class should not be instantiated directly. You need to derive a concrete subclass, and (at least) supply implementations of the standard tzinfo methods needed by the datetime methods you use. The datetime module does not supply any concrete subclasses of tzinfo. 从这段定义我们可以看出，tzinfo只是一个抽象类，而且官网已经明确说了不提供相应的实现，那我们怎么做呢？有两种做法：一是自己实现，Python官网还给出了示例代码，参考这里；另一种就是使用我们下边要讲到的dateutil模块。 dateutil 安装dateutil 1pip install python-dateutil 简介 官网对dateutil的介绍就一句话 The dateutil module provides powerful extensions to the standard datetime module, available in Python. 看到了吧，专门为拓展datetime而开发的，其中我们感兴趣的主要是如何构造时区tzinfo。 常见的使用场景： 1.转换为相应时区的时间 123456import dateutil.tz as tzfrom datetime import datetimemy_tz = tz.gettz('Asia/Shanghai')d = datetime(2018, 8, 20, tzinfo=my_tz)# datetime.datetime(2018, 8, 20, 0, 0, tzinfo=tzfile('/usr/share/zoneinfo/Asia/Shanghai')) 也就是说通过tz.gettz()我们可以得到一个tzinfo对象从而可以将其作为参数传递给datetime初始化函数。 通过这种方式，我们就可以表示一个本地时间了，比如我们获取了当前时间后想要表示成NewYork时间（换句话说就是要表示不同地方的当前时间），该怎么处理呢？ 先看正确的做法： 1234567NYC = tz.gettz('America/New_York')now_utc = datetime.now(tz.tzutc())# datetime.datetime(2018, 8, 25, 8, 15, 53, 143709, tzinfo=tzutc())now_utc.astimezone(NYC)# datetime.datetime(2018, 8, 25, 4, 15, 53, 143709, tzinfo=tzfile('/usr/share/zoneinfo/America/New_York')) 错误的做法： 1234now = datetime.utcnow()# datetime.datetime(2018, 8, 25, 8, 17, 21, 161843)now.astimezone(NYC) # datetime.datetime(2018, 8, 24, 20, 17, 21, 161843, tzinfo=tzfile('/usr/share/zoneinfo/America/New_York')) 为什么会出现这种情况呢，因为单纯的使用的utcnow()得到是一个原始naive time对象，而根据时区转换时，该时间会先从本地时区(+8 Shanghai)转换成UTC时区时间，然后再转化为NewYork时间，因此导致最终多减了8小时。 另外，有的时候我们获取到的是offset信息而不是时区信息，那么我们也可以将UTC时间转换成对应的当地时间，如下： 123# 通过tzoffset也可以构筑tzinfo对象now_utc.astimezone(tz.tzoffset('NewYork', -14400))# datetime.datetime(2018, 8, 25, 4, 15, 53, 143709, tzinfo=tzoffset('NewYork', -14400)) 2.获取当前的时区信息，并可以做相应转换 12345from dateutil.tz import tzlocaltz_local = tzlocal()type(tz_local)# dateutil.tz.tz.tzlocal 12d = datetime(2018, 8, 20, 9, 10, 5, tzinfo=tz_local)# datetime.datetime(2018, 8, 20, 9, 10, 5, tzinfo=tzlocal()) 12d.astimezone(tz.UTC)# datetime.datetime(2018, 8, 20, 1, 10, 5, tzinfo=tzutc()) pytz 在时间处理的时候，我们还经常能看到pytz这个库，这个库比较有意思的是，它与datetime的tzinfo并不完全兼容，很多时候它是独立的一套处理时间的库。 我们来看下如下代码(代码来自Paul Ganssle的文章，Paul是dateutil的核心开发者)： 12345678import pytzfrom datetime import datetime, timedeltaNYC = pytz.timezone('America/New_York')# 将timezone直接传入datetime初始化函数dt = datetime(2018, 2, 14, 12, tzinfo=NYC)print(dt)# 2018-02-14 12:00:00-04:56 可以看到实际的offset成了-04:56，这就不对了，正确使用pytz的姿势如下： 1234要使用localize方法来转化datetime对象dt = NYC.localize(datetime(2018, 2, 14, 12))print(dt)# 2018-02-14 12:00:00-05:00 再进行一些操作，如下 12345from datetime import timedeltadt_spring = dt + timedelta(days=60)print(dt_spring)# 2018-04-15 12:00:00-05:00 注意，这里的offset是-5:00，而考虑到夏令时，应当是-4:00，这是由于pytz在之前localize的时候就已经将offset设定好了，其在做其它运算之后也无法改变其offset，所以导致无法针对夏令时调整offset，所以针对pytz，每一次做类似timedelta的运算之后，都需要使用normalize函数进行调整，如下： 12print(NYC.normalize(dt_spring))# 2018-04-15 13:00:00-04:00 在项目实践除非对性能有极端要求，并不推荐使用pytz，毕竟不是每个人都熟悉这个库，项目协作过程中很难避免误用。关于pytz和dateutil的性能比较，可以参考Paul Ganssle的这篇文章pytz: The Fastest Footgun in the West 时区处理的最佳实践 所有中间步骤均使用UTC时间或者时间戳 所有中间的存储或者计算均应当使用UTC时间或者Timestamp，只有在最终显示的时候如果需要转换成本地时间，那么再将时间转换为特定时区的时间进行显示。 存储timezone信息，而不是offset 如果需要针对用户本地时区做时间转换，需要存储timezone的信息，如timezone名称，而不是offset。这是由于有些地区可能有夏令时，offset会改变，所以最好是存时区名称之类的信息，这样通过tzinfo会自动进行调整。 Reference time module document datetime module document pytz dateutil 10 things you need to know about Date and Time in Python 时区和夏令时相关的基本知识 pytz: The Fastest Footgun in the West StackOverflow","categories":[],"tags":[]},{"title":"史上最详解Python日期和时间处理（上）","slug":"python_time","date":"2018-08-12T03:52:56.000Z","updated":"2018-10-05T14:28:49.266Z","comments":true,"path":"2018/08/12/python_time/","link":"","permalink":"http://yoursite.com/2018/08/12/python_time/","excerpt":"好吧，我承认这有标题党的嫌疑，不过看了那么多文章，的确没有找到一篇让我满意的关于日期和时间处理的详解文章，于是决心自己动手亲写一篇，希望能对得起这个霸气的标题。言归正传，在Python编程中，日期和时间处理是非常繁琐的一块，不仅概念众多，且有很多不同的module, 尤其涉及时区处理的时候会将问题进一步复杂化。本文将对Python在日期和时间处理上进行一步步详细讲解，总共会分为上下两篇，其中时区处理是较为棘手的一块内容，单独拿出来作为下篇。此上篇将主要讲解时间和日期处理中的基本概念，和常用的场景。","text":"好吧，我承认这有标题党的嫌疑，不过看了那么多文章，的确没有找到一篇让我满意的关于日期和时间处理的详解文章，于是决心自己动手亲写一篇，希望能对得起这个霸气的标题。言归正传，在Python编程中，日期和时间处理是非常繁琐的一块，不仅概念众多，且有很多不同的module, 尤其涉及时区处理的时候会将问题进一步复杂化。本文将对Python在日期和时间处理上进行一步步详细讲解，总共会分为上下两篇，其中时区处理是较为棘手的一块内容，单独拿出来作为下篇。此上篇将主要讲解时间和日期处理中的基本概念，和常用的场景。 本篇目录如下： 时间和日期的表示 时间、日期对象 时间戳 日期时间字符串 日期时间处理场景 获取当前时间 获取特定时间 不同时间表示的转化 时间和日期的表示 在Python中表示日期和时间，最基本的有三种形式： 时间、日期对象(Object) 时间戳(Timestamp) 字符串(String) 在这三种基本形式中又会衍生出一些其它的概念，下面来分别讲解下： 先看张脑图来理清它们之间的关系 时间、日期对象 Python中用于表示日期和时间的对象有很多种，在详细讲解这些对象之前先要明确一个概念，即这些对象都分为两种：一种是&quot;原始的(naive)&quot;，另一种是&quot;有知的(aware)&quot;。区别在于原始的时间没有时区概念，只是单纯的表示一个日期和时间，而有知的时间会包含一些额外信息，如时区，是否夏令时等信息，而这些信息会通过tzinfo子类来进行封装，我们将在（下）中详细讲解。 datetime对象：这个是最常用的日期时间对象，可以即表示日期又表示时间。datetime对象可以直接用于日期和时间相关的计算（比如计算5天之前的时间）。另外，datetime既可以表示原始时间，也可以表示有知时间，如果要表示有知时间，则在初始化时需要传递tzinfo类型的参数。 12345678In [16]: import datetimeIn [17]: datetime.datetime.now() # 返回一个datetime对象Out[17]: datetime.datetime(2018, 7, 28, 11, 17, 38, 972555)# 也可以直接初始化一个datetime对象In [20]: datetime.datetime(2018, 7, 5, 10, 20)Out[20]: datetime.datetime(2018, 7, 5, 10, 20) time对象：与datetime类似，但只用于表示时间，不表示日期 123# 这里初始化一个datetime.time对象用于表示时间11点28分05秒In [19]: datetime.time(11, 28, 5)Out[19]: datetime.time(11, 28, 5) date对象：与datetime类似，但只表示日期，不表示时间 12In [23]: datetime.date(2018, 07, 10)Out[23]: datetime.date(2018, 7, 10) time tuple: time tuple又叫struct time，是一种用于表示日期和时间的数据结构，此数据结构主要用于time module中的相关函数。time tuple中有9个元素，如下表所示： 索引(Index) 属性(Attribute) 值(Values) 0 tm_year(年) 比如2011 1 tm_mon(月) 1 - 12 2 tm_mday(日) 1 - 31 3 tm_hour(时) 0 - 23 4 tm_min(分) 0 - 59 5 tm_sec(秒) 0 - 61 6 tm_wday(weekday) 0 - 6(0表示周日) 7 tm_yday(一年中的第几天) 1 - 366 8 tm_isdst(是否是夏令时) 默认为-1 1234In [21]: import timeIn [22]: time.localtime() # 返回一个time tuple用于表示当前时间的local timeOut[22]: time.struct_time(tm_year=2018, tm_mon=7, tm_mday=28, tm_hour=11, tm_min=26, tm_sec=19, tm_wday=5, tm_yday=209, tm_isdst=0) 总结：以上前三个对象(datetime, date, time)均来自datetime module（注意区分datetime module和datetime 对象），而最后一个time tuple主要用于time module中的相关操作。 时间戳 时间戳(Timestamp)是表示当前时间距离元年时间(epoch, 1970年1月1日00:00:00 UTC)的偏移量，这个偏移量在Python中用秒数来计算，但有些编程语言如JavaScript是用毫秒来计算的，需要注意。**另外需要知道时间戳没有时区概念，是不分时区的。**这就是为什么我们在数据库中通常存储的是时间戳，当需要向用户显示时间的时候，再转化为对应时区的时间。 123# 获取当前时间的时间戳In [24]: time.time()Out[24]: 1532750668.210267 日期时间字符串 通常在需要向用户展示时间的时候，我们都需要将时间戳或者时间对象转化为字符串形式，从而在标准输出中能够打印出相应的时间。我们可以通过将时间戳或者对象进行转化和格式化来得到相应的字符串。需要说明的是，字符串表示时间有一个标准形式称为ISO8601，可以通过专门的函数来获得。 123456789In [29]: dt = datetime.datetime.now()# 获取ISO8601标准时间字符串In [30]: dt.isoformat()Out[30]: '2018-07-28T12:11:33.582380'# 自定义格式的时间字符串In [31]: dt.strftime(\"%Y%m%d-%H:%M:%S\")Out[31]: '20180728-12:11:33' 日期时间处理场景 时间处理主要有以下几个场景： 获取当前时间 获取特定时间：例如获取5天前的时间。 不同时间表示的转化：例如将datetime对象转化为时间字符串 时区处理：这个将在《下篇》中详细讲述 下面分别介绍下这几种处理场景： 获取当前时间 获取当前时间可以通过获取datetime对象、时间戳或time tuple三种方式来获取。 1234567891011121314151617181920# 最常用的获取当前日期时间的方法，其有一个tz参数用于设置时区，默认为None，所以最终获取的是一个原始naive时间In [36]: dt = datetime.datetime.now()# 注意获取的是当地时间的时间表示，但其本身是一个原始naive时间In [37]: dtOut[37]: datetime.datetime(2018, 7, 28, 12, 58, 57, 676468)# 获取当前时间的UTC时间表示，注意这仍然是一个原始naive时间，也就是用当前的UTC时间4点59分32秒来构造一个naive时间对象，但这个对象并没有包含时区信息In [38]: utc_now = datetime.datetime.utcnow()In [39]: utc_nowOut[39]: datetime.datetime(2018, 7, 28, 4, 59, 32, 258087)# 获取当前时间的时间戳In [40]: time.time()Out[40]: 1532753948.563503# 获取当地时间的time tupleIn [41]: time.localtime()Out[41]: time.struct_time(tm_year=2018, tm_mon=7, tm_mday=28, tm_hour=13, tm_min=0, tm_sec=2, tm_wday=5, tm_yday=209, tm_isdst=0) 获取特定时间 1234567891011121314In [42]: dtOut[42]: datetime.datetime(2018, 7, 28, 12, 58, 57, 676468)# 获取两天前的时间In [43]: dt - datetime.timedelta(days=2)Out[43]: datetime.datetime(2018, 7, 26, 12, 58, 57, 676468)# 获取一周前的时间In [44]: dt - datetime.timedelta(weeks=1)Out[44]: datetime.datetime(2018, 7, 21, 12, 58, 57, 676468)# 获取3小时前的时间In [45]: dt - datetime.timedelta(hours=3)Out[45]: datetime.datetime(2018, 7, 28, 9, 58, 57, 676468) 通过timedelta基本可以满足我们获取特定时间的需求，但是这里不涉及时区相关的转化。而且只是原始naive时间之间的转化 不同时间表示的转化 先来看一张关系图： 从上图可以看出总共有5对关系，每对关系都是可以双向转化的(除timetuple无法直接转化为datetime，需要先转化为timestamp或者time string)，所以总共有9种转化，下面将分别介绍下它们之间是如何互相转化的。 datetime object &amp; timestamp 从时间戳timestamp ==&gt; datetime object 1234567891011121314151617181920In [2]: import timeIn [3]: from datetime import datetimeIn [4]: ts = time.time()# 直接通过fromtimestamp就可以获得时间戳对应的datetime对象In [5]: dt = datetime.fromtimestamp(ts)In [6]: tsOut[6]: 1532775234.192805# 注意这个datetime对象获取的是本地时间的表示，但是仍然是原始naive timeIn [7]: dtOut[7]: datetime.datetime(2018, 7, 28, 18, 53, 54, 192805)# 获取对应utc的时间表示，仍然是原始naive时间In [11]: utc_dt = datetime.utcfromtimestamp(ts)In [12]: utc_dtOut[12]: datetime.datetime(2018, 7, 28, 10, 53, 54, 192805) 从datetime object ==&gt;时间戳timestamp 12345678910111213141516171819# 定义一个函数用于返回datetime object对应的时间戳In [8]: def get_timestamp(datetime_obj): ...: if not isinstance(datetime_obj, datetime): ...: raise ValueError() ...: return (datetime_obj - datetime(1970, 1, 1)).total_seconds() ...:In [9]: get_timestamp(dt)Out[9]: 1532804034.192805# 也可以先转化为time tuple，再通过mktime函数转化为timestamp，但是精度不如上一版高In [13]: def get_timestamp_v2(datetime_obj): ...: if not isinstance(datetime_obj, datetime): ...: raise ValueError() ...: return time.mktime(datetime_obj.timetuple()) ...:In [14]: get_timestamp_v2(dt)Out[14]: 1532775234.0 datetime object &amp; time string datetime object ==&gt; time string 关于格式化的说明可以参考官网文档 12345678910In [15]: dtOut[15]: datetime.datetime(2018, 7, 28, 18, 53, 54, 192805)# 自定义格式In [16]: dt.strftime(\"%Y-%m-%dT%H:%M:%S\")Out[16]: '2018-07-28T18:53:54'# 转化为ISO8601格式字符串的快捷方式In [17]: dt.isoformat()Out[17]: '2018-07-28T18:53:54.192805' time string ==&gt; datetime object 123456In [18]: time_str = '2018-07-28T18:53:54'In [19]: dt2 = datetime.strptime(time_str, \"%Y-%m-%dT%H:%M:%S\")In [20]: dt2Out[20]: datetime.datetime(2018, 7, 28, 18, 53, 54) datetime object &amp; time tuple datetime object ==&gt; time tuple 12In [21]: dt.timetuple()Out[21]: time.struct_time(tm_year=2018, tm_mon=7, tm_mday=28, tm_hour=18, tm_min=53, tm_sec=54, tm_wday=5, tm_yday=209, tm_isdst=-1) time tuple ==&gt; datetime object 无法直接转化，需要先转化为time str或者timestamp time tuple &amp; timestamp time tuple ==&gt; timestamp 12345678In [25]: time_tupleOut[25]: time.struct_time(tm_year=2018, tm_mon=7, tm_mday=28, tm_hour=18, tm_min=53, tm_sec=54, tm_wday=5, tm_yday=209, tm_isdst=-1)# 注意：该函数精度只能到秒In [26]: ts = time.mktime(time_tuple)In [27]: tsOut[27]: 1532775234.0 timestamp ==&gt; time tuple 1234567891011121314In [27]: tsOut[27]: 1532775234.0# 转为本地时间的time tuple表示，In [29]: tt = time.localtime(ts)In [30]: ttOut[30]: time.struct_time(tm_year=2018, tm_mon=7, tm_mday=28, tm_hour=18, tm_min=53, tm_sec=54, tm_wday=5, tm_yday=209, tm_isdst=0)# 转为UTC时间的time tuple表示In [31]: utc_tt = time.gmtime(ts)In [32]: utc_ttOut[32]: time.struct_time(tm_year=2018, tm_mon=7, tm_mday=28, tm_hour=10, tm_min=53, tm_sec=54, tm_wday=5, tm_yday=209, tm_isdst=0) time tuple &amp; time string time tuple ==&gt; time string 123456789101112131415In [33]: ttOut[33]: time.struct_time(tm_year=2018, tm_mon=7, tm_mday=28, tm_hour=18, tm_min=53, tm_sec=54, tm_wday=5, tm_yday=209, tm_isdst=0)# 注意：这里strftime并非time tuple的方法，而是time module下的函数In [34]: tt.strftime(\"%Y-%m-%dT%H:%M:%S\")---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-34-1be0f512fc77&gt; in &lt;module&gt;()----&gt; 1 tt.strftime(\"%Y-%m-%dT%H:%M:%S\")AttributeError: 'time.struct_time' object has no attribute 'strftime'# 如下是正解In [35]: time.strftime(\"%Y-%m-%dT%H:%M:%S\", tt)Out[35]: '2018-07-28T18:53:54' time string ==&gt; time tuple 1234567In [36]: time_strOut[36]: '2018-07-28T18:53:54'In [37]: tt = time.strptime(time_str, \"%Y-%m-%dT%H:%M:%S\")In [38]: ttOut[38]: time.struct_time(tm_year=2018, tm_mon=7, tm_mday=28, tm_hour=18, tm_min=53, tm_sec=54, tm_wday=5, tm_yday=209, tm_isdst=-1) timestamp &amp; time string 无法相互转化，只能通过先转化为datetime object或者time tuple之后才能再转化 References time module document datetime module document 10 things you need to know about Date and Time in Python PYTHON-基础-时间日期处理小结","categories":[],"tags":[]},{"title":"Redis数据类型和常用命令","slug":"Redis数据类型和常用命令","date":"2018-07-27T10:18:18.000Z","updated":"2018-10-05T14:16:23.961Z","comments":true,"path":"2018/07/27/Redis数据类型和常用命令/","link":"","permalink":"http://yoursite.com/2018/07/27/Redis数据类型和常用命令/","excerpt":"","text":"Redis相较于其它的数据库虽然简单，但是要熟记所有命令的用法也并非易事。一个简单的技巧是通过要操作的数据类型来将这些命令进行结构化。 &lt;!--more--&gt; 数据类型和对应命令 所有存储于redis中的数据都对应于一个键值对(key-value pair), key可以是任意二进制序列，通常我们使用字符串来标记一个特定的key。在redis中我们通常称这个key为name或者就叫key, 而对于value，redis支持如下几种类型： strings lists: list内容只能是string sets: set中存储非重复的string sorted sets: 与sets类似，但是每个string都会对应一个float类型的score，从而用于排序 hashes: 键值对hash类型，也就是Python中的dict，注意在redis中最外层的key一般叫做name或者key，而value中数据类型如果是dict，那么这个dict中的key通常被称为field。 Bit arrays (or simply bitmaps): 实际存储的仍然是string，但是可以针对bit进行操作 HyperLogLogs: 用于估计unique value的数量 针对不同的数据类型，会有不同的命令，通过如下脑图可以更加清晰地记忆redis的命令 strings 12345678910111213141516171819127.0.0.1:6379&gt; set strtest xyzOK127.0.0.1:6379&gt; get strtest\"xyz\"127.0.0.1:6379&gt; mset a 1 b 2 c 3OK127.0.0.1:6379&gt; mget a b c1) \"1\"2) \"2\"3) \"3\"# 注意以下增减操作只能针对整数数字(虽然类型仍然是string类型)127.0.0.1:6379&gt; incr a(integer) 2127.0.0.1:6379&gt; incrby b 5(integer) 7127.0.0.1:6379&gt; decr b(integer) 6127.0.0.1:6379&gt; decrby b 3(integer) 3 lists lists类型中存储的仍然是string类型 12345678910111213141516171819202122232425262728293031323334353637# left push用于从左将item压入到list当中127.0.0.1:6379&gt; lpush list_test 1 2 3(integer) 3# 注意如果想看list中的内容，无法通过get直接去看，get只是针对string，而必须使用lrange127.0.0.1:6379&gt; get list_test (error) WRONGTYPE Operation against a key holding the wrong kind of value# 这里0 -1均为list index，表示从index 0 开始到-1结束，-1即从右数最后一个item127.0.0.1:6379&gt; lrange list_test 0 -11) \"3\"2) \"2\"3) \"1\"127.0.0.1:6379&gt; rpush list_test 5 7(integer) 5127.0.0.1:6379&gt; lrange list_test 0 -11) \"3\"2) \"2\"3) \"1\"4) \"5\"5) \"7\"127.0.0.1:6379&gt; lpop list_test\"3\"127.0.0.1:6379&gt; lrange list_test 0 -11) \"2\"2) \"1\"3) \"5\"4) \"7\"# 从左trim截断list，以下是截取index 0 到index 2 的item作为新的list127.0.0.1:6379&gt; ltrim list_test 0 2OK127.0.0.1:6379&gt; lrange list_test 0 -11) \"2\"2) \"1\"3) \"5\" hashes (dict) 哈希类型，在python中也就是dict类型。这也是非常常用的数据类型。 12345678910111213141516171819202122232425127.0.0.1:6379&gt; hset htest a 1(integer) 1127.0.0.1:6379&gt; hget htest a\"1\"127.0.0.1:6379&gt; hmset htest a 1 b 2 c 3OK127.0.0.1:6379&gt; hmget htest a b c1) \"1\"2) \"2\"3) \"3\"127.0.0.1:6379&gt; hgetall htest1) \"a\"2) \"1\"3) \"b\"4) \"2\"5) \"c\"6) \"3\"127.0.0.1:6379&gt; hkeys htest1) \"a\"2) \"b\"3) \"c\"127.0.0.1:6379&gt; hvals htest1) \"1\"2) \"2\"3) \"3\" 在python程序中使用redis-py driver的时候，通过dict进行操作会非常清晰和简单。 12345678910111213In [1]: import redisIn [2]: r = redis.StrictRedis(host='localhost', port=6379, db=0)In [3]: d = &#123;\"a\": 2, \"b\": 3&#125;In [4]: key = \"test:2\"In [5]: r.hmset(key, d)Out[5]: TrueIn [6]: r.hgetall(key)Out[6]: &#123;'a': '2', 'b': '3'&#125; sets 12345678910111213141516171819202122232425262728293031323334353637383940127.0.0.1:6379&gt; sadd set_test a b 33(integer) 3127.0.0.1:6379&gt; sadd set_test c a b 22(integer) 1# 可以看到不会有重复的item127.0.0.1:6379&gt; smembers set_test1) \"c\"2) \"33\"3) \"a\"4) \"b\"# 用于测试set中是否包含指定的item，如有则返回1，没有返回0127.0.0.1:6379&gt; sismember set_test a(integer) 1127.0.0.1:6379&gt; sismember set_test xx(integer) 0127.0.0.1:6379&gt; sadd set_test2 a b 56 66(integer) 4127.0.0.1:6379&gt; smembers set_test21) \"56\"2) \"a\"3) \"66\"4) \"b\"# 求交集127.0.0.1:6379&gt; sinter set_test set_test21) \"a\"2) \"b\"# 求并集127.0.0.1:6379&gt; sunion set_test set_test21) \"33\"2) \"a\"3) \"56\"4) \"c\"5) \"66\"6) \"22\"7) \"b\" sorted sets sorted sets与sets类似，可以保证item不重复，区别在于sorted sets中每个item对应一个float类型的score 123456789101112131415161718192021222324127.0.0.1:6379&gt; zadd sort_set 2.2 a(integer) 1127.0.0.1:6379&gt; zadd sort_set 2 bb(integer) 1127.0.0.1:6379&gt; zadd sort_set 10 x(integer) 1# 获取index 从0 到-1的(即所有) items127.0.0.1:6379&gt; zrange sort_set 0 -11) \"bb\"2) \"a\"3) \"x\"# 获取item bb对应的index127.0.0.1:6379&gt; zrank sort_set bb(integer) 0127.0.0.1:6379&gt; zrank sort_set x(integer) 2127.0.0.1:6379&gt; zscore sort_set x\"10\"# 用于获取对应score set中item的数量127.0.0.1:6379&gt; zcard sort_set(integer) 3 bit arrays 用于针对指定的key设置位数据为0 或 1。当我们对存储有较高要求，且对于统计为1的item的数量时，使用bit array是一个好的办法。 123456789101112131415# 针对bit 7进行设置，设置为1，返回该位之前存储的值127.0.0.1:6379&gt; setbit bit_test 7 1(integer) 0127.0.0.1:6379&gt; setbit bit_test 7 0(integer) 1127.0.0.1:6379&gt; get bit_test\"\\x00\"127.0.0.1:6379&gt; setbit bit_test 8 1(integer) 0127.0.0.1:6379&gt; setbit bit_test 9 1(integer) 0# 统计有多少位为1127.0.0.1:6379&gt; bitcount bit_test(integer) 2 HyperLogLogs redis实现了相应算法可以估计hyperloglog中存储的所有item中非重复的item的数量 1234127.0.0.1:6379&gt; pfadd loglog 1 3 5 7 1(integer) 1127.0.0.1:6379&gt; pfcount loglog(integer) 4 通用的命令 keys pattern: pattern可以为glob风格的通配符格式，最常用的是keys *查询所有的keys exists key: 查询该key是否存在 del key: 删除该key对应的数据 type key: 查询该key对应的value的数据类型 expire key: 定义多长时间后key对应的数据过期，过期后数据会被自动删除 ttl key: 查询该key对应的剩余存活时间 flushdb/flushall: flushdb用于清除当前db的所有数据，flushall清除所有数据库的数据 References Redis Type Introduction Redis Commands Redis 常用命令","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"Pandas系列6-DataFrame的分组与聚合","slug":"Pandas系列6-DataFrame的分组与聚合","date":"2018-07-23T10:18:18.000Z","updated":"2018-08-12T10:06:16.879Z","comments":true,"path":"2018/07/23/Pandas系列6-DataFrame的分组与聚合/","link":"","permalink":"http://yoursite.com/2018/07/23/Pandas系列6-DataFrame的分组与聚合/","excerpt":"","text":"在对数据进行处理的时候，分组与聚合是非常常用的操作。在Pandas中此类操作主要是通过groupby函数来完成的。 &lt;!--more--&gt; 先看一个实际的例子： 123456789101112131415161718192021222324252627282930# 生成一个原始的DataFrameIn [70]: raw_data = &#123;&apos;regiment&apos;: [&apos;Nighthawks&apos;, &apos;Nighthawks&apos;, &apos;Nighthawks&apos;, &apos;Nighthawk ...: s&apos;, &apos;Dragoons&apos;, &apos;Dragoons&apos;, &apos;Dragoons&apos;, &apos;Dragoons&apos;, &apos;Scouts&apos;, &apos;Scouts&apos;, &apos;Scou ...: ts&apos;, &apos;Scouts&apos;], ...: &apos;company&apos;: [&apos;1st&apos;, &apos;1st&apos;, &apos;2nd&apos;, &apos;2nd&apos;, &apos;1st&apos;, &apos;1st&apos;, &apos;2nd&apos;, &apos;2nd&apos;,&apos;1 ...: st&apos;, &apos;1st&apos;, &apos;2nd&apos;, &apos;2nd&apos;], ...: &apos;name&apos;: [&apos;Miller&apos;, &apos;Jacobson&apos;, &apos;Ali&apos;, &apos;Milner&apos;, &apos;Cooze&apos;, &apos;Jacon&apos;, &apos;Ry ...: aner&apos;, &apos;Sone&apos;, &apos;Sloan&apos;, &apos;Piger&apos;, &apos;Riani&apos;, &apos;Ali&apos;], ...: &apos;preTestScore&apos;: [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3], ...: &apos;postTestScore&apos;: [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70]&#125; ...:In [71]: df = pd.DataFrame(raw_data, columns = [&apos;regiment&apos;, &apos;company&apos;, &apos;name&apos;, &apos;preTes ...: tScore&apos;, &apos;postTestScore&apos;])In [72]: dfOut[72]: regiment company name preTestScore postTestScore0 Nighthawks 1st Miller 4 251 Nighthawks 1st Jacobson 24 942 Nighthawks 2nd Ali 31 573 Nighthawks 2nd Milner 2 624 Dragoons 1st Cooze 3 705 Dragoons 1st Jacon 4 256 Dragoons 2nd Ryaner 24 947 Dragoons 2nd Sone 31 578 Scouts 1st Sloan 2 629 Scouts 1st Piger 3 7010 Scouts 2nd Riani 2 6211 Scouts 2nd Ali 3 70 通过groupby函数生成一个groupby对象，如下： 123456789101112131415161718192021222324# 当针对特定列（此例是&apos;preTestScore&apos;）进行分组时，需要通过df[&apos;colume_name&apos;](此例是df[&apos;regiment&apos;])来指定键名In [73]: groupby_regiment = df[&apos;preTestScore&apos;].groupby(df[&apos;regiment&apos;])# 生成的groupby对象没有做任何计算，只是将数据按键进行分组In [74]: groupby_regimentOut[74]: &lt;pandas.core.groupby.SeriesGroupBy object at 0x11112cef0&gt;# 分组的聚合统计In [75]: groupby_regiment.describe()Out[75]: count mean std min 25% 50% 75% maxregimentDragoons 4.0 15.50 14.153916 3.0 3.75 14.0 25.75 31.0Nighthawks 4.0 15.25 14.453950 2.0 3.50 14.0 25.75 31.0Scouts 4.0 2.50 0.577350 2.0 2.00 2.5 3.00 3.0# 也可以针对特定统计单独计算In [76]: groupby_regiment.mean()Out[76]:regimentDragoons 15.50Nighthawks 15.25Scouts 2.50Name: preTestScore, dtype: float64 整个分组统计的过程，可以通过下图更清晰地展示： 聚合函数 聚合的时候，既可以使用Pandas内置的函数进行聚合计算，也可以使用自定义的函数进行聚合计算，我们先来看下内置的函数： 另外，我们也可以自定义聚合函数： 1234567891011In [81]: def my_agg(pre_test_score_group): ...: return np.sum(np.power(pre_test_score_group, 2)) ...:In [82]: df[&apos;preTestScore&apos;].groupby(df[&apos;regiment&apos;]).apply(my_agg)Out[82]:regimentDragoons 1562Nighthawks 1557Scouts 26Name: preTestScore, dtype: int64 通过上面的例子我们可以看到，通过apply函数也可以完成类似for循环的迭代，在pandas中尽可能使用apply函数来代替for循环迭代，以提高性能。 根据多个键进行分组和聚合 1234567891011121314151617181920# 如果有多个键，将多个键放到一个list当中，作为groupby的参数In [77]: df[&apos;preTestScore&apos;].groupby([df[&apos;regiment&apos;], df[&apos;company&apos;]]).mean()Out[77]:regiment companyDragoons 1st 3.5 2nd 27.5Nighthawks 1st 14.0 2nd 16.5Scouts 1st 2.5 2nd 2.5Name: preTestScore, dtype: float64# unstack之后变成表格模式，更加清晰In [78]: df[&apos;preTestScore&apos;].groupby([df[&apos;regiment&apos;], df[&apos;company&apos;]]).mean().unstack()Out[78]:company 1st 2ndregimentDragoons 3.5 27.5Nighthawks 14.0 16.5Scouts 2.5 2.5 References Apply Operations To Groups In Pandas Pandas official doc - groupby","categories":[],"tags":[]},{"title":"Pandas系列5-DataFrame之过滤","slug":"Pandas系列5-DataFrame之过滤","date":"2018-07-06T10:18:18.000Z","updated":"2018-08-12T10:06:03.730Z","comments":true,"path":"2018/07/06/Pandas系列5-DataFrame之过滤/","link":"","permalink":"http://yoursite.com/2018/07/06/Pandas系列5-DataFrame之过滤/","excerpt":"","text":"Pandas的条件过滤是使用非常频繁的技巧，在这一节我们将看到各种不同的过滤技巧，如果读者有其它过滤技巧，也欢迎告诉我。 &lt;!--more--&gt; 条件过滤与赋值 通过loc进行行过滤，并对过滤后的行进行赋值 123456789101112131415161718192021222324252627In [34]: dfOut[34]: age color height0 20 blue 1651 30 red 1752 15 green 185# 注意这里赋值需要使用如下方式，而不能使用chained index# 具体参考http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copyIn [38]: df.loc[df.color == &apos;blue&apos;,&apos;height&apos;] = 199In [39]: dfOut[39]: age color height0 20 blue 1991 30 red 1752 15 green 185# 表示列数据除了上例中使用&apos;.&apos;,还可以使用&apos;[]&apos;,如下:In [40]: df.loc[df2[&apos;color&apos;]==&apos;blue&apos;, &apos;height&apos;] = 175In [41]: dfOut[41]: age color height0 20 blue 1751 30 red 1752 15 green 185 除了上述的过滤方式外，还可以通过query method来进行过滤查询，如下： 12345678910111213In [248]: df2Out[248]: age color height0 20 black 1551 33 green 1772 22 NaN 1883 20 blue 175In [250]: df2.query(&apos;age&gt;20 &amp; age&lt;40&apos;)Out[250]: age color height1 33 green 1772 22 NaN 188 空值判断 在数据处理的过程中，空值判断是非常常用的技巧，在Pandas中我们主要通过以下几种方式来判断空值。 isnull函数: 用于针对Series、DataFrame判断是否为null notnull函数: 用于判断非null值 np.isnan函数: 用于针对某个标量值进行判断是否为nan(null)。需要注意的是这个函数不能用于字符串类型的值进行判断，因此如果array中有字符串类型，需要用其它方式进行判断，如isinstance isnull函数 12345678910111213141516171819202122232425262728293031323334353637383940In [605]: dfxOut[605]: 1 20a1 2 4a2 5 5b1 5 7In [606]: dfx.iloc[1, 1] = np.nanIn [607]: dfxOut[607]: 1 20a1 2 4.0a2 5 NaNb1 5 7.0In [608]: dfx.isnull()Out[608]: 1 20a1 False Falsea2 False Trueb1 False False# 如果该列所有的值均不为null则返回False，只要有一个值为null则返回TrueIn [609]: dfx.isnull().any()Out[609]:1 False2 Truedtype: bool# 针对DataFrame中的所有值进行检查，只要有一个null值，则返回TrueIn [610]: dfx.isnull().any().any()Out[610]: True# 返回null值的数量In [611]: dfx.isnull().sum().sum()Out[611]: 1 将isnull用于过滤条件： 123456789101112In [244]: df2Out[244]: age color height0 20 black 1551 33 green 1772 22 NaN 1883 20 blue 165In [245]: df2.loc[df2[&apos;color&apos;].isnull(), :]Out[245]: age color height2 22 NaN 188 notnull函数 notnull的使用与isnull类似,如下： 1234567891011121314In [248]: df2Out[248]: age color height0 20 black 1551 33 green 1772 22 NaN 1883 20 blue 175In [249]: df2.loc[df2.color.notnull(), :]Out[249]: age color height0 20 black 1551 33 green 1773 20 blue 175 np.isnan函数 需要注意的是判断dataframe中某个值是否为空，不能直接用== np.nan来判断，而需要使用np.isnan函数如下 12345678910111213In [616]: dfx.iloc[1, 1] == np.nanOut[616]: FalseIn [614]: np.isnan(dfx.iloc[1, 1])Out[614]: True# 其它判断方式同样不行In [617]: dfx.iloc[1, 1] is NoneOut[617]: FalseIn [618]: if not dfx.iloc[1, 1]: print(&quot;True&quot;)In [619]: isin函数 使用isin函数 123456789101112131415161718192021222324252627282930In [764]: dfOut[764]: age color food height score stateJane 30 blue Steak 178 4.6 NYNick 2 green Lamb 181 8.3 TXAaron 12 red Mango 178 9.0 FLPenelope 4 white Apple 178 3.3 ALDean 32 gray Cheese 175 1.8 AKChristina 33 black Melon 178 9.5 TXCornelia 69 red Beans 178 2.2 TXIn [765]: df3 = df[df[&apos;state&apos;].isin([&apos;NY&apos;, &apos;TX&apos;])]In [766]: df3Out[766]: age color food height score stateJane 30 blue Steak 178 4.6 NYNick 2 green Lamb 181 8.3 TXChristina 33 black Melon 178 9.5 TXCornelia 69 red Beans 178 2.2 TX# 也可以使用&apos;~&apos;或者&apos;-&apos;来选择不在列表中的项In [773]: df3 = df[-df[&apos;state&apos;].isin([&apos;NY&apos;, &apos;TX&apos;])]In [774]: df3Out[774]: age color food height score stateAaron 12 red Mango 178 9.0 FLPenelope 4 white Apple 178 3.3 ALDean 32 gray Cheese 175 1.8 AK 多过滤条件 当有多个过滤条件时，我们就需要使用逻辑操作符&amp;, |,如下： 12345678910111213141516171819202122In [251]: df2Out[251]: age color height0 20 black 1551 33 green 1772 22 NaN 1883 20 blue 175In [254]: df2.loc[(df2.age&gt;20) &amp; (df2.color.notnull())]Out[254]: age color height1 33 green 177# 注意在逻辑操作符两边的过滤条件必须使用小括号括起来，否则条件过滤不起作用，如下：In [253]: df2.loc[df2.age&gt;20 &amp; df2.color.notnull()]Out[253]: age color height0 20 black 1551 33 green 1772 22 NaN 1883 20 blue 175 过滤后的赋值计算 在实际项目中，很多时候我们根据条件选取了一些行之后，我们要针对这些行中的数据需要做些操作（比如针对age进行加1操作），更复杂的我们需要获取本行的其它列的数据共同计算和判断。这里我们可以使用如下技巧: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061In [256]: dfOut[256]: age color food height score stateJane 30 blue Steak 165 4.6 NYNick 2 green Lamb 70 8.3 TXAaron 12 red Mango 120 9.0 FLPenelope 4 white Apple 80 3.3 ALDean 32 gray Cheese 180 1.8 AKChristina 33 black Melon 172 9.5 TXCornelia 69 red Beans 150 2.2 TX# 使用mask作为我们的筛选条件In [258]: mask = (df.color==&apos;blue&apos;)# 选出符合条件的行，并对age列的数据进行加1操作In [260]: df.loc[mask, &apos;age&apos;] = df.loc[mask, &apos;age&apos;] + 1In [261]: dfOut[261]: age color food height score stateJane 31 blue Steak 165 4.6 NYNick 2 green Lamb 70 8.3 TXAaron 12 red Mango 120 9.0 FLPenelope 4 white Apple 80 3.3 ALDean 32 gray Cheese 180 1.8 AKChristina 33 black Melon 172 9.5 TXCornelia 69 red Beans 150 2.2 TX# 更复杂的，我们如果需要同一行的其它数据进行计算，那么我们就需要使用apply函数和并选出响应的列，如下:In [262]: df_with_age_height = df.loc[mask, [&apos;age&apos;, &apos;height&apos;]]In [265]: df.loc[mask, &apos;score&apos;] = df_with_age_height.apply(lambda row: row[&apos;age&apos;] + row[&apos; ...: height&apos;]/100, axis=1)In [266]: dfOut[266]: age color food height score stateJane 31 blue Steak 165 32.65 NYNick 2 green Lamb 70 8.30 TXAaron 12 red Mango 120 9.00 FLPenelope 4 white Apple 80 3.30 ALDean 32 gray Cheese 180 1.80 AKChristina 33 black Melon 172 9.50 TXCornelia 69 red Beans 150 2.20 TX# 使用apply仍然是使用迭代的方式，我们可以通过vectorization的方式直接计算，如下In [10]: mask = (df.color == &apos;red&apos;)In [13]: df_with_age_height = df.loc[mask, [&apos;age&apos;, &apos;height&apos;]]In [14]: df.loc[mask, &apos;score&apos;] = (df_with_age_height[&apos;age&apos;] + df_with_age_height[&apos;height&apos;])/100In [15]: dfOut[15]: age color food height score stateJane 30 blue Steak 165 1.95 NYNick 2 green Lamb 70 8.30 TXAaron 12 red Mango 120 1.32 FLPenelope 4 white Apple 80 3.30 ALDean 32 gray Cheese 180 1.80 AKChristina 33 black Melon 172 9.50 TXCornelia 69 red Beans 150 2.19 TX 关于vectorization矢量化的相关议题，可以参考文章Pandas系列4-数据矢量化","categories":[],"tags":[]},{"title":"Python定时任务-schedule vs. Celery vs. APScheduler","slug":"Python定时任务-schedule-vs--Celery-vs--APScheduler","date":"2018-06-21T10:18:18.000Z","updated":"2018-08-12T10:06:55.935Z","comments":true,"path":"2018/06/21/Python定时任务-schedule-vs--Celery-vs--APScheduler/","link":"","permalink":"http://yoursite.com/2018/06/21/Python定时任务-schedule-vs--Celery-vs--APScheduler/","excerpt":"","text":"本文详细讲述了在Python开发中常用的几种定时任务途径，并重点对比了schedule, Celery和APScheduler &lt;!--more--&gt; 在Python开发过程中我们经常需要执行定时任务，而此类任务我们通常有如下选项： 自己造轮子 使用schedule库 使用Celery定时任务 使用APScheduler 自己造轮子实现，最大的优势就是灵活性，调试方便，对于某些特定系统也许也是一种选择，不过对于大多数应用来说，我们应当尽可能地使用开源的成熟的方案。下面对后三种方案分别讨论： 使用schedule库 schedule库是一个轻量级的定时任务方案，优势是使用简单，也不需要做什么配置；缺点是无法动态添加任务，也无法将任务持久化。 安装 1pip install schedule 使用 12345678910111213141516import scheduleimport timedef job(): print(&quot;I&apos;m working...&quot;)schedule.every(10).minutes.do(job)schedule.every().hour.do(job)schedule.every().day.at(&quot;10:30&quot;).do(job)schedule.every(5).to(10).minutes.do(job)schedule.every().monday.do(job)schedule.every().wednesday.at(&quot;13:15&quot;).do(job)while True: schedule.run_pending() time.sleep(1) 使用Celery Celery在Python领域可谓大名鼎鼎，我们通常将Celery作为一个任务队列来使用，不过Celery也同时提供了定时任务功能。通常，当我们的解决方案中已经在使用Celery的时候可以考虑同时使用其定时任务功能，但是Celery无法在Flask这样的系统中动态添加定时任务（在Django中有相应的插件可以实现动态添加任务），而且如果对于不使用Celery的项目，单独为定时任务搭建Celery显得过于重量级了。(搭建Celery比较麻烦，还需要配置诸如RabbitMQ之类消息分发程序)。 Celery安装在此不再赘述，大家可以参考官网的资料 使用 Celery虽然无法动态添加定时任务，但是可以在程序固定位置添加定时任务，如下： 1234567891011121314151617181920212223from celery import Celeryfrom celery.schedules import crontabapp = Celery()# 此处on_after_configure装饰符意味着当Celery app配置完成之后调用该hook函数@app.on_after_configure.connectdef setup_periodic_tasks(sender, **kwargs): # Calls test(&apos;hello&apos;) every 10 seconds. sender.add_periodic_task(10.0, test.s(&apos;hello&apos;), name=&apos;add every 10&apos;) # Calls test(&apos;world&apos;) every 30 seconds sender.add_periodic_task(30.0, test.s(&apos;world&apos;), expires=10) # Executes every Monday morning at 7:30 a.m. sender.add_periodic_task( crontab(hour=7, minute=30, day_of_week=1), test.s(&apos;Happy Mondays!&apos;), )@app.taskdef test(arg): print(arg) 这里调用add_periodic_task用于添加一个定时任务，相当于在Celery config文件中的beat_schedule设置项中添加了一项，如下： 1234567app.conf.beat_schedule = &#123; &apos;add-every-30-seconds&apos;: &#123; &apos;task&apos;: &apos;tasks.add&apos;, &apos;schedule&apos;: 30.0, &apos;args&apos;: (16, 16) &#125;,&#125; 在add_periodic_task中指定job function时需要用.s()来调用 使用APScheduler 笔者认为APScheduler是在实际项目最好用的一个工具库。它不仅可以让我们在程序中动态添加和删除我们的定时任务，还支持持久化，且其持久化方案支持很多形式，包括(Memory, MongoDB, SQLAlchemy, Redis, RethinkDB, ZooKeeper), 也可以非常好与一些Python framework集成(包括asyncio, gevent, Tornado, Twisted, Qt). 笔者所在的项目使用的是Flask框架，也有相应的插件可以供我们直接使用。 但是笔者没有使用插件，而是直接将APScheduler集成于项目代码中。 初始化scheduler 12345678910111213141516171819# 可以在初始化Flask的时候调用，并将返回的scheduler赋给appdef init_scheduler(): # 这里用于持久化的设置，代码中演示使用MongoDB # client用于设置你自己的MongoDB的handler, 即MongoClient对象 jobstores = &#123; &apos;default&apos;: MongoDBJobStore(client=your_db_handler, collection=&quot;schedule_job&quot;) &#125; executors = &#123; &apos;default&apos;: ThreadPoolExecutor(20) &#125; job_defaults = &#123; &apos;coalesce&apos;: False, &apos;max_instances&apos;: 5 &#125; # 这里使用BackgroundScheduler即可 scheduler = BackgroundScheduler(jobstores=jobstores, executors=executors, job_defaults=job_defaults, timezone=utc) # 注意这里一定要调用start启动scheduler scheduler.start() return scheduler 添加定时任务 APScheduler将定时任务分为三种： interval: 比如每隔5分钟执行一次任务 cron: 比如每天早上5点执行一次任务 date: 比如在2018年5月5日执行一次任务 我们以添加cron job为例： 123456789101112def test_job(name): print &quot;hello, %s&quot; % name def add_daily_job(name): exec_time = datetime.now() + timedelta(minutes=2) hour = exec_time.strftime(&quot;%H&quot;) minute = exec_time.strftime(&quot;%M&quot;) # 这里要选择&apos;cron&apos; # 另外，job_id可以根据你自己的情况设定，其会被用于remove_job current_app.scheduler.add_job( test_job, &apos;cron&apos;, hour=hour, minute=minute, args=[name], id=job_id) 删除定时任务 通过在add_job时使用的job_id可以删除对应的定时任务。实际上在我们添加任务的时候，APScheduler会把相应的任务信息存储于我们jobstore中设置的持久化存储方案，这里使用的是MongoDB，然后当删除的时候会将相应的任务从MongoDB中删除。 12def remove_daily_job(job_id): current_app.scheduler.remove_job(job_id) 总结： APScheduler在实际使用过程中拥有最大的灵活性，可以满足我们的大部分定时任务的相关需求；Celery比较重量级，通常如果项目中已有Celery在使用，而且不需要动态添加定时任务时可以考虑使用；schedule非常轻量级，使用简单，但是不支持任务的持久化，也无法动态添加删除任务，所以主要用于简单的小型应用。 References Schedule Celery APScheduler","categories":[],"tags":[]},{"title":"Pandas系列4-数据矢量化","slug":"Pandas系列4-数据矢量化","date":"2018-06-21T10:18:18.000Z","updated":"2018-08-12T10:05:50.000Z","comments":true,"path":"2018/06/21/Pandas系列4-数据矢量化/","link":"","permalink":"http://yoursite.com/2018/06/21/Pandas系列4-数据矢量化/","excerpt":"","text":"我们在处理数据问题时，经常会遇到的问题是要将原有数据进行转化，比如在原有 数据的基础上+1操作，或者将原有数据的字符串全部转化为小写字符，更复杂的是 要将原有数据的一部分提取出来使用。这些问题都是数据转化问题，即原有的数据 不能直接使用，而要进一步转化后才能使用。 &lt;!--more--&gt; 问题 我们在处理数据问题时，经常会遇到的问题是要将原有数据进行转化，比如在原有数据的基础上+1操作，或者将原有数据的字符串全部转化为小写字符，更复杂的是要将原有数据的一部分提取出来使用。这些问题都是数据转化问题，即原有的数据不能直接使用，而要进一步转化后才能使用。 示例 这里举一个笔者在实际项目中遇到的例子来说明。 笔者项目中需要收集的app version信息，原始信息如下： 12345678In [167]: dfOut[167]: app_version uid0 7.23.1-180522122 11 7.20.1-180502135 22 7.23.1-180522122 33 7.23.1-180522122 44 7.16.7-180411077 5 但是实际上，我们只需要&quot;-&quot;之前的版本号，而且后续比较的时候要用'-'之前的数字进行比较，因此这样就涉及到了将原版本数据进行转化，即只提取'-'之前的数字，而舍弃后边的数字。 迭代 一个显而易见的做法是通过遍历的方式来逐行修改，如下图所示： 1234567891011121314In [178]: %%timeit ...: for index, row in df.iterrows(): ...: df.iloc[index, 0] = row[&apos;app_version&apos;].split(&apos;-&apos;)[0] ...:2.34 ms ± 47.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)In [179]: dfOut[179]: app_version uid0 7.23.1 11 7.20.1 22 7.23.1 33 7.23.1 44 7.16.7 5 再进一步，我们可以使用apply方法，如下： 12345678910111213141516171819202122In [181]: dfOut[181]: app_version uid0 7.23.1-180522122 11 7.20.1-180502135 22 7.23.1-180522122 33 7.23.1-180522122 44 7.16.7-180411077 5In [182]: %%timeit ...: df[&apos;app_version&apos;] = df[&apos;app_version&apos;].apply(lambda x: x.split(&apos;-&apos;)[0]) ...:247 µs ± 11.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)In [183]: dfOut[183]: app_version uid0 7.23.1 11 7.20.1 22 7.23.1 33 7.23.1 44 7.16.7 5 我们可以发现使用apply不仅使得代码更加简洁，而且速度也有了较明显的提升。但是以上方法本质上都是通过迭代的方式一条一条的修改，那么我们能否进一步提升性能呢？ 矢量化 12345678910111213In [197]: dfOut[197]: app_version uid0 7.23.1-180522122 11 7.20.1-180502135 22 7.23.1-180522122 33 7.23.1-180522122 44 7.16.7-180411077 5In [198]: %%timeit ...: df[&apos;app_version&apos;] = df[&apos;app_version&apos;].str.split(&apos;-&apos;).str.get(0) ...:424 µs ± 11.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) 这里发现矢量化貌似不能提高性能啊，这是为什么？ 这里我猜测是由于我们的矢量化代码是分为两步操作，且在数据量较小的情况下就会显得慢 为了验证这个假设，我做了如下实验： 先将原数据concat为2560条记录，然后再计算时间 1234567891011121314151617182557 7.23.1-180522122 32558 7.23.1-180522122 42559 7.16.7-180411077 5[2560 rows x 2 columns]In [232]: df3 = dfIn [233]: %%timeit ...: df[&apos;app_version&apos;] = df[&apos;app_version&apos;].apply(lambda x: x.split(&apos;-&apos;)[0]) ...:1.36 ms ± 35.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)# 矢量化方式In [250]: %%timeit ...: df[&apos;app_version&apos;] = df[&apos;app_version&apos;].str.split(&quot;-&quot;).str.get(0) ...:2.61 ms ± 113 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) 发现单纯的数据量增大并没有影响结果，那么用其它转化来测试下，这里获取字符串长度的转化进行实验 12345678910In [253]: %%timeit ...: df[&apos;length&apos;] = df[&apos;app_version&apos;].str.len() ...:901 µs ± 17.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)In [254]: %%timeit ...: df3[&apos;length&apos;] = df3[&apos;app_version&apos;].apply(lambda x: len(x)) ...: ...:1.16 ms ± 14.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) 我们看到在这里就体现出了矢量化的优势，因为这里大家都是一步。 结论：当矢量化步数只有一步时，其性能还是要比apply方式好的，但当需要多步的时候，不一定好于apply方式。 那么，我们能否将其转化为一步呢？后发现有extract这样的函数，使用如下： 1234567891011121314In [312]: dfOut[312]: app_version uid0 7.23.1-180522122 11 7.20.1-180502135 22 7.23.1-180522122 33 7.23.1-180522122 44 7.16.7-180411077 5In [313]: %%timeit ...: df[&apos;app_version&apos;] = df[&apos;app_version&apos;].str.extract(r&quot;([0-9\\.]+)-[0-9]+&quot;, expand=Fal ...: se) ...:247 µs ± 8.95 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) 通过extract终于实现了一步的矢量化。而且性能上也是最优的。 这里需要注意的是，如果使用timeit, 由于多次操作，会导致后续df中'app_version'的值变为NaN。当我们只操作一次的时候则不存在此问题。 References Working with text data Working with strings 优化Pandas代码执行速度入门指南 Pandas 中map, applymap and apply的区别","categories":[],"tags":[]},{"title":"Pandas系列3-DataFrame之增加与删除","slug":"Pandas系列3-DataFrame之增加与删除","date":"2018-06-16T10:18:18.000Z","updated":"2018-08-12T10:05:38.026Z","comments":true,"path":"2018/06/16/Pandas系列3-DataFrame之增加与删除/","link":"","permalink":"http://yoursite.com/2018/06/16/Pandas系列3-DataFrame之增加与删除/","excerpt":"","text":"在使用Pandas的过程增删改查是频繁使用的操作，这一节主要就是展示DataFrame常用的增加和删除操作 &lt;!--more--&gt; 增加行和增加列 12345678910111213141516171819202122232425262728293031323334353637# 增加一列，我们可以有两种方式，如下：In [345]: dfOut[345]: one twoa 0 1b 4 5c 8 9d 12 13In [346]: df[&apos;three&apos;] = [3, 5, 5, 7]In [347]: dfOut[347]: one two threea 0 1 3b 4 5 5c 8 9 5d 12 13 7# 或者使用locIn [369]: dfOut[369]: one two threea 0 1 2b 4 5 6c 8 9 10d 12 13 14In [370]: df.loc[:, &quot;four&quot;] = [1, 4, 5, 9]In [371]: dfOut[371]: one two three foura 0 1 2 1b 4 5 6 4c 8 9 10 5d 12 13 14 9 需要注意的是使用如上两种方式增加一列的时候，其数组的长度必须与原有DataFrame的行数相同，否则会报如下错误 1ValueError: Length of values does not match length of index 增加行同样我们也可以使用loc, 如下: 123456789101112131415161718In [376]: dfOut[376]: one two three foura 0 1 2 3b 4 5 6 7c 8 9 10 11d 12 13 14 15In [377]: df.loc[&apos;e&apos;] = [3, 7, 8, 9]In [378]: dfOut[378]: one two three foura 0 1 2 3b 4 5 6 7c 8 9 10 11d 12 13 14 15e 3 7 8 9 但很多时候，我们并不需要row index, 只想自动增加一行，那么可以通过如下的方式 1234567891011In [379]: df.loc[df.shape[0]+1] = [3, 5, 9, 9]In [380]: dfOut[380]: one two three foura 0 1 2 3b 4 5 6 7c 8 9 10 11d 12 13 14 15e 3 7 8 96 3 5 9 9 另外，我们还可以将数据转化为Series，然后利用concat或者append的方式将其与原有的DataFrame进行合并。这种方式不仅可以添加一行数据，也可以一次性添加多行数据。 123456789101112131415161718In [392]: df = pd.DataFrame(np.arange(16).reshape((4,4)), index=[1, 2, 3, 4], columns=[&apos;a&apos;, ...: &apos;b&apos;, &apos;c&apos;,&apos;d&apos;])In [393]: df2 = pd.DataFrame(np.arange(16).reshape((4,4)), index=[5, 6, 7, 8], columns=[&apos;a&apos; ...: , &apos;b&apos;, &apos;c&apos;,&apos;d&apos;])# 这里相当于添加了多行数据In [394]: pd.concat([df, df2])Out[394]: a b c d1 0 1 2 32 4 5 6 73 8 9 10 114 12 13 14 155 0 1 2 36 4 5 6 77 8 9 10 118 12 13 14 15 更多关于concat和append将在后续的章节详细讲解。 删除行和删除列 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748In [751]: dfOut[751]: age color food height score stateJane 30 blue Steak 178 4.6 NYNick 2 green Lamb 181 8.3 TXAaron 12 red Mango 178 9.0 FLPenelope 4 white Apple 178 3.3 ALDean 32 gray Cheese 175 1.8 AKChristina 33 black Melon 178 9.5 TXCornelia 69 red Beans 178 2.2 TX# 使用drop删除index为&apos;Dean&apos;的行In [752]: df.drop([&apos;Dean&apos;])Out[752]: age color food height score stateJane 30 blue Steak 178 4.6 NYNick 2 green Lamb 181 8.3 TXAaron 12 red Mango 178 9.0 FLPenelope 4 white Apple 178 3.3 ALChristina 33 black Melon 178 9.5 TXCornelia 69 red Beans 178 2.2 TXIn [749]: dfOut[749]: age color food height score stateJane 30 blue Steak 178 4.6 NYNick 2 green Lamb 181 8.3 TXAaron 12 red Mango 178 9.0 FLPenelope 4 white Apple 178 3.3 ALDean 32 gray Cheese 175 1.8 AKChristina 33 black Melon 178 9.5 TXCornelia 69 red Beans 178 2.2 TX# 使用drop删除名为&apos;height&apos;的列，注意需要使用axis=1# 使用inplace来空值是在同一块内存还是copyIn [750]: df.drop([&apos;height&apos;], axis=1, inplace=True)Out[750]: age color food score stateJane 30 blue Steak 4.6 NYNick 2 green Lamb 8.3 TXAaron 12 red Mango 9.0 FLPenelope 4 white Apple 3.3 ALDean 32 gray Cheese 1.8 AKChristina 33 black Melon 9.5 TXCornelia 69 red Beans 2.2 TX# drop多列df.drop([&apos;height&apos;, &apos;food&apos;], axis=1, inplace=True) 条件删除 由于在数据清洗的过程中经常需要删除不符合条件的record，所以以下这种条件过滤行就非常有用。需要注意的是，这里是重新生成了一个DataFrame，而不是直接在原有的DataFrame上修改 1234567891011In [755]: df2 = df[df.color!=&apos;blue&apos;]In [756]: df2Out[756]: age color food height score stateNick 2 green Lamb 181 8.3 TXAaron 12 red Mango 178 9.0 FLPenelope 4 white Apple 178 3.3 ALDean 32 gray Cheese 175 1.8 AKChristina 33 black Melon 178 9.5 TXCornelia 69 red Beans 178 2.2 TX 去重 使用drop_duplicates，我们可以去掉重复项，这是一个非常有用的函数，下面我们来详细分析下 通过参数subset,指定去重比较时用哪些column。如果不指定则所有的数据都会比较，只有所有列的数据都一致的时候才会去掉，否则不会去掉，如下： 123456789101112131415161718192021In [459]: df_con2Out[459]: uid num1 num20 a1 1 4.01 a2 3 5.02 b1 5 7.00 a1 2 4.01 a2 5 NaN2 a3 2 2.0In [460]: df_drop = df_con2.drop_duplicates()In [461]: df_dropOut[461]: uid num1 num20 a1 1 4.01 a2 3 5.02 b1 5 7.00 a1 2 4.01 a2 5 NaN2 a3 2 2.0 我们可以让两个dataframe只比较uid列，只要这一列的数据重复，我们就认为重复，如下： 123456789In [462]: df_drop2 = df_con2.drop_duplicates(subset=&apos;uid&apos;)In [463]: df_drop2Out[463]: uid num1 num20 a1 1 4.01 a2 3 5.02 b1 5 7.02 a3 2 2.0 另外从上边的例子可以看出，其去重是去掉了后边出现的重复的项，我们也可以保留后边的项，将前边的项去掉，那么就需要使用keep参数。另外，我们也可以直接在DataFrame中进行去重，而不需要再另外copy一份数据，这可以通过inplace=True来实现，示例如下： 123456789101112131415161718192021In [453]: df_conOut[453]: uid num1 num20 a1 1 4.01 a2 3 5.02 b1 5 7.00 a1 2 4.01 a2 5 NaN2 a3 2 2.0# 注意这里没有赋值操作，因为使用了inplace=True# 使用keep=&apos;last&apos;用于保存后边的数据，删除前边的重复项In [454]: df_con.drop_duplicates(subset=&apos;uid&apos;, keep=&apos;last&apos;, inplace=True)In [455]: df_conOut[455]: uid num1 num22 b1 5 7.00 a1 2 4.01 a2 5 NaN2 a3 2 2.0 References StackOverflow 删除行和列 Merging","categories":[],"tags":[]},{"title":"Pandas系列2-DataFrame之数据定位","slug":"Pandas系列2-DataFrame之数据定位","date":"2018-06-15T10:18:18.000Z","updated":"2018-08-12T10:05:16.900Z","comments":true,"path":"2018/06/15/Pandas系列2-DataFrame之数据定位/","link":"","permalink":"http://yoursite.com/2018/06/15/Pandas系列2-DataFrame之数据定位/","excerpt":"","text":"在Pandas中我们往往需要先定位数据才能进行相应的赋值、修改等后续操作，因此定位是Pandas中非常重要的一环，本文将详解Pandas中的各种定位方式。 &lt;!--more--&gt; 在Pandas中我们主要通过以下几个函数来定位DataFrame中的特定数据 iloc loc iat at 总的来说，分为两种： 一种是通过lables(即row index和column names，这里row index可以字符，日期等非数字index)(使用loc, at); 另一种通过index(这里特指数字位置index)(使用iloc, iat) loc和at的区别在于， loc可以选择特定的行或列，但是at只能定位某个特定的值，标量值。一般情况下，我们iloc和loc更加通用，而at, iat有一定的性能提升。 具体示例可以参考Reference中StackOverflow的示例 下面展示一些特别的： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071In [630]: dfOut[630]: age color food height score stateJane 30 blue Steak 165 4.6 NYNick 2 green Lamb 70 8.3 TXAaron 12 red Mango 120 9.0 FLPenelope 4 white Apple 80 3.3 ALDean 32 gray Cheese 180 1.8 AKChristina 33 black Melon 172 9.5 TXCornelia 69 red Beans 150 2.2 TX# 选择某一行数据In [631]: df.loc[&apos;Dean&apos;]Out[631]:age 32color grayfood Cheeseheight 180score 1.8state AKName: Dean, dtype: object# 选择某一列数据，逗号前面是行的label，逗号后边是列的label，使用&quot;:&quot;来表示选取所有的，本例是选取所有的行，当&apos;:&apos;在逗号后边时表示选取所有的列，但通常我们可以省略。In [241]: df.loc[:, &apos;color&apos;]Out[241]:Jane blueNick greenAaron redPenelope whiteDean grayChristina blackCornelia redName: color, dtype: object# 也可以如下选取一列，但是与前者是有区别的，具体参考Reference中的《Returning a view versus a copy》In [632]: df.loc[:][&apos;color&apos;]Out[632]:Jane blueNick greenAaron redPenelope whiteDean grayChristina blackCornelia redName: color, dtype: object# 选择某几行数据，注意无论选择多行还是多列，都需要将其label放在一个数组当中，而选择单个行或列，则不需要放在数组当中In [634]: df.loc[[&apos;Nick&apos;, &apos;Dean&apos;]]Out[634]: age color food height score stateNick 2 green Lamb 70 8.3 TXDean 32 gray Cheese 180 1.8 AK# 注意以下这种用法不行，这是由于Pandas会认为逗号后边是列的labeldf.loc[&apos;Nick&apos;, &apos;Dean&apos;]# 选择范围In [636]: df.loc[&apos;Nick&apos;:&apos;Christina&apos;]Out[636]: age color food height score stateNick 2 green Lamb 70 8.3 TXAaron 12 red Mango 120 9.0 FLPenelope 4 white Apple 80 3.3 ALDean 32 gray Cheese 180 1.8 AKChristina 33 black Melon 172 9.5 TX# iloc的特定用法, 可以用-1这样index来获取最后一行的数据In [637]: df.iloc[[1, -1]]Out[637]: age color food height score stateNick 2 green Lamb 70 8.3 TXCornelia 69 red Beans 150 2.2 TX 数据定位是后续条件过滤、赋值以及各种转换的基础，一定要熟练掌握。 另外，在定位某一个具体的元素的时候，loc和at并不完全相同 12345678910111213141516171819202122232425# loc支持以下两种定位方式In [726]: df.loc[&apos;Jane&apos;, &apos;score&apos;]Out[726]: 4.6In [727]: df.loc[&apos;Jane&apos;][&apos;score&apos;]Out[727]: 4.6# 但是at只支持第一种定位方式In [729]: df.at[&apos;Nick&apos;, &apos;height&apos;]Out[729]: 181In [730]: df.at[&apos;Nick&apos;][&apos;height&apos;]---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-730-948408df1727&gt; in &lt;module&gt;()----&gt; 1 df.at[&apos;Nick&apos;][&apos;height&apos;]~/.pyenv/versions/3.6.4/envs/data_analysis/lib/python3.6/site-packages/pandas/core/indexing.py in __getitem__(self, key) 1867 1868 key = self._convert_key(key)-&gt; 1869 return self.obj._get_value(*key, takeable=self._takeable) 1870 1871 def __setitem__(self, key, value):TypeError: _get_value() missing 1 required positional argument: &apos;col&apos; 有两点需要说明： 在针对特定元素赋值的时候最好使用at来进行操作，性能提升还是很明显的。 loc的两种方式并不等同，df.loc['Jane', 'score']是在同一块内存中对数据进行操作，而df.loc['Jane']['score']是在另一个copy上进行操作，具体参考Returning a view versus a copy References StackOverflow: iloc vs. loc vs. iat vs. at Returning a view versus a copy","categories":[],"tags":[]},{"title":"Pandas系列1-DataFrame之初始化","slug":"Pandas系列1-DataFrame之初始化","date":"2018-06-08T10:18:18.000Z","updated":"2018-10-05T14:17:12.035Z","comments":true,"path":"2018/06/08/Pandas系列1-DataFrame之初始化/","link":"","permalink":"http://yoursite.com/2018/06/08/Pandas系列1-DataFrame之初始化/","excerpt":"","text":"Pandas中如果要初始化DataFrame对象，实际又很多种方式，本文将详解Pandas初始化的几种不同方式。 &lt;!--more--&gt; DataFrame有多种初始化方法，主要分为以下几种情况： 通过Object初始化 通过文件初始化 通过SQL查询结果初始化 通过NoSQL数据库查询结果初始化 下面分别介绍： 通过object初始化 这又分为以下几种方式 Dict of 1D ndarrays, lists, dicts, or Series 2-D numpy.ndarray Structured or record ndarray A Series Another DataFrame 通过list 通过1D data series初始化的时候，如果有多列，那么需要等长 1234567# columns参数是通过一个list参数来指定column labelsdf = pd.DataFrame([['a1', 1], ['a2', 4]], columns=['uid', 'score'])In [477]: dfOut[477]: uid score0 a1 11 a2 4 通过Dict of 1D ndarray 12345678In [298]: df = pd.DataFrame(&#123;'col1': np.arange(3), 'col2': np.arange(5, 8)&#125;)In [299]: dfOut[299]: col1 col20 0 51 1 62 2 7 通过Dict of lists 123456789In [294]: df = pd.DataFrame(&#123;'col1': [1, 2, 3, 4], 'col2': ['a', 'b', 'c', 'd']&#125;)In [295]: dfOut[295]: col1 col20 1 a1 2 b2 3 c3 4 d 通过list of dicts 注意与上边的dict of lists区分，如果最外层是dict，那么key值默认是column label。 而在list of dicts中，每个dict都是一个record，或者说一行 1234567891011121314151617181920# 可以不等长，缺失值自动设为NaNIn [49]: data2 = [&#123;'a': 1, 'b': 2&#125;, &#123;'a': 5, 'b': 10, 'c': 20&#125;]In [50]: pd.DataFrame(data2)Out[50]: a b c0 1 2 NaN1 5 10 20.0In [51]: pd.DataFrame(data2, index=['first', 'second'])Out[51]: a b cfirst 1 2 NaNsecond 5 10 20.0In [52]: pd.DataFrame(data2, columns=['a', 'b'])Out[52]: a b0 1 21 5 10 通过Dict of Series 1234567891011121314151617181920212223242526272829303132In [314]: s = pd.Series(range(5))In [315]: sOut[315]:0 01 12 23 34 4dtype: int64In [316]: p = pd.Series(range(8, 13))In [317]: pOut[317]:0 81 92 103 114 12dtype: int64In [318]: df = pd.DataFrame(&#123;'a': s, 'b': p&#125;)In [319]: dfOut[319]: a b0 0 81 1 92 2 103 3 114 4 12 通过2-D numpy.ndarray 12345678910In [289]: df = pd.DataFrame(np.arange(16).reshape((4,4)), columns=['one', 'two', 'three', ...: 'four'], index=['a', 'b', 'c','d'])In [290]: dfOut[290]: one two three foura 0 1 2 3b 4 5 6 7c 8 9 10 11d 12 13 14 15 通过文件初始化 pandas通过各种数据文件也可以初始化，比如csv文件，excel文件，json文件，html文件等，详见下图 下面以read_csv详细解释下读取csv文件以及初始化的过程 read_csv的完整文档参考read_csv api，下面通过示例对常用的parameter进行解释： 1234csv_path = \"./test.csv\"columns = ['id', 'name', 'age']dtype = &#123;'id': int, 'name': object, 'age': int&#125;pd.read_csv(csv_path, header=None, names=columns, dtype=dtype) filepath_or_buffer, 这个是最基本的参数，用以指明文件的路径(路径可以是字符串，也可以是各种path对象，详见文档)或者文件对象(也可以接收类文件对象, 即提供read method， 如StringIO对象)。另外，这个参数也可以是一个URL，而这个URL可以http, ftp, 或者s3的url. 对于没有权限限制的url，直接使用read_csv可以大大简化代码，但是通过我们的数据不会放置到公开的url地址上，因此这就涉及权限的问题，通常还是通过其它手段将文件下载到本地后再读取。 header, 这个参数用于设置第几行为column names, 默认是'infer'，即Pandas会自动推断哪一行是column names。当文件中没有column names时，相当于设定header=0。很多时候想要忽略原始的column names而自己设定column names，那么可以将这个参数设置为None, 然后通过names参数来设定column names names, 用于设定column names dtype, 用于设定每一列对应的数据类型，需要注意的是对string类型需要设置为object nrows, 要读取多少行，通过这个参数我们可以部分读取文件 usecols, 用于选定列，即指定哪些列load进DataFrame中，通过这个参数可以只读取我们需要的数据，从而减少内存占用，加快load速度。 通过SQL查询结果初始化 1234import pandas.io.sql as sql# conn是数据库的连接对象sql.read_frame('select * from test', conn) NoSQL查询结果初始化 这里以MongoDB为例 123456# 从MongoDB中查询年龄大于20岁的用户，查询返回一个cursor对象user_results = user.find(&#123;\"age\": &#123;\"$gt\": 20&#125;&#125;)# 将cursor对象转化为list，然后初始化# columns可以用于选取相应的field的数据，只有在这个列表中的field才会被load进DataFrame对象当中，如果没有对应的数据，会被填入NaNdf = pd.DataFrame(list(user_results), columns=['id', 'age', 'name'] 这里需要注意的是如果不指定columns参数，有可能导致某些为空的field没有对应的列，如果指定了列名称，则如果相对应的域没有数据的话，就会自动置为nan References read_csv API IO Tools 《利用Python进行数据分析》","categories":[],"tags":[{"name":"pandas","slug":"pandas","permalink":"http://yoursite.com/tags/pandas/"}]},{"title":"MongoDB大批量读写数据优化记录","slug":"MongoDB大批量读写数据优化记录","date":"2018-04-20T10:18:18.000Z","updated":"2018-08-12T10:07:41.645Z","comments":true,"path":"2018/04/20/MongoDB大批量读写数据优化记录/","link":"","permalink":"http://yoursite.com/2018/04/20/MongoDB大批量读写数据优化记录/","excerpt":"","text":"本文主要阐述了MongoDB大批量数据读写过程中的一些优化技巧 &lt;!--more--&gt; 用批量写入代替单个写入 最开始，我的代码逻辑是这样的： 12for uid, data in user_dict.items(): user_collection.insert_one(&#123;&apos;uid&apos;:uid, &apos;user_data&apos;: data&#125;) 这种方法在数据量较小时可以很好的工作，但是当数据量非常大时，此种操作会非常慢，我们需要通过批量写入的方式来写入数据。 12user_data = (&#123;&apos;uid&apos;: uid, &apos;user_data&apos;: data&#125; for uid, data in user_dict.items())user_collection.insert_many(user_data) 调整insert_many参数 再来看是否可以通过调整insert_many参数来进一步优化性能。 ordered: 这个参数为True时，迫使MongoDB按顺序同步插入数据；而如果为False，则MongoDB会并发的不按固定顺序进行批量插入。显然当我们对性能有要求时，将该参数设为False是非常必要的。 bypass_document_validation: MongoDB3.2之后加入了document validation功能，用于验证写入的文档是否符合collection制定的规则，具体可以参考reference中的链接。而既然是验证就肯定需要花费时间，当我们对性能有极致要求时，也可以将此参数设为True，从而越过验证，直接写入。 session: 关于session，请参考References中的Client Session链接。 修改后的代码如下： 12user_data = (&#123;&apos;uid&apos;: uid, &apos;user_data&apos;: data&#125; for uid, data in user_dict.items())user_collection.insert_many(user_data, ordere=False, bypass_document_validation=True) 最终性能的提升是非常明显的，时间量级从天降为分钟。 批量更新 前面的例子在插入操作时非常有效，但是对于更新操作由于update_many无法针对每一个doc进行更新，如本例中针对每一个uid进行更新，那么就需要使用bulk_write操作。 12345678from pymongo import UpdateOneupdate_operations = []for uid, user_data in user_dict.items(): op = UpdateOne(&#123;&apos;uid&apos;: uid&#125;, &#123;&apos;$set&apos;: &#123;&apos;user_data&apos;: user_data&#125;&#125;, upsert=True) update_operations.append(op)user_collection.bulk_write(update_operations, ordered=False, bypass_document_validation=True) 批量读取 批量读取我们可以使用$in操作符，但是需要注意的是如果$in针对的list过大，那么可能会导致报错pymongo.errors.DocumentTooLarge, 目前我的做法是将大的list分割成1000个一段，然后分段查询 1234567891011list_length = len(uid_list)iter_size = 1000current = 0while current &lt; list_length: end = current + iter_size uid_segment = uid_list[current: end] result_cursor = mongo_collection.find(&#123;&quot;uid&quot;: &#123;&quot;$in&quot;: uid_segment&#125;&#125;) for user_info in result_cursor: # do something ... current = current + iter_size 异常处理 在实践过程中，会遇到异常的情况，尤其是写入的时候，可能由于各种原因导致写入失败，因此需要catch exception，并打印详细信息，如下： 12345try: user_collection.insert_many( data_iter, ordered=False, bypass_document_validation=True)except BulkWriteError as e: lg.error(e.details) References: PyMongo API MongoDB document validation Client Session","categories":[],"tags":[]},{"title":"在Flask中使用Celery的最佳实践","slug":"flask_celery","date":"2018-02-22T10:18:18.000Z","updated":"2018-08-12T10:10:02.073Z","comments":true,"path":"2018/02/22/flask_celery/","link":"","permalink":"http://yoursite.com/2018/02/22/flask_celery/","excerpt":"","text":"主要是笔者在项目实践中总结的关于Celery使用的一些最佳实践，欢迎补充 &lt;!--more--&gt; 写在前面 本最佳实践是基于作者有限的经验，欢迎大家共同讨论，可以持续维护此最佳实践。另本文中所使用的环境为Mac&amp;Ubuntu环境，软件版本如下： Celery (4.1.0) Flask (0.12.1) RabbitMQ(3.6.9) librabbitmq (1.6.1) 介绍 简单来说Celery是一个异步的任务队列，当我们需要将一些任务(比如一些需要长时间操作的任务)异步操作的时候，这时候Celery就可以帮到我们，另外Celery还支持定时任务(类似Crontab)。详细的介绍可以参考官网 使用RabbitMQ作为Broker RabbitMQ是官方推荐使用的Broker，它实际是一个消息中间件，负责消息的路由分发，安装RabbitMQ如下： 123# install on Ubuntuapt-get updateapt-get install rabbitmq-server -yq 需要注意的是，线上环境我们需要创建新的账号，并将guest账号删除，操作如下： 1234rabbitmqctl add_user myuser mypassword # 新增用户rabbitmqctl add_vhost myvhost # 新增vhost，以使用不同的命名空间rabbitmqctl set_permissions -p myvhost myuser &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; # 设置权限rabbitmqctl delete_user guest # 安全原因，删除guest 注意：vhost是一个虚拟空间，用于区分不同类型的消息 然后，在Celery的配置中配置broker URL 1CELERY_BROKER_URL = &apos;amqp://myuser:mypassword@localhost:5672/myvhost&apos; 注意：当使用amqp协议头时，如果安装有librabbitmq则使用librabbitmq，否则使用pyamqp Celery的日志输出 在task中想要输出日志，最好的方法是通过如下方式 1234567from celery.utils.log import get_task_loggerlg = get_task_logger(__name__)@celery.taskdef log_test(): lg.debug(&quot;in log_test()&quot;) 但是仅如此会发现所有的日志最后都跑到shell窗口的stdout当中，原来必须得在启动celery的时候使用-f option来指定输出文件，如下： 1celery -A main.celery worker -l debug -f log/celery/celery_task.log &amp; -A：指定celery实例 worker: 启动worker进程 -l：指定log level，这里指定log level为debug level -f：指定输出的日志文件 使用Redis作为backend 当使用Redis作为存储后端的时候，我们可以通过设置DB number来使得Celery的结果存储与其它数据存储隔离开来，比如在笔者的项目中，redis还用作缓存的存储后端，因此为了区分，Celery在使用Redis的时候使用的DB number是1（默认是0），关于Redis DB number可以参考这里. 因此我们的backend设置如下： 1CELERY_RESULT_BACKEND = &apos;redis://localhost:6379/1&apos; # 最后的数字1代表DB number 查看Celery任务的结果可以通过Redis-cli连接Redis数据库进行查看 123&gt; redis-cli&gt; select 1 # 这里选择DB 1， 也可以在使用redis-cli -n 1来进入指定的DB&gt; get key # 获取指定key对应的结果 调试代码 我认为此处是非常重要的一个技巧，即在调试代码的时候，我们可以将delay或者apply_async先去掉，直接调用worker的函数进行同步调试，调试成功后再加上delay或者apply_async method Celery可能会遇到的坑 Celery4.x版本使用librabbitmq的问题 Celery 4.x版本在使用librabbitmq时，会出现类似这样的错误 1Received and deleted unknown message. Wrong destination?!? 完整错误如图： 解决这个问题有两个方式： 推荐方式，更改配置项task_protocol为1。 Github上Robert Kopaczewski详细解释了这个问题，原文如下： Apparently librabbitmq issue is related to new default protocol in celery 4.x. You can switch to previous protocol version by either putting CELERY_TASK_PROTOCOL = 1 in your settings if you're using Django or settings app.conf.task_protocol = 1 in celeryconf.py. 另一种方式是不使用librabbitmq, 通过pip uninstall librabbitmq, 并且更改broker配置的协议头为'pyamqp',如下，也可以解决这个问题。 1BROKER_URL = &apos;pyamqp://guest:guest@localhost:5672/%2F&apos; 由于librabbitmq的性能优势，我们还是推荐方式1来解决该问题。 RabbitMQ远程连接问题 如果RabbitMQ与Celery不在同一台机器上，除在Celery配置的时候要将BROKER_URL设置为正确的IP地址外，还需要将Rabbitmq的配置文件/usr/local/etc/rabbitmq/rabbitmq-env.conf中的NODE_IP_ADDRESS更改为0.0.0.0 1NODE_IP_ADDRESS=0.0.0.0 Celery import问题 123456789101112131415The message has been ignored and discarded.Did you remember to import the module containing this task?Or maybe you&apos;re using relative imports?Please seehttp://docs.celeryq.org/en/latest/internals/protocol.htmlfor more information.The full contents of the message body was:&apos;\\x8e\\xa7expires\\xc0\\xa3utc\\xc3\\xa4args\\x91\\x85\\xa3tid\\xb85971a43d47f84bb278f77fc2\\xa3sen\\xa2A1\\xa2tt\\xa2ar\\xa2co\\xc4\\x00\\xa1t\\xa4like\\xa5chord\\xc0\\xa9callbacks\\xc0\\xa8errbacks\\xc0\\xa7taskset\\xc0\\xa2id\\xc4$c133dbf8-2c89-4311-b7cf-c377041058ec\\xa7retries\\x00\\xa4task\\xd9$tasks.messageTasks.send_like_message\\xa5group\\xc0\\xa9timelimit\\x92\\xc0\\xc0\\xa3eta\\xc0\\xa6kwargs\\x80&apos; (239b)Traceback (most recent call last): File &quot;/Users/liufeng/.pyenv/versions/2.7.13/envs/kaopu_backend/lib/python2.7/site-packages/celery/worker/consumer/consumer.py&quot;, line 561, in on_task_received strategy = strategies[type_]KeyError: u&apos;tasks.messageTasks.send_like_message&apos; 出现这条错误是由于我们的tasks跟celery并不是在同一个文件中，即不是同一个module，当我们通过如下命令启动task worker时，实际只加载了app module，而没有加载tasks相关的module 12celery -A app.celery worker -l info` 要解决这个问题，必须为celery配置文件添加import参数，如下 1app.config[&apos;imports&apos;] = [&apos;tasks.messageTasks&apos;] Celery unregistered task问题 在开发过程中遇到了这样一个问题 12345678910111213141516[2017-08-31 15:38:19,605: ERROR/MainProcess] Received unregistered task of type u&apos;app.tasks.messageTasks.send_follow_message&apos;.The message has been ignored and discarded.Did you remember to import the module containing this task?Or maybe you&apos;re using relative imports?Please seehttp://docs.celeryq.org/en/latest/internals/protocol.htmlfor more information.The full contents of the message body was:&apos;\\x8e\\xa7expires\\xc0\\xa3utc\\xc3\\xa4args\\x91\\x86\\xa6sender\\xa5Jenny\\xa9target_id\\xb859a5313847f84be534ad7d46\\xabtarget_type\\xa4user\\xa7content\\xc4\\x00\\xa8receiver\\xb859a5313847f84be534ad7d46\\xa4type\\xa6follow\\xa5chord\\xc0\\xa9callbacks\\xc0\\xa8errbacks\\xc0\\xa7taskset\\xc0\\xa2id\\xc4$a4d40c14-1976-41a6-a753-d2a495929920\\xa7retries\\x00\\xa4task\\xd9*app.tasks.messageTasks.send_follow_message\\xa5group\\xc0\\xa9timelimit\\x92\\xc0\\xc0\\xa3eta\\xc0\\xa6kwargs\\x80&apos; (312b)Traceback (most recent call last): File &quot;/Users/liufeng/.pyenv/versions/2.7.13/envs/kaopu_backend/lib/python2.7/site-packages/celery/worker/consumer/consumer.py&quot;, line 561, in on_task_received strategy = strategies[type_]KeyError: u&apos;app.tasks.messageTasks.send_follow_message&apos; 解决这个问题，最开始是根据提示，将所有涉及到task的module全部加上from __future__ import absolute_import 之后运行之后还是不行，后来发现是由于之前启动时使用的是app module， 但是我的代码已经改成了main.py，所以重新启动了celery，最后问题解决 使用镜像迁移系统也依然需要重新添加rabbitmq的用户 问题最开始是发现无法点赞，也无法Follow用户，通过http消息发现出现502错误，于是登录到服务器检查，发现应用服务本身没有任何报错，于是又去查看Celery的日志，结果发现出现如下错误： 1[2017-11-13 16:32:01,243: ERROR/MainProcess] consumer: Cannot connect to amqp://celeryuser:**@loc alhost:5672/celeryvhost: Couldn&apos;t log in: a socket error occurred. 经过一番搜索发现网上的评论主要是说URL不对的情况下会出现这种情况，但是我的URL没有改过啊，那又会是什么问题呢？继续看，发现有人提到了权限问题，于是又是一番检查，发现RabbitMQ中并没有原先设置的用户（我使用的是原系统的镜像，原以为用户也是已经设置好的） 12# 查看有哪些用户rabbitmqctl list_users 然后就简单了，按照步骤创建用户，vhost，再赋予权限，删除guest，然后就终于都连好了 另外，发现从镜像复制系统后，RabbitMQ并不能正常工作，必须杀掉原先的进程，重新启动 更改task的代码后，重启Celery 需要注意的是，在更改task的代码后，必须重新启动Celery，否则代码改动无法生效，可能导致一些意外的问题","categories":[],"tags":[]},{"title":"Python collections.deque vs. Queue.Queue vs. multiprocessing.Queue","slug":"Python-collections-deque-vs--Queue-Queue-vs--multiprocessing-Queue","date":"2018-02-12T10:18:18.000Z","updated":"2018-08-12T10:08:41.412Z","comments":true,"path":"2018/02/12/Python-collections-deque-vs--Queue-Queue-vs--multiprocessing-Queue/","link":"","permalink":"http://yoursite.com/2018/02/12/Python-collections-deque-vs--Queue-Queue-vs--multiprocessing-Queue/","excerpt":"","text":"总体上来说，当需要在进程间通信的时候需要使用multiprocessing.Queue; 当在同一个进程当中，而需要多线程之间通信的时候，可以使用Queue.Queue；而至于collections.deque一般就是在同一个线程当中，作为一种数据结构来使用的。下面分别讲述一下它们的用法： &lt;!--more--&gt; multiprocessing.Queue multiprocessing提供了两种进程间通信机制，一种是我们要说的Queue，另外一种是Pipe。而实际上Queue也是通过Pipe来实现的。具体可以参考进程间通信 Queue常用methods： Queue.qsize(): 返回queue中item的数量，注意这个数量并不准确, not reliable Queue.empty(): return True if queue is empty, not reliable Queue.full(): return True if queue is full, not reliable Queue.put(item[, block[, timeout]]): block表示是否阻塞，默认为True即阻塞，如果设定了timeout，则阻塞timeout时长，如果仍然没有空余的slot，则raise Queue.full exception。如果block=False，那么就不阻塞，当queue full时，直接报Queue.full exception。 Queue.put_nowait(item): Equivalent to put(item, False) Queue.get([block[, timeout]]) Queue.get_nowait(): Equivalent to get(False) 示例： 1234567891011from multiprocessing import Process, Queuedef f(q): q.put([42, None, &apos;hello&apos;])if __name__ == &apos;__main__&apos;: q = Queue() p = Process(target=f, args=(q,)) p.start() print q.get() # prints &quot;[42, None, &apos;hello&apos;]&quot; p.join() 需要注意的是Queue不提供join()和task_done()，因此在producer process中无法确保所有的task均已经被处理, 如果需要join and task_done就需要使用multiprocessing.JoinableQueue，详情参看JoinableQueue Queue.Queue Queue.Queue通常用于同一个进程中的不同线程间的通信，其提供的方法与multiprocessing.Queue类似，但是多出了两个methods如下： task_done(): 用于告知任务完成 join(): 用于等待队列中所有的任务完成。具体使用见下图 collections.deque 主要用于队列这种数据结构，通过append和popleft来实现队列的FIFO机制。常用方法如下： extendleft appendleft popleft extend append pop 具体参考官网 示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; d = deque(&apos;ghi&apos;) # make a new deque with three items&gt;&gt;&gt; for elem in d: # iterate over the deque&apos;s elements... print elem.upper()GHI&gt;&gt;&gt; d.append(&apos;j&apos;) # add a new entry to the right side&gt;&gt;&gt; d.appendleft(&apos;f&apos;) # add a new entry to the left side&gt;&gt;&gt; d # show the representation of the dequedeque([&apos;f&apos;, &apos;g&apos;, &apos;h&apos;, &apos;i&apos;, &apos;j&apos;])&gt;&gt;&gt; d.pop() # return and remove the rightmost item&apos;j&apos;&gt;&gt;&gt; d.popleft() # return and remove the leftmost item&apos;f&apos;&gt;&gt;&gt; list(d) # list the contents of the deque[&apos;g&apos;, &apos;h&apos;, &apos;i&apos;]&gt;&gt;&gt; d[0] # peek at leftmost item&apos;g&apos;&gt;&gt;&gt; d[-1] # peek at rightmost item&apos;i&apos;&gt;&gt;&gt; list(reversed(d)) # list the contents of a deque in reverse[&apos;i&apos;, &apos;h&apos;, &apos;g&apos;]&gt;&gt;&gt; &apos;h&apos; in d # search the dequeTrue&gt;&gt;&gt; d.extend(&apos;jkl&apos;) # add multiple elements at once&gt;&gt;&gt; ddeque([&apos;g&apos;, &apos;h&apos;, &apos;i&apos;, &apos;j&apos;, &apos;k&apos;, &apos;l&apos;])&gt;&gt;&gt; d.rotate(1) # right rotation&gt;&gt;&gt; ddeque([&apos;l&apos;, &apos;g&apos;, &apos;h&apos;, &apos;i&apos;, &apos;j&apos;, &apos;k&apos;])&gt;&gt;&gt; d.rotate(-1) # left rotation&gt;&gt;&gt; ddeque([&apos;g&apos;, &apos;h&apos;, &apos;i&apos;, &apos;j&apos;, &apos;k&apos;, &apos;l&apos;])&gt;&gt;&gt; deque(reversed(d)) # make a new deque in reverse orderdeque([&apos;l&apos;, &apos;k&apos;, &apos;j&apos;, &apos;i&apos;, &apos;h&apos;, &apos;g&apos;])&gt;&gt;&gt; d.clear() # empty the deque&gt;&gt;&gt; d.pop() # cannot pop from an empty dequeTraceback (most recent call last): File &quot;&lt;pyshell#6&gt;&quot;, line 1, in -toplevel- d.pop()IndexError: pop from an empty deque&gt;&gt;&gt; d.extendleft(&apos;abc&apos;) # extendleft() reverses the input order&gt;&gt;&gt; ddeque([&apos;c&apos;, &apos;b&apos;, &apos;a&apos;])","categories":[],"tags":[]},{"title":"MongoDB Sharding","slug":"MongoDB-Sharding","date":"2018-02-03T10:18:18.000Z","updated":"2018-08-12T10:07:28.933Z","comments":true,"path":"2018/02/03/MongoDB-Sharding/","link":"","permalink":"http://yoursite.com/2018/02/03/MongoDB-Sharding/","excerpt":"","text":"详细介绍MongoDB sharding &lt;!--more--&gt; 介绍 Sharding Key Sharding key必须是在对应的collection当中所有的文档都存在的field，比如我要sharding我的user collection，我就可以找nick_name这种所有documents中都存在的field来做为sharding key，当然还要考虑其它因素，但是这个是一个必要条件。 sharding key可以一个或者多个fields组成，还以user document为例，除了可以使用nick_name还可以加一个created time一起作为sharding key。当然具体使用什么field要根据系统的实际情况来综合考虑。 sharding只能有一个，而且设定后不可更改 针对已经有数据的collections做sharding时，原collections必须有以sharding key开头的索引；如果是空的collection，则MongoDB会自动创建对应的索引 Sharding的优势 读写效率 由于我们可以将相应的写操作分布在不同的shard上，可以更加均匀分担写压力，因此写的性能一般情况下会得到提高。 读取操作时，如果query中包含shard key对应的field，或者以这个field为prefix的compound index key 存储容量 由于是水平分割，因此几乎可以无限扩容，不需要担心某个collection过大而无法单机存储的问题 高可用 从MongoDB3.2开始，config server已经支持replicaset模式，这样的话sharding节点和config节点均可以通过replicaset来保证高可用。 关于Sharding的考虑 由于sharding的不可逆性，所以最好是先不使用sharding，在业务运行一段时间后，根据业务的情况来做决定，这主要需要考虑如下因素： 某个collection的数据是否非常大，达到未来可能一台机器的存储容量都无法解决该collection的存储。 是否有些collection存在性能问题，希望通过sharding来解决 由于具有不可逆性，我们可以只针对需要做sharding的collection做sharding，而其它collection不需要sharding。不做sharding的collection会存储于primary shard之上（所以可见这个shard应该有更大的容量） 针对需要分片的collection，需要确定满足以下几个条件： 选取的key可以使得collection的数据能均衡地分布在不同的shard上 写请求的时候也能均匀分布，比如时间戳作为key时，虽然也能将key均匀分布到不同的shard上，但是同一时间段，写请求就会持续在一个shard上，这样就不是均匀写了 我们针对该collection的查询主要是使用哪个field来作为查询条件，那么这个field最好是作为shard key 作为sharding key的field需要有索引，或者是作为compound索引的prefix。 尽量避免会导致jumbo chunk的key，比如通过age来作为key，同一个age的数据均会放到同一个chunk，这就可能导致chunk不断变大，当超过chunk size时，就会变成jumbo chunk Sharding策略 Sharding有两种策略：一种是ranged sharding; 另一种是hashed sharding。 如果选择的key，其值域非常接近，尤其是单调递增或者递减(increase or decrease monotonically, 也可以翻译成单向递增递减)，就会导致所有的新增操作都会集中在最大的range的shard上（如果是递增）；或者集中在最小range的那个shard上（如果是递减）。这就导致无法均匀地进行写操作（因为所有的写操作都集中在一个shard甚至一个chunk上）。而那个最大或最小range的shard将成为系统的瓶颈。具体参考shard-key-monotonic。在这种情况下，我们应当选择hashed sharding. hashed sharding。MongoDB通过hash算法(MD5)来计算key的hash值，然后将数据存储于对应范围的chunk上。由于算法的原因，即使非常&quot;近&quot;的key通过hash之后的值可能会有很大的差异，从而可以更加均匀的分布在不同的chunk之上。hashed sharding的缺点在于当我们的业务有很多范围查询时，（比如uid 5~20)，此时由于可能分布在很多不同的chunk上，所以就必须进行广播时操作，即要求所有的shard都进行查询，看是否有在这个范围内的数据，如果有就会交给mongos，mongos再负责将这些数据组合成一个完整的答案给客户端。这种情况下，势必会影响到查询性能。所以具体使用什么样的策略，需要综合考虑很多因素。 Splitting and Migration Splitting splitting就是当chunk中存储的数据量超过设定的chunk size时(默认64M)，将其分裂成两个chunk，具体参考chunk split Migration 数据迁移，是将某一个shard中的chunk数据迁移到另一个shard上。迁移有两种方式，一种是手动迁移，一种自动迁移。大部分时候，MongoDB会通过balancer进行自动迁移，以保持chunk在shard上能够均匀分布。手动迁移仅用于某些特定场景如bulk insert。 关于balancer balancer是一个后台进程，它会自动判断含有最多chunk的shard和最少chunk的shard之间的chunk数量差达到了设定的threshold，如果达到了，就会自动进行数据迁移。","categories":[],"tags":[]},{"title":"MongoDB的事务、ACID和一致性","slug":"MongoDB的事务、ACID和一致性","date":"2018-01-10T10:18:18.000Z","updated":"2018-08-12T10:08:00.765Z","comments":true,"path":"2018/01/10/MongoDB的事务、ACID和一致性/","link":"","permalink":"http://yoursite.com/2018/01/10/MongoDB的事务、ACID和一致性/","excerpt":"","text":"在前一篇《理解数据库的事务，ACID，CAP和一致性》我已经将数据库的一些基本概念包括事务，ACID，CAP，一致性，隔离性等都深入的介绍了一遍，而此篇主要是针对MongoDB数据库系统做一下深入的了解，主要希望弄清楚如下几个问题： &lt;!--more--&gt; MongoDB是如何实现事务的？有哪些局限？ MongoDB的一致性是如何保证的？ MongoDB的事务 首先我们需要知道MongoDB是有多种存储引擎的，不同的存储引擎在实现ACID的时候，使用不同的机制。而Mongodb从3.0开始默认使用的是WiredTiger引擎，本文后续所有文字均是针对WiredTiger引擎。 WiredTiger引擎可以针对单个文档来保证ACID特性，但是当需要操作多个文档的时候无法保证ACID，也即无法提供事务支持。但是，我们是否就无法实现事务呢？实际上，MongoDB本身虽然不支持跨文档的事务，但是我们依然可以可以在应用层来获取类似事务的支持。这其中有很多方式，MongoDB公司的Antoine Girbal曾经撰写过文章详细阐释了五种方式来支持事务，可以参考Reference中的链接。不过在此之前，让我们先了解下MongoDB在单文档上是如何实现ACID特性的。 单文档的ACID是如何实现的？ MongoDB在更新单个文档时，会对该文档加锁，而要理解MongoDB的锁机制，需要先了解以下几个概念： Intent Lock（我把它翻译为意图锁): 意图锁表明读写方(reader-writer)意图针对更细粒度的资源进行读取或写入操作。比如：如果当某个Collection被加了intent lock，那么说明读写方意图针对该Collection中的某个文档进行读或写的操作。如下图所示： 上图展示了当reader or writer需要操作文档时，相对更高的层级都需要加intent lock. Multiple granularity locking (我把它翻译为多粒度锁机制): MongoDB采用的是所谓的MGL多粒度锁机制，具体可以参考文末的wiki链接。简单来说就是结合了多种不同粒度的锁，包括S锁（Shared lock），X锁（Exclusive lock), IS锁(Intent Share lock), IX(Intent Exclusive lock)，这几种锁的互斥关系如下表所示： 下面，我用一个例子来简单说明下。假设我要更改name为Jim的document 1db.user_collection.update(&#123;&apos;name&apos;: &apos;Jim&apos;&#125;, &#123;$set: &#123;&apos;age&apos;: 26, &apos;score&apos;: 50&#125;&#125;) 此时，如图1所示，MongoDB会为name为Jim的document加上X锁，同时为包含该document的Collection，Database和instance都加上IX锁，这时，针对该文档的操作就保证了原子性。 需要注意的是： 如果当age修改成功，而score没有修改成功时，MongoDB会自动回滚，因此我们可以说针对单个文档，MongoDB是支持事务，保证ACID的（严格来说，要想保证Durability，需要在写操作时使用特殊的write concern，这个后边再谈） 所有的锁都是平等的，它们是排在一个队列里，符合FIFO原则。但是，MongoDB做了优化，即当一个锁被采用时，所有与它兼容的锁（即上表为yes的锁）都会被采纳，从而可以并发操作。举个例子，当你针对Collection A中的Document a使用S锁时，其它reader可以同时使用S锁来读取该Document a，也可以同时读取同一个Collection的Document b.因为所有的S锁都是兼容的。那么，如果此时针对Collection A中的Document c进行写操作是否可以呢？显然需要为Document c赋予x锁，此时Collection A就需要IX锁，而由于IX和IS是兼容的，所以没有问题。简单来说，只要不是同一个Document，读写操作是可以并发的；如果是同一个Document，读可以并发，但写不可以。 WiredTiger针对global, db, collection level只能使用intent lock。另外，针对冲突的情况，WiredTiger会自动重试。 跨文档的事务支持 前面已经说过，针对多文档，MongoDB是不支持事务的，但是我们的应用却可以自己去实现类事务的功能，这里只针对其中最常用的两步提交方式来做详细阐释。 假设我们有两个账户A和B，现在我们要让账户A转账100元给账户B，我们需要将整个过程放在一个事务当中，来保证数据的一致性。在这个应用模拟的事务当中，需要涉及两个Collection，一个是accounts collection，另一个是transaction collection（用于存储交易的信息和状态）。 先来看下transaction最终成功的大体流程：如图2所示 伪代码如下： initial accounts 123456789bulk_result = db.accounts.insert( [ &#123; _id: &quot;A&quot;, balance: 1000, pendingTransactions: [] &#125;, &#123; _id: &quot;B&quot;, balance: 1000, pendingTransactions: [] &#125; ])if bulk_result.nInserted != 2: print &quot;insert account failed.&quot; return False add a transaction 123456write_result = db.transactions.insert( &#123; _id: 1, source: &quot;A&quot;, destination: &quot;B&quot;, value: 100, state: &quot;initial&quot;, lastModified: new Date() &#125;)if write_result.nInserted != 1: print &quot;transaction failed&quot; return False update transaction to pending 1234567891011t = db.transactions.findOne( &#123; state: &quot;initial&quot; &#125; )result = db.transactions.update( &#123; _id: t._id, state: &quot;initial&quot; &#125;, &#123; $set: &#123; state: &quot;pending&quot; &#125;, $currentDate: &#123; lastModified: true &#125; &#125;)if result.nModified != 1: print &quot;transaction failed&quot; return False update accounts &amp; push transaction id 123456789101112result_source = db.accounts.update( &#123; _id: t.source, pendingTransactions: &#123; $ne: t._id &#125; &#125;, &#123; $inc: &#123; balance: -t.value &#125;, $push: &#123; pendingTransactions: t._id &#125; &#125;)result_destination = db.accounts.update( &#123; _id: t.destination, pendingTransactions: &#123; $ne: t._id &#125; &#125;, &#123; $inc: &#123; balance: t.value &#125;, $push: &#123; pendingTransactions: t._id &#125; &#125;)if result_source.nModified != 1 or result_destination.nModified != 1: # 进入回滚的流程 ... return False update transaction to applied 1234567891011result = db.transactions.update( &#123; _id: t._id, state: &quot;pending&quot; &#125;, &#123; $set: &#123; state: &quot;applied&quot; &#125;, $currentDate: &#123; lastModified: true &#125; &#125;)if result.nModified != 1: # 重新update accounts &amp; push transaction id # 注意：如果上一步是成功的，pendingTransactions列表中会有相应的Transaction，那么就不会重复更新账户 ... pull transaction id 1234567891011result_source = db.accounts.update( &#123; _id: t.source, pendingTransactions: t._id &#125;, &#123; $pull: &#123; pendingTransactions: t._id &#125; &#125;)result_destination = db.accounts.update( &#123; _id: t.destination, pendingTransactions: t._id &#125;, &#123; $pull: &#123; pendingTransactions: t._id &#125; &#125;)if result_source.nModified != 1 or result_destination.nModified != 1: # 重新执行pull transaction id ... update transaction to done 12345678910result = db.transactions.update( &#123; _id: t._id, state: &quot;applied&quot; &#125;, &#123; $set: &#123; state: &quot;done&quot; &#125;, $currentDate: &#123; lastModified: true &#125; &#125;)if result.nModified != 1: # 重新从pull transaction id执行 ... 包含回滚和失败的整体流程如图3： 从上图可以看出，任何一步失败都有相应的应对措施来保证事务或者执行完毕或者回滚。当然所有的实现都需要应用程序自己实现，更何况如果涉及多个应用并发的情况时，会更加复杂，如何保证多个事务不互相影响，又会进一步增加复杂度，这也就是为什么如果需要此类跨文档事务支持的时候推荐使用关系数据库。 MongoDB的一致性 当我们说外部一致性时，是针对分布式系统所说的CAP理论中的一致性，简单来说就是如何使得多台机器的副本保持一致，实际上Mongodb只能做到最终一致性，总会有“不一致时间窗口”，这是由于Mongodb在更新操作的时候，需要将同样的更新复制到副本节点当中，而这段时间无法保证reader读到的一定是最新数据，即使ReadConcern设置为majority，也只能保证返回目前大多数节点的所持有的数据，而不一定是最新的数据（比如，只有primary节点更新完成，其它所有secondary节点都还没有更新完成）。 当我们说内部一致性时，是针对ACID中的一致性，这里主要针对如何避免脏读，当Mongodb无法在大多数节点成功的更新操作时，会导致回滚操作，这时如果Reader已经读取了更改后的数据，就会产生脏读现象。而避免脏读，可以通过设置Read Concern和Write Concer来实现，关于Write Concern和Read Concern，请参考Reference中链接，已经讲的很详细了，不再赘述。当我们设置Read Concern为majority时，可以保证返回的数据是大多数节点所持有的数据，这种情况是不会发生回滚的，也就避免了脏读。还有一种情况可能出现脏读，就是当writer写数据时，虽然已经写入到了内存当中，但是并没有写入到磁盘中，这时reader读取到了更新后的数据，但当Mongodb将内存中的数据写入磁盘时可能会产生错误，从而导致磁盘写入失败，这时就可能导致该数据丢失，这种情况下也会产生脏读，而为了避免这种情况，我们需要在Write Concern设置的时候使用j:1，这样实际是在写入journal之后才返回写入成功，保证不会出现上述的脏读现象。当然这种情况下，性能势必会受到影响。所以还是要根据业务情况来决定，非关键业务不需要很强的一致性的情况下，也不需要此种设置。 Reference 五个解决方案让MongoDB拥有RDBMS的鲁棒性事务 Wiki: multiple granularity locking mongodb locking MongoDB 2 phase commits MongoDW Write Concern机制 MongoDB 写安全(Write Concern)","categories":[],"tags":[]},{"title":"理解数据库的事务，ACID，CAP和一致性","slug":"db_acid_cap","date":"2018-01-08T10:18:18.000Z","updated":"2018-08-12T10:11:02.288Z","comments":true,"path":"2018/01/08/db_acid_cap/","link":"","permalink":"http://yoursite.com/2018/01/08/db_acid_cap/","excerpt":"","text":"本文详细阐述了数据库常用的一些概念如事务、ACID、CAP等 &lt;!--more--&gt; 什么是事务 事务是指由一系列数据库操作组成的一个完整的逻辑过程，这个过程中的所有操作要么都成功，要么都不成功。比如：常见的例子就是银行转账的例子，一次转账操作会包含多个数据库操作，而这些数据库操作需要放到一个事务当中，保证其要么都成功，要么都不成功。 什么是ACID ACID是事务的四个特性，指的是atomicity，原子性；consistency，一致性；isolation，隔离性；durability，持久性。 原子性(atomicity): 指所有在事务中的操作要么都成功，要么都不成功，所有的操作都不可分割，没有中间状态。一旦某一步执行失败，就会全部回滚到初始状态。 一致性(consistency): 指的是逻辑上的一致性，即所有操作是符合现实当中的期望的。具体参考下一节 隔离性(isolation): 即不同事务之间的相互影响和隔离的程度。比如，不同的隔离级别，事务的并发程度也不同，最强的隔离状态是所有的事务都是串行化的（serializable）（即一个事务完成之后才能进行下一个事务），这样并发性也会降到最低，在保证了强一致性的情况下，性能也会受很大影响，所以在实际工程当中，往往会折中一下。 持久性(durability): 可以简单地理解为事务执行完毕后数据不可逆并持久化存储于存储系统当中 理解一致性 实际上我们通常说的数据库事务的一致性和分布式系统的一致性并不是一个概念。这里可以区分成“内部一致性”和“外部一致性”。“内部一致性”搞数据库的人很少这么说，一般就直接说一致性，更准确的说是“Consistency in ACID”（“事务 ACID 属性中的一致性”）；“外部一致性”是针对分布式系统而言的，分布式领域提及的 Consistency 表示系统的正确性模型，著名的也是臭名昭著的 CAP 理论中的 C 就是这个范畴的。这主要是由于分布式系统写入和读取都可能不在同一台机器上，而这必然会有一段时间导致不同机器上所存的数据不一致的情况，这就是所谓的“不一致时间窗口”。 内部一致性 要理解内部一致性也就是我们通常所说的ACID中的一致性，就必须从反面考虑什么情况下是不一致的。不一致的情况主要有以下几种情况： 修改丢失：丢失修改是事务A和B先后更改数据数据x（假设初始是x0)，但是在A未正式更改前，B已经读取了原先的数据x0，最后A更改后为x1，B更改的并不是A更新后的x1，而是更改的x0，更改后假设为x2，这时x2将x1覆盖了，相当于事务A针对x的更改丢失了。 脏读： 事务T1读取了T2更改的x，但是T2在实际存储数据时可能出错回滚了，这时T1读取的实际是无效的数据，这种情况下就是脏读 不可重复读：是说在T1读取x时，由于中间T2更改了x，所以T1前后两次读取的x值不相同，这就是所谓的不可重复读 幻读：在T1读取符合某个条件的所有记录时，T2增加了一条符合该条件的记录，这就导致T1执行过程中前后读取的记录可能不一致，即T2之后读取时会多出一条记录。 其中前三种（丢失修改、不能重复读、脏读）都是由于并发事务在修改同一份数据的时候导致的问题，此类问题可以通过对同一个资源加锁的方式来解决，而最后一种情况是由于不同事务并发时，新增数据导致的问题，对于新增的记录是无法加锁的，此种情况只能通过事务的串行化来解决。而串行化与并发是矛盾的，所以要在性能和事务的一致性强度上取得一个平衡，就涉及到不同的隔离等级，关于隔离等级，详见理解隔离性一节。 外部一致性 在分布式系统中我们所说的一致性，也就是外部一致性，通常会分为强一致性，弱一致性，还有最终一致性，而要理解外部一致性，需要对CAP理论（Consistency，Availability和Partition Tolerance）有所了解，关于CAP详见CAP定理一节。 强一致性：指系统中的某个数据被成功更新后，后续任何对该数据的读取操作都将得到更新后的值 弱一致性：弱一致性是相对于强一致性而言，它不保证总能得到最新的值； 最终一致性：是弱一致性的特殊形式，即保证在没有新的更新的条件下，经过一段“不一致时间窗口”，最终所有的访问都是最后更新的值。最常见的是DNS服务，更新域名指向的机器后，多级缓存要等到expiration time的时候才会更新，但是随着时间的推移，最终数据会趋于一致。 理解隔离性 事务的隔离级别从低到高有 读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable） Read Uncommitted：事务读数据时不会加锁，写数据时会有行级共享锁。假设事务1先于事务2，当事务1更新数据的时候，事务2可以读取事务1未提交的数据，但是不能更新事务1正在更新的数据。而如果事务1只是读数据，那么事务2既可以读数据，也可以更新数据。 这种情况下无法规避脏读，不可重复读的问题。 Read Committed：即在一个事务修改数据过程中，如果事务还没提交，其他事务不能读该数据，或者说只能读取committed的数据。事务读数据的瞬间会加行级共享锁，一旦读完该行，立即释放该行级共享锁；而写数据的瞬间会加行级排它锁，直到事务结束。这种情况下就避免了脏读，但是却不能避免不可重复读的问题 Repeatable Read：当然就再升一级，为的就是避免不可重复读的问题，所以名字叫repeatable read。怎么实现的呢，我们知道read committed是，事务读操作只在读的一瞬间加锁，读完这行就释放锁了，而repeatable read级别是读的一瞬间加锁，但是一直到事务结束才释放锁。但是repeatable read不能解决幻读的问题，因为幻读是增加记录，并不是更改原先的记录。 Serialization：到达这一级别的隔离，可以彻底解决一致性的所有问题。一般来说是通过加表锁来解决串行化的问题。 CAP定理 CAP理论主要是针对分布式存储系统的，C是指Consistency一致性，A是指Availability可用性，P是指Partition tolerance分区容忍性。CAP定理认为分布式系统中这三个特性最多只能同时满足两个特性。下面我们来分别看下这三个特性究竟是什么意思。 一致性(Consistency): 指在分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本） 可用性(Availability): 在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。（对数据更新具备高可用性） 分区容忍性(Partition tolerance): 即当节点之间无法正常通信时，就产生了分区，而分区产生后，依然能够保证服务可用，那么我们就说系统是分区容忍的。显然如果节点越多，且备份越多，分区容忍度就越高（因为即便是其中一个或多个节点挂了，仍然有其它节点和备份可用）。 那么，为什么说三个特性无法全部保证呢？首先，假如我们要保证分区容忍性，必然要做多个副本节点，而这必然会带来一致性的问题，即保证多个节点的数据是相同的，但是，要让多个节点数据相同，就必须要花时间去复制数据，这还是能够正常通信的情况下，那么在数据复制的过程中为了保持一致性，就不能对外提供服务，所以这段时间就无法满足可用性的问题。 实际工程通常会采取一些折中措施，比如并不保证强一致性，只保证最终一致性，什么意思呢？比如，有三个数据节点互为备份，某份数据在节点A更改后，需要将更改复制到节点B和C，假设复制过程中，有客户访问该数据，那么此时不保证是一致的，即访问A节点的用户得到的是最新数据，而访问B和C节点的用户得到是老数据，但是最终，数据会复制完成，所以最终A、B、C三个节点的数据是一致的。（比如像文章点赞这种数据，延迟下也没有关系啦） Reference 深入分析事务的隔离级别 如何理解数据库的内部一致性和外部一致性？","categories":[],"tags":[]},{"title":"Flask-Login详解","slug":"Flask-Login","date":"2017-10-12T10:18:18.000Z","updated":"2018-10-05T14:24:37.606Z","comments":true,"path":"2017/10/12/Flask-Login/","link":"","permalink":"http://yoursite.com/2017/10/12/Flask-Login/","excerpt":"","text":"本文主要通过源码分析Flask-Login插件，并详述其使用方法 &lt;!-- more --&gt; 关于Flask登录认证的详细过程请参见拙作&lt;&lt;使用Flask实现用户登陆认证的详细过程&gt;&gt;一文，而本文则偏重于详细介绍Flask-Login的原理，代码的解析。 首次登陆 我们首先来看一下首次登录验证的流程图： Flask-Login在登录过程中主要负责： 将用户对象存入request context中 将用户ID，Session ID等信息存入Session中 在&lt;&lt;使用Flask实现用户登陆认证的详细过程&gt;&gt;中我们已经介绍过如何通过Flask-Login来实现登录的过程，其中最重要的代码就是login_user，如下： 1login_user(user, remember=remember_me) 那么login_user具体做了什么呢？我们来看下源码 123456789101112131415def login_user(user, remember=False, force=False, fresh=True): if not force and not user.is_active: return False user_id = getattr(user, current_app.login_manager.id_attribute)() session['user_id'] = user_id session['_fresh'] = fresh session['_id'] = current_app.login_manager._session_identifier_generator() if remember: session['remember'] = 'set' _request_ctx_stack.top.user = user user_logged_in.send(current_app._get_current_object(), user=_get_user()) return True getattr(user, current_app.login_manager.id_attribute)() 这里login_manager.id_attribute是一个字符串'get_id'。因此这句的意思是获取User对象的get_id method，然后执行，从而获取到用户的ID 通过session['user_id'] = user_id来将用户的ID存储进Session当中，后面紧跟着将fresh信息，session id信息，remember信息存储进session。 注意：Flask的session是以cookie为基础，但是是在Server端使用secret key并使用AES之类的对称加密算法进行加密的，然后将加密后的cookie发送给客户端。由于是加密后的数据，客户端无法篡改数据，也无法获知session中的信息，只能保存该session信息，在之后的请求中携带该session信息 _request_ctx_stack.top.user = user这里是将user对象存储进当前的request context中，_request_ctx_stack是一个LocalStack对象，top属性指向的就是当前的request context。关于LocalStack及相关技术，请参考拙作&lt;&lt;Werkzeug(Flask)之Local、LocalStack和LocalProxy&gt;&gt; user_logged_in.send(current_app._get_current_object(), user=_get_user()) 此句中user_logged_in是Flask-Login定义的signal，此处通过send来发射此signal，当注册监听此signal的回调函数收到此signal之后就会执行函数。这里send有两个参数，第一个参数是sender对象，此处通过current_app._get_current_object()来获取当前的app对象，即此signal的sender设为当前的应用；第二个参数是该signal携带的数据，此处将user对象做为signal的数据传递给相应的回调函数。关于signal的详细解释请参考拙作&lt;&lt;Flask Signals详解&gt;&gt; 非首次登陆 非首次登陆流程图如下： 在这个流程图中，Flask-Login主要起如下作用： 从session中获取用户ID 当用户的请求访问的是受登录保护的路由时，就要通过用户ID重新load user，如果load user失败则进入鉴权失败处理流程，如果成功，则允许正常处理请求 那么Flask-Login究竟是如何保护路由的呢？我们来看个例子： 123456@app.route('/')@app.route('/main')@login_requireddef main(): return render_template( 'main.html', username=current_user.username) 我们看到只要给路由函数加一个@login_required装饰器就可以了，那么这个装饰器究竟是怎么做到的呢？来看下源码： 1234567891011121314151617# flask_login/utils.pydef login_required(func): @wraps(func) def decorated_view(*args, **kwargs): # 如果request method为例外method，即在EXEMPT_METHODS中的method，可以不必鉴权 if request.method in EXEMPT_METHODS: return func(*args, **kwargs) # 如果_login_disabled为True则不必鉴权 elif current_app.login_manager._login_disabled: return func(*args, **kwargs) # 正常鉴权 elif not current_user.is_authenticated: return current_app.login_manager.unauthorized() return func(*args, **kwargs) return decorated_view 默认情况下只有OPTIONS method在EXEMPT_METHODS set中，而GET、PUT、POST等常见的methods都需要鉴权 _login_disabled默认为False 正常鉴权的关键在于current_user.is_authenticated是否为True，为True则正常处理请求，为False则进入unauthorized处理流程。那么这个current_user到底怎么就能鉴权了？它是怎么来的呢？来看下定义： 12# flask_login/utils.pycurrent_user = LocalProxy(lambda: _get_user()) 原来current_user是一个LocalProxy对象，其代理的对象需要通过_get_user()来获取，简单来说_get_user()会返回两种用户，一种是正常的用户对象(鉴权成功)，一种是anonymous用户对象(鉴权失败)。而正常的用户对象其is_authenticated属性总是为True，相对的anonymous用户对象的is_authenticated属性总是为False LocalProxy对象每次操作都会重新获取代理的对象从而实现动态更新，关于LocalProxy的详细说明请参考拙作&lt;&lt;Werkzeug(Flask)之Local、LocalStack和LocalProxy&gt;&gt; 而要实现动态更新的关键就在于_get_user函数，接下来我们看下_get_user函数是如何获取user对象的： 123456# flask_login/utils.pydef _get_user(): if has_request_context() and not hasattr(_request_ctx_stack.top, 'user'): current_app.login_manager._load_user() return getattr(_request_ctx_stack.top, 'user', None) 在之前的首次登陆那小节中，我们已经知道用户鉴权成功后，会将User对象保存在当前的request context当中，这时我们调用_get_user函数时就会直接从request context中获取user对象return getattr(_request_ctx_stack.top, 'user', None) 但如果是非首次登陆，当前request context中并没有保存user对象，就需要调用current_app.login_manager._load_user()来去load user对象，接下来再看看如何去load： 1234567891011121314151617181920212223242526272829303132# flask_login/login_manager.pydef _load_user(self): '''Loads user from session or remember_me cookie as applicable''' user_accessed.send(current_app._get_current_object()) # first check SESSION_PROTECTION config = current_app.config if config.get('SESSION_PROTECTION', self.session_protection): deleted = self._session_protection() if deleted: return self.reload_user() # If a remember cookie is set, and the session is not, move the # cookie user ID to the session. # # However, the session may have been set if the user has been # logged out on this request, 'remember' would be set to clear, # so we should check for that and not restore the session. is_missing_user_id = 'user_id' not in session if is_missing_user_id: cookie_name = config.get('REMEMBER_COOKIE_NAME', COOKIE_NAME) header_name = config.get('AUTH_HEADER_NAME', AUTH_HEADER_NAME) has_cookie = (cookie_name in request.cookies and session.get('remember') != 'clear') if has_cookie: return self._load_from_cookie(request.cookies[cookie_name]) elif self.request_callback: return self._load_from_request(request) elif header_name in request.headers: return self._load_from_header(request.headers[header_name]) return self.reload_user() _load_user大体的过程是首先检查SESSION_PROTECTION设置，如果SESSION_PROTECTION 为strong或者basic类型，那么就会执行_session_protection()动作，否则不执行此操作。_session_protection在session_id不一致的时候(比如IP变化会导致session id的变化)才真正有用，这时，如果为basic类型或者session permanent为True时，只标注session为非新鲜的(not fresh)；而如果为strong，则会删除session中的用户信息，并重新load user，即调用reload_user。 session permanent为True时，用户退出浏览器不会删除session，其会保留permanent_session_lifetime s(默认是31天)，但是当其为False且SESSION_PROTECTION 设为strong时，用户的session就会被删除。 接下来的代码是说当session中没有用户信息时(这里通过是否能获取到user_id来判断)，如果有则直接reload_user,如果没有，则有三种方式来load user，一种是通过remember cookie，一种通过request，一种是通过request header，依次尝试。 remember cookie是指，当用户勾选'remember me'复选框时，Flask-Login会将用户信息放入到指定的cookie当中，同样也是加密的。这就是为什么当session中没有携带用户信息时，我们可以通过remember cookie来获取用户的信息 而reload_user是如何获取用户的呢，来看下源代码： 1234567891011121314151617181920212223# flask_login/login_manager.pydef reload_user(self, user=None): ctx = _request_ctx_stack.top if user is None: user_id = session.get('user_id') if user_id is None: # 当无法获取到有效的用户id时，就认为是anonymous user ctx.user = self.anonymous_user() else: # user callback就是我们通过@login_manager.user_loader装饰的函数，用于获取user object if self.user_callback is None: raise Exception( \"No user_loader has been installed for this \" \"LoginManager. Add one with the \" \"'LoginManager.user_loader' decorator.\") user = self.user_callback(user_id) if user is None: ctx.user = self.anonymous_user() else: ctx.user = user else: ctx.user = user 首先获取user id，如果获取不到有效的id，就将user设为anonymous user 获取到id后，再通过@login_manager.user_loader装饰的函数获取到user对象，如果没有获取到有效的user对象，就认为是anonymous user 最后将user保存于request context中（无论是正常的用户还是anonymous用户） 至此，我们已经将Flask-Login的核心代码剖析了一遍，如果你有收获，不妨点个赞鼓励一下吧！","categories":[],"tags":[{"name":"flask","slug":"flask","permalink":"http://yoursite.com/tags/flask/"}]},{"title":"Werkzeug(Flask)之Local、LocalStack和LocalProxy","slug":"Werkzeug-Local","date":"2017-10-08T10:18:18.000Z","updated":"2018-10-05T14:16:01.368Z","comments":true,"path":"2017/10/08/Werkzeug-Local/","link":"","permalink":"http://yoursite.com/2017/10/08/Werkzeug-Local/","excerpt":"","text":"在我们使用Flask以及Werkzeug框架的过程中，经常会遇到如下三个概念：Local、LocalStack和LocalProxy。尤其在学习Flask的Request Context和App Context的过程中，这几个概念出现的更加频繁，另外很多Flask插件都会使用这三个概念对应的技术。那么这三个东西到底是什么？我们为什么需要它们？以及如何使用呢？本篇文章主要就是来解答这些问题。 Local 这部分我们重点介绍Local概念，主要分为以下几个部分： 为什么需要Local？ Local的使用 Local的实现 为什么需要Local？ 在Python的标准库中提供了thread local对象用于存储thread-safe和thread-specific的数据，通过这种方式存储的数据只在本线程中有效，而对于其它线程则不可见。正是基于这样的特性，我们可以把针对线程全局的数据存储进thread local对象，举个简单的例子 12345&gt;&gt;from threading import local&gt;&gt;thread_local_data = local()&gt;&gt;thread_local_data.user_name=\"Jim\"&gt;&gt;thread_local_data.user_name'Jim' 使用thread local对象虽然可以基于线程存储全局变量，但是在Web应用中可能会存在如下问题： 有些应用使用的是greenlet协程，这种情况下无法保证协程之间数据的隔离，因为不同的协程可以在同一个线程当中。 即使使用的是线程，WSGI应用也无法保证每个http请求使用的都是不同的线程，因为后一个http请求可能使用的是之前的http请求的线程，这样的话存储于thread local中的数据可能是之前残留的数据。 为了解决上述问题，Werkzeug开发了自己的local对象，这也是为什么我们需要Werkzeug的local对象 Local的使用 先举一个简单的示例： 1234567891011from werkzeug.local import Local, LocalManagerlocal = Local()local_manager = LocalManager([local])def application(environ, start_response): local.request = request = Request(environ) ...# make_middleware会确保当request结束时，所有存储于local中的对象的reference被清除application = local_manager.make_middleware(application) 首先Local对象需要通过LocalManager来管理，初次生成LocalManager对象需要传一个list类型的参数，list中是Local对象，当有新的Local对象时，可以通过local_manager.locals.append()来添加。而当LocalManager对象清理的时候会将所有存储于locals中的当前context的数据都清理掉 上例中当local.request被赋值之后，其可以在当前context中作为全局数据使用 所谓当前context(the same context)意味着是在同一个greenlet(如果有)中，也就肯定是在同一个线程当中 那么Werkzeug的Local对象是如何实现这种在相同的context环境下保证数据的全局性和隔离性的呢？ Local的实现 我们先来看下源代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 在有greenlet的情况下，get_indent实际获取的是greenlet的id，而没有greenlet的情况下获取的是thread idtry: from greenlet import getcurrent as get_identexcept ImportError: try: from thread import get_ident except ImportError: from _thread import get_identclass Local(object): __slots__ = ('__storage__', '__ident_func__') def __init__(self): object.__setattr__(self, '__storage__', &#123;&#125;) object.__setattr__(self, '__ident_func__', get_ident) def __iter__(self): return iter(self.__storage__.items()) # 当调用Local对象时，返回对应的LocalProxy def __call__(self, proxy): \"\"\"Create a proxy for a name.\"\"\" return LocalProxy(self, proxy) # Local类中特有的method，用于清空greenlet id或线程id对应的dict数据 def __release_local__(self): self.__storage__.pop(self.__ident_func__(), None) def __getattr__(self, name): try: return self.__storage__[self.__ident_func__()][name] except KeyError: raise AttributeError(name) def __setattr__(self, name, value): ident = self.__ident_func__() storage = self.__storage__ try: storage[ident][name] = value except KeyError: storage[ident] = &#123;name: value&#125; def __delattr__(self, name): try: del self.__storage__[self.__ident_func__()][name] except KeyError: raise AttributeError(name) 这段代码实际是对__storage__ dict的封装，而这个dict中的key使用的就是get_indent函数获取的id（当有greenlet时使用greenlet id，没有则使用thread id） __storage__ dict中的value也是一个dict，这个dict就是该greenlet(或者线程)对应的local存储空间 通过重新实现__getattr__, __setattr__等魔术方法，我们在greenlet或者线程中使用local对象时，实际会自动获取greenlet id(或者线程id)，从而获取到对应的dict存储空间，再通过name key就可以获取到真正的存储的对象 当我们需要释放local数据的内存时，可以通过调用release_local()函数来释放当前context的local数据，如下 12345&gt;&gt;&gt; loc = Local()&gt;&gt;&gt; loc.foo = 42&gt;&gt;&gt; release_local(loc) # release_local实际调用local对象的__release_local__ method&gt;&gt;&gt; hasattr(loc, 'foo')False LocalStack LocalStack与Local对象类似，都是可以基于Greenlet协程或者线程进行全局存储的存储空间(实际LocalStack是对Local进行了二次封装），区别在于其数据结构是栈的形式。示例如下： 1234567891011&gt;&gt;&gt; ls = LocalStack()&gt;&gt;&gt; ls.push(42)&gt;&gt;&gt; ls.top42&gt;&gt;&gt; ls.push(23)&gt;&gt;&gt; ls.top23&gt;&gt;&gt; ls.pop()23&gt;&gt;&gt; ls.top42 从示例看出Local对象存储的时候是类似字典的方式，需要有key和value，而LocalStack是基于栈的，通过push和pop来存储和弹出数据 另外，当我们想释放存储空间的时候，也可以调用release_local() LocalStack在Flask框架中会频繁的出现，其Request Context和App Context的实现都是基于LocalStack，具体可以参考Github上的Flask源码 LocalProxy LocalProxy用于代理Local对象和LocalStack对象，而所谓代理就是作为中间的代理人来处理所有针对被代理对象的操作，如下图所示： 接下来我们将重点讲下如下内容： LocalProxy的使用 LocalProxy代码解析 为什么要使用LocalProxy LocalProxy的使用 初始化LocalProxy有三种方式： 通过Local或者LocalStack对象的__call__ method 12345678910111213from werkzeug.local import Locall = Local()# these are proxiesrequest = l('request')user = l('user')from werkzeug.local import LocalStack_response_local = LocalStack()# this is a proxyresponse = _response_local() 上述代码直接将对象像函数一样调用，这是因为Local和LocalStack都实现了__call__ method，这样其对象就是callable的，因此当我们将对象作为函数调用时，实际调用的是__call__ method，可以看下本文开头部分的Local的源代码，会发现__call__ method会返回一个LocalProxy对象 通过LocalProxy类进行初始化 12l = Local()request = LocalProxy(l, 'request') 实际上这段代码跟第一种方式是等价的，但这种方式是最'原始'的方式，我们在Local的源代码实现中看到其__call__ method就是通过这种方式生成LocalProxy的 使用callable对象作为参数 1request = LocalProxy(get_current_request()) 通过传递一个函数，我们可以自定义如何返回Local或LocalStack对象 那么LocalProxy是如何实现这种代理的呢？接下来看下源码解析 LocalProxy代码解析 下面截取LocalProxy的部分代码，我们来进行解析 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# LocalProxy部分代码@implements_boolclass LocalProxy(object): __slots__ = ('__local', '__dict__', '__name__', '__wrapped__') def __init__(self, local, name=None): object.__setattr__(self, '_LocalProxy__local', local) object.__setattr__(self, '__name__', name) if callable(local) and not hasattr(local, '__release_local__'): # \"local\" is a callable that is not an instance of Local or # LocalManager: mark it as a wrapped function. object.__setattr__(self, '__wrapped__', local) def _get_current_object(self): \"\"\"Return the current object. This is useful if you want the real object behind the proxy at a time for performance reasons or because you want to pass the object into a different context. \"\"\" # 由于所有Local或LocalStack对象都有__release_local__ method, 所以如果没有该属性就表明self.__local为callable对象 if not hasattr(self.__local, '__release_local__'): return self.__local() try: # 此处self.__local为Local或LocalStack对象 return getattr(self.__local, self.__name__) except AttributeError: raise RuntimeError('no object bound to %s' % self.__name__) @property def __dict__(self): try: return self._get_current_object().__dict__ except RuntimeError: raise AttributeError('__dict__') def __getattr__(self, name): if name == '__members__': return dir(self._get_current_object()) return getattr(self._get_current_object(), name) def __setitem__(self, key, value): self._get_current_object()[key] = value def __delitem__(self, key): del self._get_current_object()[key] if PY2: __getslice__ = lambda x, i, j: x._get_current_object()[i:j] def __setslice__(self, i, j, seq): self._get_current_object()[i:j] = seq def __delslice__(self, i, j): del self._get_current_object()[i:j] # 截取部分操作符代码 __setattr__ = lambda x, n, v: setattr(x._get_current_object(), n, v) __delattr__ = lambda x, n: delattr(x._get_current_object(), n) __str__ = lambda x: str(x._get_current_object()) __lt__ = lambda x, o: x._get_current_object() &lt; o __le__ = lambda x, o: x._get_current_object() &lt;= o __eq__ = lambda x, o: x._get_current_object() == o 首先在__init__ method中传递的local参数会被赋予属性_LocalProxy__local,该属性可以通过self.local进行访问，关于这一点可以看StackOverflow的问题回答 LocalProxy通过_get_current_object来获取代理的对象。需要注意的是当初始化参数为callable对象时，则直接调用以返回Local或LocalStack对象，具体看源代码的注释。 重载了绝大多数操作符，以便在调用LocalProxy的相应操作时，通过_get_current_object method来获取真正代理的对象，然后再进行相应操作 为什么要使用LocalProxy 可是说了这么多，为什么一定要用proxy，而不能直接调用Local或LocalStack对象呢？这主要是在有多个可供调用的对象的时候会出现问题，如下图： 我们再通过下面的代码也许可以看出一二： 123456789101112131415# use Local object directlyfrom werkzeug.local import LocalStackuser_stack = LocalStack()user_stack.push(&#123;'name': 'Bob'&#125;)user_stack.push(&#123;'name': 'John'&#125;)def get_user(): # do something to get User object and return it return user_stack.pop()# 直接调用函数获取user对象user = get_user()print user['name']print user['name'] 打印结果是： 12JohnJohn 再看下使用LocalProxy 1234567891011121314# use LocalProxyfrom werkzeug.local import LocalStack, LocalProxyuser_stack = LocalStack()user_stack.push(&#123;'name': 'Bob'&#125;)user_stack.push(&#123;'name': 'John'&#125;)def get_user(): # do something to get User object and return it return user_stack.pop()# 通过LocalProxy使用user对象user = LocalProxy(get_user)print user['name']print user['name'] 打印结果是： 12JohnBob 怎么样，看出区别了吧，直接使用LocalStack对象，user一旦赋值就无法再动态更新了，而使用Proxy，每次调用操作符(这里[]操作符用于获取属性)，都会重新获取user，从而实现了动态更新user的效果。见下图： Flask以及Flask的插件很多时候都需要这种动态更新的效果，因此LocalProxy就会非常有用了。 至此，我们针对Local、LocalStack和LocalProxy的概念已经做了详细阐释，如果你觉得文章对你有帮助，不妨点个赞吧！","categories":[],"tags":[{"name":"flask","slug":"flask","permalink":"http://yoursite.com/tags/flask/"},{"name":"local","slug":"local","permalink":"http://yoursite.com/tags/local/"},{"name":"localstack","slug":"localstack","permalink":"http://yoursite.com/tags/localstack/"},{"name":"localproxy","slug":"localproxy","permalink":"http://yoursite.com/tags/localproxy/"},{"name":"werkzeug","slug":"werkzeug","permalink":"http://yoursite.com/tags/werkzeug/"}]},{"title":"Flask Signals 详解","slug":"Flask-Signals","date":"2017-10-05T14:22:18.000Z","updated":"2018-08-12T10:08:57.286Z","comments":true,"path":"2017/10/05/Flask-Signals/","link":"","permalink":"http://yoursite.com/2017/10/05/Flask-Signals/","excerpt":"","text":"Flask Signals和操作系统的signals系统很类似，都是通过信号（也可以说是事件 event）来通知已经注册的回调函数，让回调函数自动开始执行。Flask定义了自己 的一套核心signals和对应的functions(用于发起消息，注册回调函数)，我们需要 定义自己的回调函数，然后注册到对应的signal，这样就可以在收到该信号的时候 自动执行我们定义的回调函数。 &lt;!--more--&gt; Flask Signals简介 Flask Signals和操作系统的signals系统很类似，都是通过信号（也可以说是事件event）来通知已经注册的回调函数，让回调函数自动开始执行。Flask定义了自己的一套核心signals和对应的functions(用于发起消息，注册回调函数)，我们需要定义自己的回调函数，然后注册到对应的signal，这样就可以在收到该信号的时候自动执行我们定义的回调函数。 什么情况下需要使用Signals? 当我们需要使用观察者模式来解耦模块之间的信息传递的时候，Signals系统就可以帮助我们轻松达到目的。观察者模式如下图(图片来自voidcn) 与Hook函数的区别 试想，当我们需要监听某个事件，当它发生的时候，需要执行一系列functions，来实现诸如log记录等功能时，我们就可以使用Signals系统来实现，但是这里有一个疑问就是这个功能通过hook函数似乎也可以实现，比如通过before_request decorator实现记录日志的功能和使用request_started来记录日志就非常相似， 如下代码所示： 1234567891011121314151617181920#!/usr/bin/env python# -*- coding: utf-8 -*-from flask import Flask, request, request_startedapp = Flask(__name__)@app.before_requestdef print_url_in_hook(): print &quot;in hook, url: %s&quot; % request.url@app.route(&quot;/&quot;)def hello(): return &quot;Hello, World!&quot;def print_url_in_signal_subscriber(sender, **extra): print &quot;in signal subscriber, url: %s&quot; % request.urlif __name__ == &quot;__main__&quot;: request_started.connect(print_url_in_signal_subscriber, app) app.run() 当收到http请求后，打印如下： 123in signal subscriber, url: http://localhost:5000/in hook, url: http://localhost:5000/127.0.0.1 - - [05/Oct/2017 16:57:20] &quot;GET / HTTP/1.1&quot; 200 - 那么到底什么情况下使用signal，什么情况下使用hook函数呢？我们来看下它们的主要区别： signal的callback函数是无顺序的，而hook函数的执行是按照定义的顺序执行的。（这一点虽然是官网提出的区别，但是实际测试发现signal执行实际是按照注册的顺序执行的，即先通过connect进行注册的回调函数会先被执行） signal无法直接abort这个request请求，相比较在hook函数中可以直接abort request，即直接返回response给客户端，而无需再执行后续的操作。 signal可以通过参数携带数据，而hook函数通常不会携带额外的参数 与RabbitMQ等消息中间件的区别 Rabbitmq与signals都支持观察者模式，但是它们的区别也是很明显的： Rabbitmq之类的消息中间件更加重量级，提供更多功能，如分布式部署，消息存储备份等功能，而signal系统显然更加轻量级，只提供简单的消息分发功能 Rabbitmq之类的消息中间件可以在不同的系统间传递消息，从而使得不同的功能模块可以使用不同的语言进行开发，而signal系统显然仅限于Flask系统中使用 显然，signal系统使用局限性更大，但也更加轻量级，在只是简单的进行消息分发的系统中，使用signal更加简单方便 怎么使用Signals? Flask提供的signal机制优先使用blinker提供的库，但当blinker没有安装的时候，Flask也可以回退到使用自己的库。但是鉴于官网推荐使用blinker，所以我们最好还是安装blinker。 使用blinker 安装blinker 1pip install blinker 测试Flask signal是否使用blinker 1234In [1]: from flask import signalsIn [2]: signals.signals_availableOut[2]: True 当signals.signals_available返回True时，说明使用的是Blinker库 使用Flask Built-in signals Flask内置有多个signals可以直接使用，这些signals会自动emit(发射)，我们只需要定义自己的回调函数，然后通过connect方式来subscribe我们定义的函数到对应的signal即可监听该signal 下表展示了Flask内置的Signals，详细请参考Flask built-in signals: Signals 说明 template_rendered 当template被成功渲染之后会触发 before_render_template 当template被渲染之前会触发 request_started 当request context建立好之后，并在request被处理之前 request_finished 当发送response给客户端之后被触发 got_request_exception 当request处理过程中发生异常时，该signal会被触发，它甚至早于程序中的异常处理 request_tearing_down 当request tear down的时候触发，无论何种情况该signal都会被触发，即使发生异常 appcontext_tearing_down 当应用的context tear down的时候触发 appcontext_pushed 当应用的context被push时触发 appcontext_popped 当应用的context被pop时触发 message_flashed 当应用发送flash message时触发 之前的例子我们已经看到如何使用request_started signal了，这里需要说明两点： 在定义回调函数时，第一个参数必须是sender对象（即发送该signal的对象），第二个参数**extra用于接受额外的参数，也防止将来Flask在发送signal时添加新的参数。 使用connect注册回调函数时，第一个参数是回调函数，这个是必须的，第二参数是sender对象，是可选的，但最佳实践是要明确发送该signal的对象 另外，我们也可以临时性注册一个回调函数，这个尤其在进行单元测试时非常有用，因为我们不想在实际程序中添加测试相关的回调函数，因此需要一种机制在测试完成后，再取消注册该回调函数，有两种方式可以此种临时注册的机制： 一种是通过contextmanagerdecorator和disconnect函数一起来实现，如下： 123456789101112131415from flask import template_renderedfrom contextlib import contextmanager@contextmanagerdef captured_templates(app): recorded = [] def record(sender, template, context, **extra): recorded.append((template, context)) # 当使用with关键字进入with context时，自动注册record函数到template_rendered signal template_rendered.connect(record, app) try: yield recorded finally: # with context结束时会自动调用disconnect函数来解除注册 template_rendered.disconnect(record, app) 使用时代码如下： 1234567with captured_templates(app) as templates: rv = app.test_client().get(&apos;/&apos;) assert rv.status_code == 200 assert len(templates) == 1 template, context = templates[0] assert template.name == &apos;index.html&apos; assert len(context[&apos;items&apos;]) == 10 另外一种方式是使用connect_to函数 123456from flask import template_rendereddef captured_templates(app, recorded, **extra): def record(sender, template, context): recorded.append((template, context)) return template_rendered.connected_to(record, app) 使用时代码如下： 1234templates = []with captured_templates(app, templates, **extra): ... template, context = templates[0] 自定义signals的使用 自定义signal 当我们需要自定义signal时，我们可以直接使用blinker库 首先定义一个namespace 12from blinker import Namespacemy_signals = Namespace() 使用我们自定义的namespace定义自己的signal 1upload_image_finished = my_signals.signal(&apos;upload_image_finished&apos;) 至此，我们就定义了一个signal，名为upload_image_finished 发射自定义signal 1234567from flask import current_appdef upload_image(image_path, upload_url): # upload image code ... # after upload image upload_image_finished.send(current_app._get_current_object()) 当在类的method中使用send函数发射signal时，我们可以选择该类的对象作为sender对象，因此直接使用self作为参数，但是当我们不是在类的method当中，或者我们想让应用对象作为sender，那么我们就需使用如上代码所示的current_app._get_current_object()来获取应用对象 使用sender时，第一个参数是sender对象，是必选的。其余实际我们还可以传递更多参数(记得我们的callback函数使用了**extra), 这样的话我们实际就拥有了传递更多数据的能力。 注册回调函数的简化写法 从文章的第一个示例可以看出我们需要通过调用connect函数来对回调函数进行注册, 其实还有一个简化的写法可以把回调函数的定义和注册过程结合在一起，如下： 12345from flask import template_rendered@template_rendered.connect_via(app)def when_template_rendered(sender, template, context, **extra): print &apos;Template %s is rendered with %s&apos; % (template.name, context) 通过connect_via装饰器来简化回调函数定义和注册的过程 Reference Flask Signals Blinker Doc","categories":[],"tags":[]},{"title":"Flask Restful API权限管理设计与实现","slug":"flask_restful_api","date":"2017-08-10T08:21:18.000Z","updated":"2018-08-12T05:50:21.523Z","comments":true,"path":"2017/08/10/flask_restful_api/","link":"","permalink":"http://yoursite.com/2017/08/10/flask_restful_api/","excerpt":"","text":"在使用flask设计restful api的时候，有一个很重要的问题就是如何进行权限管理，以及如何进行角色的定义，在网上找了一下没有发现有类似的资料，虽然有些针对网站进行的权限管理设计，但是跟restful api接口的权限管理还是有很多不同的，于是乎自己动手，丰衣足食。为方便后来者，特撰此文！ 权限的设计 从本质上思考，我需要为每个API接口设定相应的权限，所以针对API的权限列表跟普通网站的权限设计是不同的，普通网站的权限设计是针对某个功能，比如是否可以comment功能，通常的权限定义如下： 123456class Permission: &quot;&quot;&quot; 权限表 &quot;&quot;&quot; COMMENT = 0x01 # 评论 MODERATE_COMMENT = 0x02 # 移除评论 但是针对restful api，我们更希望权限是针对我们的api接口，而restful api接口是跟我们路由的endpoint以及http method相关的，所以我们的权限设计应该是类似如下示例中的样子： 123# 这里comments是路由的endpoint，接口在判断用户是否有权限的时候# 可以先获取到endpoint和http method，然后就可以查看其是否有权限comment_permission = &#123;&quot;comments&quot;: &#123;&quot;post&quot;: True, &quot;get&quot;: True, &quot;delete&quot;: False&#125;&#125; 角色的设计 通常，我们在做网站的角色设计时会将角色存储在数据库当中，并会通过或运算(|)赋予角色以特定权限，如下： 1234567891011121314151617181920212223242526272829303132333435class Role(db.Model): &quot;&quot;&quot; 用户角色 &quot;&quot;&quot; id = db.Column(db.Integer, primary_key=True) # 该用户角色名称 name = db.Column(db.String(164)) # 该用户角色是否为默认 default = db.Column(db.Boolean, default=False, index=True) # 该用户角色对应的权限 permissions = db.Column(db.Integer) # 该用户角色和用户的关系 # 角色为该用户角色的所有用户 users = db.relationship(&apos;User&apos;, backref=&apos;role&apos;, lazy=&apos;dynamic&apos;) @staticmethod def insert_roles(): &quot;&quot;&quot; 创建用户角色 &quot;&quot;&quot; roles = &#123; # 定义了两个用户角色(User, Admin) &apos;User&apos;: (Permission.COMMENT, True), &apos;Admin&apos;: (Permission.COMMENT | Permission.MODERATE_COMMENT, False) &#125; for r in roles: role = Role.query.filter_by(name=r).first() if role is None: # 如果用户角色没有创建: 创建用户角色 role = Role(name=r) role.permissions = roles[r][0] role.default = roles[r][1] db.session.add(role) db.session.commit() 这里其实我一直没有搞明白，为什么要将角色存储于数据库当中，在我看来这只会导致更多的I/O操作从而影响系统的性能，因此我在设计角色的时候根本没有考虑存储到数据库中，角色的数据结构在系统运行时，直接存在内存当中，这样在接口调用时，可以直接使用角色相关的数据结构。而且由于我们的权限设计也不太相同，所以我针对restful api设计的Role如下： 123456789101112131415161718USER = 1ADMIN = 2VISITOR = 3Role = &#123; USER: &#123; &quot;comment&quot;: &#123;&quot;post&quot;: True, &quot;patch&quot;: True, &quot;get&quot;: True, &quot;delete&quot;: True&#125;, &quot;share&quot;: &#123;&quot;post&quot;: True&#125; &#125;, ADMIN: &#123; &quot;comment&quot;: &#123;&quot;post&quot;: True, &quot;patch&quot;: True, &quot;get&quot;: True, &quot;delete&quot;: True&#125;, &quot;share&quot;: &#123;&quot;post&quot;: True&#125; &#125;, VISITOR: &#123; &quot;comment&quot;: &#123;&quot;get&quot;: True&#125;, &quot;share&quot;: &#123;&quot;post&quot;: True&#125; &#125;&#125; 用户可以被赋予特定的role，如下： 1userA = &#123;&quot;name&quot;: &quot;John&quot;, &quot;role&quot;: USER&#125; 那么接口如何判断用户是否有权限访问呢？ 首先用户访问接口时都会带有用户信息，restful api一般是通过token来表明身份，系统通过token来获取用户的信息，比如用户名，然后我们可以通过用户名来获取用户的角色role，假设我们访问的接口是comments endpoint的post接口，那么就可以如下判断： 12345678910111213def access_control(user): &quot;&quot;&quot;判断用户是否有访问权限，有就返回True，没有返回False&quot;&quot;&quot; # 首先要获取到API的endpoint和http method，此处代码省略 ... role = user.get(&apos;role&apos;, VISITOR) try: if not Role[role][endpoint][http_method]: return False return True except KeyError: return False 由于基本所有的接口都需要access control，那么我们把上边的代码稍作改变，让它成为一个decorator，同时，user信息也可以直接获取而不需要从参数传递，如下： 1234567891011121314151617181920212223from functools import wrapsdef get_role(): # 这里get_resource_by_name用于从数据库中获取该用户的信息，这个需要自己去定义 # 另外我们可以在登录验证的时候或者token验证的时候讲user name存储于全局变量g中，这样我们可以随时获取该用户名 user = UserModel.get_resource_by_name(g.user_name) return user.get(&quot;role&quot;, VISITOR)def access_control(func): @wraps(func) def wrap_func(*args, **kwargs): # 同样要先获取到API的endpoint和http method，此处代码省略 ... try: if not Role[role][endpoint][http_method]: return make_response( jsonify(&#123;&apos;error&apos;: &apos;no permission&apos;&#125;), 403) return func(*args, **kwargs) except KeyError: return make_response( jsonify(&#123;&apos;error&apos;: &apos;no permission&apos;&#125;), 403) return wrap_func 以下是一个获取图片resource的使用示例 1234567891011from flask_restful import Resourceclass ImageResource(Resource): def __init__(self): super(ImageResource, self).__init__() @token_auth.login_required @access_control def get(self, resource_id): response = resource_get(resource_id) return response 这里另外一个decortor @token_auth.login_required用于token验证，大家可以先不用理会。到这里我们已经可以针对每个接口自动判断该用户是否有权限访问了，而所有权限的变化，都可以通过修改Role中的权限来进行更改，而不需要更改原来的代码，很爽吧，有木有？ 不过，笔者在项目中还遇到了另外一个问题，有时候针对一个接口所有的user都应该有权限，但是针对特定的resource，只能resource owner可以操作，举个栗子，比如我们要删除某个评论，但是只允许发布评论的人才有权限删除，也就是comment resource的owner才可以使用delete接口删除，但是我们所有的用户在Role定义的时候delete接口都是True，这个怎么办呢？ 这就需要我们在access_control检测完了之后再进一步检测该用户是否是resource owner，所以我们就需要进一步检测，这里添加一个decorator如下： 12345678910111213def get_resource_owner(): &quot;&quot;&quot;获取resource的owner&quot;&quot;&quot; # 自定义，代码省略 ...def owner_permission_required(func): @wrap(func) def wrap_func(*args, **kwargs): if g.user_name == get_resource_owner(): return func(*args, **kwargs) return make_response( jsonify(&#123;&apos;error&apos;: &apos;no permission&apos;&#125;), 403) return wrap_func 使用如下： 12345678910111213from flask_restful import Resourceclass CommentResource(Resource): def __init__(self): super(CommentResource, self).__init__() @token_auth.login_required @access_control @owner_permission_required @marshal_with(image_fields) def delete(self, resource_id): response = resource_delete(resource_id) return response 注意：decorator的顺序是不能改变的。 至此，Restful API权限管理相关的设计就完成了，如果文章给你带来了启发，记得点赞哦！","categories":[],"tags":[]},{"title":"Python字符编码之理解","slug":"Python字符编码之理解","date":"2017-04-22T10:18:18.000Z","updated":"2018-08-12T08:02:56.392Z","comments":true,"path":"2017/04/22/Python字符编码之理解/","link":"","permalink":"http://yoursite.com/2017/04/22/Python字符编码之理解/","excerpt":"","text":"在从普通程序员进阶到优秀程序员的路上，字符编码是一个不得不跨过去的坎，我们几乎所有的程序都会涉及到字符处理，如果跨不过这个坎，那么几乎注定会面对一些坑。 &lt;!--more--&gt; 本篇文章试图通过实际的例子来阐释字符编码解码的过程，从而能够更加清晰地认识程序到底是怎样处理字符的。在进入正文之前，你需要先了解字符集和字符编码的区别，需要知道什么是Unicode，什么是UTF-8，GBK等基本概念，如果你不了解，请移步下面的几篇文章： 字符编码详解 字符编码笔记 之后，我们试想一下，程序处理字符的过程是怎样的？我想最开始一定是先打开一个编辑器，把程序写出来，然后将程序保存为一个源文件（Python中就是.py文件），所以我们先从文件的存储开始说起。 源文件的存储 我们在编辑器中写的代码都是字符形式存在的，而当我们要将这些字符存储到硬盘时，必须有一个编码过程，因为计算机只能认识0/1序列，所以这些字符就必须通过一些编码规则转化成二进制序列，然后再存储到硬盘。比如我们写了下面一段程序 12s = &apos;你好&apos;print repr(s), s 当我们存储该文件时，如果是以GB2312编码方式进行存储的，那么文件的二进制表示是这样的 1234➜ testProgram hexdump -C gb2312encodingfile.py00000000 73 20 3d 20 27 c4 e3 ba c3 27 0a 70 72 69 6e 74 |s = &apos;....&apos;.print|00000010 20 72 65 70 72 28 73 29 2c 20 73 0a | repr(s), s.|0000001c 这里73代表s 20代表空格 3d代表= 27代表' c4 e3代表你 ba c3代表好，以此类推 在这里可以看出汉字在GB2312中是用两个字节来表示的。 我们再使用utf-8来存储同样的一段代码，看看其二进制表示是什么样子 1234➜ testProgram hexdump -C utf8encodingfile.py00000000 73 20 3d 20 27 e4 bd a0 e5 a5 bd 27 0a 70 72 69 |s = &apos;......&apos;.pri|00000010 6e 74 20 72 65 70 72 28 73 29 2c 20 73 0a |nt repr(s), s.|0000001e 同样的这里73代表s 20代表空格 3d代表= 27代表' 但是你好汉字是用三个字节来表示的 e4 bd a0代表你 e5 a5 bd代表好 现在源文件已经以二进制码流存储到了硬盘，那么源代码又是如何执行的呢？ 源代码执行 源代码执行的时候，Python解释器首先会将源文件load进内存当中，然后一行行开始读取文件并解释执行。 但是这里需要注意的是，如果是str字符串，python解释器只会读取其二进制码流，假设我们使用的是gb2312encodingfile.py，那么s指向的字符串你好读进内存后的表示就是c4 e3 ba c3, 当我们使用print打印的时候，如果是在Windows的console上执行，则可以正确执行，显示如下 12➜ testProgram python gb2312encodingfile.py&apos;\\xc4\\xe3\\xba\\xc3&apos; 你好 但是在Linux上或者mac上无法正确执行，显示如下： 12➜ testProgram python gb2312encodingfile.py&apos;\\xc4\\xe3\\xba\\xc3&apos; ��� 这是由于Windows console默认是GBK编解码的(GB2312的扩展)，所以可以将\\xc4\\xe3\\xba\\xc3正确解码显示成汉字你好，但是在Linux或者Mac上，console的默认编解码方式是UTF-8，所以也就无法将\\xc4\\xe3\\xba\\xc3正确显示出来。 另外一个小插曲是，如果代码中有汉字，需要在文件开头声明编码方式(#-*- coding: utf-8 - 或者# coding=utf8)，否则解释器默认使用ASCII编码方式去打开源文件，这样就会报错，如下 123➜ testProgram python gb2312encodingfile.py File &quot;gb2312encodingfile.py&quot;, line 1SyntaxError: Non-UTF-8 code starting with &apos;\\xc4&apos; in file gb2312encodingfile.py on line 1, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details 但是如果我们的字符串是unicode对象的字符串，那么Python解释器会将字符串的字节序列先进行解码，然后再将解码后的字节序列的引用赋给s，可以更改utf8encodingfile.py代码如下： 123456#-*- coding: utf-8 -*-s = &apos;你好&apos;print repr(s), su = u&apos;你好&apos;print repr(u), u 保存后，使用hexdump查看其二进制编码如下： 12345678➜ testProgram hexdump -C utf8encodingfile.py00000000 23 2d 2a 2d 20 63 6f 64 69 6e 67 3a 20 75 74 66 |#-*- coding: utf|00000010 2d 38 20 2d 2a 2d 0a 73 20 3d 20 27 e4 bd a0 e5 |-8 -*-.s = &apos;....|00000020 a5 bd 27 0a 70 72 69 6e 74 20 72 65 70 72 28 73 |..&apos;.print repr(s|00000030 29 2c 20 73 0a 0a 75 20 3d 20 75 27 e4 bd a0 e5 |), s..u = u&apos;....|00000040 a5 bd 27 0a 70 72 69 6e 74 20 72 65 70 72 28 75 |..&apos;.print repr(u|00000050 29 2c 20 75 0a |), u.|00000055 仔细观察会发现两个你好字符串都编码成了e4 bd a0 e5 a5 bd 然后在mac上执行，结果如下： 123➜ testProgram python utf8encodingfile.py&apos;\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd&apos; 你好u&apos;\\u4f60\\u597d&apos; 你好 可以看出s指向的字节序列是\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd，而u指向的字节序列是\\u4f60\\u597d (也就是将e4 bd a0 e5 a5 bd 解码成了\\u4f60\\u597d) 但是如果我们更改的是gb2312encodingfile.py，并使用gb2312编码保存，再执行这个程序看看会是什么结果。 结果直接报错： 1234➜ testProgram python gb2312encodingfile.py File &quot;gb2312encodingfile.py&quot;, line 5 u = u&apos;���&apos;SyntaxError: (unicode error) &apos;utf8&apos; codec can&apos;t decode byte 0xc4 in position 0: invalid continuation byte 这是由于Python解释器尝试用声明的utf-8编码方式去解码gb2312编码的字节序列，所以造成了这样的错误。 至此我们已经知道了Python如何读写源文件的，那么Python执行的时候又是如何读写外部文件的呢？ 文件读写 现在我们使用如下代码尝试将字符串写到文件当中，注意源码保存使用utf-8, 文件名为utf8encodingfile_write.py 12345678#-*- coding: utf-8 -*-s = &apos;你好&apos;with open(&apos;stroutput.txt&apos;, &apos;w&apos;) as f: f.write(s)u = u&apos;你好&apos;with open(&apos;unicodeoutput.txt&apos;, &apos;w&apos;) as f: f.write(u) 在mac上执行，结果如下： 12345➜ testProgram python utf8encodingfile_write.pyTraceback (most recent call last): File &quot;utf8encodingfile_write.py&quot;, line 8, in &lt;module&gt; f.write(u)UnicodeEncodeError: &apos;ascii&apos; codec can&apos;t encode characters in position 0-1: ordinal not in range(128) *'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)*说明系统在写文件编码的时候尝试使用ASCII来进行编码，可是我们明明已经声明了使用utf-8啊。 原来文件头声明使用utf-8，只是用于解释器去解释源码文件的时候使用，当我们调用write去写一个文件的时候，会调用系统默认的编码设置来进行编码。我们来看下系统默认的编码是什么： 123&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getdefaultencoding()&apos;ascii&apos; 果然，系统默认就是ascii编码方式。 解决这个问题有两种方式，一种是修改系统默认的编码方式，另一种是在open的时候指定编码方式，其中第二种显然更加优雅一些。 1234567891011# 通过修改系统默认编码方式来实现utf-8编码import sysreload(sys) # 这里必须reload一下才能找到setdefaultencoding methodsys.setdefaultencoding(&apos;utf-8&apos;)# 通过在codecs.open中设置编码方式import codecswith codecs.open(&quot;filename&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: f.write(u) 同样的，当我们读取一个文件的时候，也可以通过codecs.open来设定编解码方式，但是首先我们需要知道这个要读取的文件的编码方式，假设文件是以utf-8的方式进行编码的，读取的时候就可以如下： 1234import codecswith open(&quot;somefile&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f: content = f.read() 另外，有时我们并非从文件中读取，而是直接使用了一个非标准字符，这是就需要使用decode先解码 12345# if not decode, will raise exception: &apos;ascii&apos; codec can&apos;t# decode byte 0xe2 in position 0: ordinal not in range(128)dash = &apos;–&apos;.decode(&quot;utf8&quot;)if dash in title: title = title.split(dash)[0] 至此，关于Python编码就讲完了，如果你有收获，就请点个赞鼓励下吧！","categories":[],"tags":[]},{"title":"最新Python异步编程详解","slug":"python_async","date":"2017-02-19T10:18:18.000Z","updated":"2018-08-12T10:11:56.995Z","comments":true,"path":"2017/02/19/python_async/","link":"","permalink":"http://yoursite.com/2017/02/19/python_async/","excerpt":"","text":"我们都知道对于I/O相关的程序来说，异步编程可以大幅度的提高系统的吞吐量，因为在某个I/O操作的读写过程中，系统可以先去处理其它的操作（通常是其它的I/O操作），那么Python中是如何实现异步编程的呢？ &lt;!--more--&gt; 简单的回答是Python通过协程(coroutine)来实现异步编程。那究竟啥是协程呢？这将是一个很长的故事。 故事要从yield开始说起(已经熟悉yield的读者可以跳过这一节)。 yield yield是用来生成一个生成器的(Generator), 生成器又是什么呢？这又是一个长长的story，所以这次我建议您移步到这里： 完全理解Python迭代对象、迭代器、生成器，而关于yield是怎么回事，建议看这里：[翻译]PYTHON中YIELD的解释 好了，现在假设你已经明白了yield和generator的概念了，请原谅我这种不负责任的说法但是这真的是一个很长的story啊！ 总的来说，yield相当于return，它将相应的值返回给调用next()或者send()的调用者，从而交出了cpu使用权，而当调用者再调用next()或者send()时，又会返回到yield中断的地方，如果send有参数，又会将参数返回给yield赋值的变量，如果没有就跟next()一样赋值为None。但是这里会遇到一个问题，就是嵌套使用generator时外层的generator需要写大量代码，看如下示例： 注意以下代码均在Python3.6上运行调试 1234567891011121314151617181920212223242526272829303132333435363738394041#!/usr/bin/env python# encoding:utf-8def inner_generator(): i = 0 while True: i = yield i if i &gt; 10: raise StopIterationdef outer_generator(): print(&quot;do something before yield&quot;) from_inner = 0 from_outer = 1 g = inner_generator() g.send(None) while 1: try: from_inner = g.send(from_outer) from_outer = yield from_inner except StopIteration: breakdef main(): g = outer_generator() g.send(None) i = 0 while 1: try: i = g.send(i + 1) print(i) except StopIteration: breakif __name__ == &apos;__main__&apos;: main() 为了简化，在Python3.3中引入了yield from yield from 使用yield from有两个好处， 可以将main中send的参数一直返回给最里层的generator， 同时我们也不需要再使用while循环和send (), next()来进行迭代。 我们可以将上边的代码修改如下： 1234567891011121314151617181920212223242526def inner_generator(): i = 0 while True: i = yield i if i &gt; 10: raise StopIterationdef outer_generator(): print(&quot;do something before coroutine start&quot;) yield from inner_generator()def main(): g = outer_generator() g.send(None) i = 0 while 1: try: i = g.send(i + 1) print(i) except StopIteration: breakif __name__ == &apos;__main__&apos;: main() 执行结果如下： 1234567891011do something before coroutine start12345678910 这里inner_generator()中执行的代码片段我们实际就可以认为是协程，所以总的来说逻辑图如下： 接下来我们就看下究竟协程是啥样子 协程coroutine 协程的概念应该是从进程和线程演变而来的，他们都是独立的执行一段代码，但是不同是线程比进程要轻量级，协程比线程还要轻量级。多线程在同一个进程中执行，而协程通常也是在一个线程当中执行。它们的关系图如下： 我们都知道Python由于GIL(Global Interpreter Lock)原因，其线程效率并不高，并且在*nix系统中，创建线程的开销并不比进程小，因此在并发操作时，多线程的效率还是受到了很大制约的。所以后来人们发现通过yield来中断代码片段的执行，同时交出了cpu的使用权，于是协程的概念产生了。在Python3.4正式引入了协程的概念，代码示例如下： 12345678910111213141516import asyncio# Borrowed from http://curio.readthedocs.org/en/latest/tutorial.html.@asyncio.coroutinedef countdown(number, n): while n &gt; 0: print(&apos;T-minus&apos;, n, &apos;(&#123;&#125;)&apos;.format(number)) yield from asyncio.sleep(1) n -= 1loop = asyncio.get_event_loop()tasks = [ asyncio.ensure_future(countdown(&quot;A&quot;, 2)), asyncio.ensure_future(countdown(&quot;B&quot;, 3))]loop.run_until_complete(asyncio.wait(tasks))loop.close() 示例显示了在Python3.4引入两个重要概念协程和事件循环， 通过修饰符@asyncio.coroutine定义了一个协程，而通过event loop来执行tasks中所有的协程任务。之后在Python3.5引入了新的async &amp; await语法，从而有了原生协程的概念。 async &amp; await 在Python3.5中，引入了aync&amp;await 语法结构，通过&quot;aync def&quot;可以定义一个协程代码片段，作用类似于Python3.4中的@asyncio.coroutine修饰符，而await则相当于&quot;yield from&quot;。 先来看一段代码，这个是我刚开始使用async&amp;await语法时，写的一段小程序。 12345678910111213141516171819202122232425#!/usr/bin/env python# encoding:utf-8import asyncioimport requestsimport timeasync def wait_download(url): response = await requests.get(url) print(&quot;get &#123;&#125; response complete.&quot;.format(url))async def main(): start = time.time() await asyncio.wait([ wait_download(&quot;http://www.163.com&quot;), wait_download(&quot;http://www.mi.com&quot;), wait_download(&quot;http://www.google.com&quot;)]) end = time.time() print(&quot;Complete in &#123;&#125; seconds&quot;.format(end - start))loop = asyncio.get_event_loop()loop.run_until_complete(main()) 这里会收到这样的报错： 123456Task exception was never retrievedfuture: &lt;Task finished coro=&lt;wait_download() done, defined at asynctest.py:9&gt; exception=TypeError(&quot;object Response can&apos;t be used in &apos;await&apos; expression&quot;,)&gt;Traceback (most recent call last): File &quot;asynctest.py&quot;, line 10, in wait_download data = await requests.get(url)TypeError: object Response can&apos;t be used in &apos;await&apos; expression 这是由于requests.get()函数返回的Response对象不能用于await表达式，可是如果不能用于await，还怎么样来实现异步呢？ 原来Python的await表达式是类似于&quot;yield from&quot;的东西，但是await会去做参数检查，它要求await表达式中的对象必须是awaitable的，那啥是awaitable呢？ awaitable对象必须满足如下条件中其中之一： A native coroutine object returned from a native coroutine function . 原生协程对象 A generator-based coroutine object returned from a function decorated with types.coroutine() . types.coroutine()修饰的基于生成器的协程对象，注意不是Python3.4中asyncio.coroutine An object with an await method returning an iterator. 实现了__await__ method，并在其中返回了iterator的对象 根据这些条件定义，我们可以修改代码如下： 12345678910111213141516171819202122232425262728293031#!/usr/bin/env python# encoding:utf-8import asyncioimport requestsimport timeasync def download(url): # 通过async def定义的函数是原生的协程对象 print(&quot;get %s&quot; % url) response = requests.get(url) print(response.status_code)async def wait_download(url): await download(url) # 这里download(url)就是一个原生的协程对象 print(&quot;get &#123;&#125; data complete.&quot;.format(url))async def main(): start = time.time() await asyncio.wait([ wait_download(&quot;http://www.163.com&quot;), wait_download(&quot;http://www.mi.com&quot;), wait_download(&quot;http://www.baidu.com&quot;)]) end = time.time() print(&quot;Complete in &#123;&#125; seconds&quot;.format(end - start))loop = asyncio.get_event_loop()loop.run_until_complete(main()) 至此，程序可以运行，不过仍然有一个问题就是它并没有真正地异步执行 看一下运行结果： 12345678910get http://www.163.com200get http://www.163.com data complete.get http://www.baidu.com200get http://www.baidu.com data complete.get http://www.mi.com200get http://www.mi.com data complete.Complete in 0.49027466773986816 seconds 会发现程序始终是同步执行的，这就说明仅仅是把涉及I/O操作的代码封装到async当中是不能实现异步执行的。必须使用支持异步操作的非阻塞代码才能实现真正的异步。目前支持非阻塞异步I/O的库是aiohttp 1234567891011121314151617181920212223242526272829303132#!/usr/bin/env python# encoding:utf-8import asyncioimport aiohttpimport timeasync def download(url): # 通过async def定义的函数是原生的协程对象 print(&quot;get: %s&quot; % url) async with aiohttp.ClientSession() as session: async with session.get(url) as resp: print(resp.status) # response = await resp.read()# 此处的封装不再需要# async def wait_download(url):# await download(url) # 这里download(url)就是一个原生的协程对象# print(&quot;get &#123;&#125; data complete.&quot;.format(url))async def main(): start = time.time() await asyncio.wait([ download(&quot;http://www.163.com&quot;), download(&quot;http://www.mi.com&quot;), download(&quot;http://www.baidu.com&quot;)]) end = time.time() print(&quot;Complete in &#123;&#125; seconds&quot;.format(end - start))loop = asyncio.get_event_loop()loop.run_until_complete(main()) 再看一下测试结果： 1234567get: http://www.mi.comget: http://www.163.comget: http://www.baidu.com200200200Complete in 0.27292490005493164 seconds 可以看出这次是真正的异步了。 好了现在一个真正的实现了异步编程的小程序终于诞生了。 而目前更牛逼的异步是使用uvloop或者pyuv，这两个最新的Python库都是libuv实现的，可以提供更加高效的event loop。 uvloop和pyuv 关于uvloop可以参考uvloop pyuv可以参考这里pyuv pyuv实现了Python2.x和3.x，但是该项目在github上已经许久没有更新了，不知道是否还有人在维护。 uvloop只实现了3.x, 但是该项目在github上始终活跃。 它们的使用也非常简单，以uvloop为例，只需要添加以下代码就可以了 123import asyncioimport uvloopasyncio.set_event_loop_policy(uvloop.EventLoopPolicy()) 关于Python异步编程到这里就告一段落了，而引出这篇文章的引子实际是关于网上有关Sanic和uvloop的组合创造的惊人的性能，感兴趣的同学可以找下相关文章，也许后续我会再专门就此话题写一篇文章，欢迎交流！","categories":[],"tags":[]},{"title":"Spark启动时的master参数以及Spark的部署方式","slug":"spark_initial","date":"2017-02-10T10:18:18.000Z","updated":"2018-08-12T10:12:24.246Z","comments":true,"path":"2017/02/10/spark_initial/","link":"","permalink":"http://yoursite.com/2017/02/10/spark_initial/","excerpt":"","text":"我们在初始化SparkConf时，或者提交Spark任务时，都会有master参数需要设置，如下： &lt;!--more--&gt; 12conf = SparkConf().setAppName(appName).setMaster(master)sc = SparkContext(conf=conf) 1234/bin/spark-submit \\ --cluster cluster_name \\ --master yarn-cluster \\ ... 但是这个master到底是何含义呢？文档说是设定master url，但是啥是master url呢？说到这就必须先要了解下Spark的部署方式了。 我们要部署Spark这套计算框架，有多种方式，可以部署到一台计算机，也可以是多台(cluster)。我们要去计算数据，就必须要有计算机帮我们计算，当然计算机越多(集群规模越大)，我们的计算力就越强。但有时候我们只想在本机做个试验或者小型的计算，因此直接部署在单机上也是可以的。Spark部署方式可以用如下图形展示： 下面我们就来分别介绍下。 Local模式 Local模式就是运行在一台计算机上的模式，通常就是用于在本机上练手和测试。它可以通过以下集中方式设置master。 local: 所有计算都运行在一个线程当中，没有任何并行计算，通常我们在本机执行一些测试代码，或者练手，就用这种模式。 local[K]: 指定使用几个线程来运行计算，比如local[4]就是运行4个worker线程。通常我们的cpu有几个core，就指定几个线程，最大化利用cpu的计算能力 local[*]: 这种模式直接帮你按照cpu最多cores来设置线程数了。 使用示例： 1234/bin/spark-submit \\ --cluster cluster_name \\ --master local[*] \\ ... 总而言之这几种local模式都是运行在本地的单机版模式，通常用于练手和测试，而实际的大规模计算就需要下面要介绍的cluster模式。 cluster模式 cluster模式肯定就是运行很多机器上了，但是它又分为以下三种模式，区别在于谁去管理资源调度。（说白了，就好像后勤管家，哪里需要资源，后勤管家要负责调度这些资源） standalone模式 这种模式下，Spark会自己负责资源的管理调度。它将cluster中的机器分为master机器和worker机器，master通常就一个，可以简单的理解为那个后勤管家，worker就是负责干计算任务活的苦劳力。具体怎么配置可以参考Spark Standalone Mode 使用standalone模式示例： 1234/bin/spark-submit \\ --cluster cluster_name \\ --master spark://host:port \\ ... --master就是指定master那台机器的地址和端口，我想这也正是--master参数名称的由来吧。 mesos模式 这里就很好理解了，如果使用mesos来管理资源调度，自然就应该用mesos模式了，示例如下： 1234/bin/spark-submit \\ --cluster cluster_name \\ --master mesos://host:port \\ ... yarn模式 同样，如果采用yarn来管理资源调度，就应该用yarn模式，由于很多时候我们需要和mapreduce使用同一个集群，所以都采用Yarn来管理资源调度，这也是生产环境大多采用yarn模式的原因。yarn模式又分为yarn cluster模式和yarn client模式： yarn cluster: 这个就是生产环境常用的模式，所有的资源调度和计算都在集群环境上运行。 yarn client: 这个是说Spark Driver和ApplicationMaster进程均在本机运行，而计算任务在cluster上。 使用示例： 1234/bin/spark-submit \\ --cluster cluster_name \\ --master yarn-cluster \\ ...","categories":[],"tags":[]},{"title":"Python Traceback 详解","slug":"python_traceback","date":"2017-02-07T10:18:18.000Z","updated":"2018-08-12T10:14:46.316Z","comments":true,"path":"2017/02/07/python_traceback/","link":"","permalink":"http://yoursite.com/2017/02/07/python_traceback/","excerpt":"","text":"刚接触Python的时候，简单的异常处理已经可以帮助我们解决大多数问题，但是随着逐渐地深入，我们会发现有很多情况下简单的异常处理已经无法解决问题了，如下代码，单纯的打印异常所能提供的信息会非常有限。 &lt;!--more--&gt; 12345678910111213def func1(): raise Exception(&quot;--func1 exception--&quot;)def main(): try: func1() except Exception as e: print eif __name__ == &apos;__main__&apos;: main() 执行后输出如下： 1--func1 exception-- 通过示例，我们发现普通的打印异常只有很少量的信息（通常是异常的value值），这种情况下我们很难定位在哪块代码出的问题，以及如何出现这种异常。那么到底要如何打印更加详细的信息呢？下面我们就来一一介绍。 sys.exc_info和traceback object Python程序的traceback信息均来源于一个叫做traceback object的对象，而这个traceback object通常是通过函数sys.exc_info()来获取的，先来看一个例子： 12345678910111213141516171819import sysdef func1(): raise NameError(&quot;--func1 exception--&quot;)def main(): try: func1() except Exception as e: exc_type, exc_value, exc_traceback_obj = sys.exc_info() print &quot;exc_type: %s&quot; % exc_type print &quot;exc_value: %s&quot; % exc_value print &quot;exc_traceback_obj: %s&quot; % exc_traceback_objif __name__ == &apos;__main__&apos;: main() 执行后输出如下： 123exc_type: &lt;type &apos;exceptions.NameError&apos;&gt;exc_value: --func1 exception--exc_traceback_obj: &lt;traceback object at 0x7faddf5d93b0&gt; 通过以上示例我们可以看出，sys.exc_info()获取了当前处理的exception的相关信息，并返回一个元组，元组的第一个数据是异常的类型(示例是NameError类型)，第二个返回值是异常的value值，第三个就是我们要的traceback object. 有了traceback object我们就可以通过traceback module来打印和格式化traceback的相关信息，下面我们就来看下traceback module的相关函数。 traceback module Python的traceback module提供一整套接口用于提取，格式化和打印Python程序的stack traces信息，下面我们通过例子来详细了解下这些接口： print_tb 123456789101112131415161718import sysimport tracebackdef func1(): raise NameError(&quot;--func1 exception--&quot;)def main(): try: func1() except Exception as e: exc_type, exc_value, exc_traceback_obj = sys.exc_info() traceback.print_tb(exc_traceback_obj)if __name__ == &apos;__main__&apos;: main() 输出： 1234File &quot;&lt;ipython-input-23-52bdf2c9489c&gt;&quot;, line 11, in main func1()File &quot;&lt;ipython-input-23-52bdf2c9489c&gt;&quot;, line 6, in func1 raise NameError(&quot;--func1 exception--&quot;) 这里我们可以发现打印的异常信息更加详细了，下面我们了解下print_tb的详细信息： 1traceback.print_tb(tb[, limit[, file]]) tb: 这个就是traceback object, 是我们通过sys.exc_info获取到的 limit: 这个是限制stack trace层级的，如果不设或者为None，就会打印所有层级的stack trace file: 这个是设置打印的输出流的，可以为文件，也可以是stdout之类的file-like object。如果不设或为None，则输出到sys.stderr。 print_exception 1234567891011121314151617181920import sysimport tracebackdef func1(): raise NameError(&quot;--func1 exception--&quot;)def func2(): func1()def main(): try: func2() except Exception as e: exc_type, exc_value, exc_traceback_obj = sys.exc_info() traceback.print_exception(exc_type, exc_value, exc_traceback_obj, limit=2, file=sys.stdout)if __name__ == &apos;__main__&apos;: main() 输出： 123456Traceback (most recent call last): File &quot;&lt;ipython-input-24-a68061acf52f&gt;&quot;, line 13, in main func2() File &quot;&lt;ipython-input-24-a68061acf52f&gt;&quot;, line 9, in func2 func1()NameError: --func1 exception-- 看下定义： 1traceback.print_exception(etype, value, tb[, limit[, file]]) 跟print_tb相比多了两个参数etype和value，分别是exception type和exception value，加上tb(traceback object)，正好是sys.exc_info()返回的三个值 另外，与print_tb相比，打印信息多了开头的&quot;Traceback (most...)&quot;信息以及最后一行的异常类型和value信息 还有一个不同是当异常为SyntaxError时，会有&quot;^&quot;来指示语法错误的位置 print_exc print_exc是简化版的print_exception, 由于exception type, value和traceback object都可以通过sys.exc_info()获取，因此print_exc()就自动执行exc_info()来帮助获取这三个参数了，也因此这个函数是我们的程序中最常用的，因为它足够简单 12345678910111213141516171819import sysimport tracebackdef func1(): raise NameError(&quot;--func1 exception--&quot;)def func2(): func1()def main(): try: func2() except Exception as e: traceback.print_exc(limit=1, file=sys.stdout)if __name__ == &apos;__main__&apos;: main() 输出（由于limit=1，因此只有一个层级被打印出来）： 1234Traceback (most recent call last): File &quot;&lt;ipython-input-25-a1f5c73b97c4&gt;&quot;, line 13, in main func2()NameError: --func1 exception-- 定义如下： 1traceback.print_exc([limit[, file]]) 只有两个参数，够简单 format_exc 1234567891011121314151617181920import loggingimport sysimport tracebacklogger = logging.getLogger(&quot;traceback_test&quot;)def func1(): raise NameError(&quot;--func1 exception--&quot;)def func2(): func1()def main(): try: func2() except Exception as e: logger.error(traceback.format_exc(limit=1, file=sys.stdout))if __name__ == &apos;__main__&apos;: main() 从这个例子可以看出有时候我们想得到的是一个字符串，比如我们想通过logger将异常记录在log里，这个时候就需要format_exc了，这个也是最常用的一个函数，它跟print_exc用法相同，只是不直接打印而是返回了字符串。 traceback module中还有一些其它的函数，但因为并不常用，就不在展开来讲，感兴趣的同学可以看下参考链接中的文档。 获取线程中的异常信息 通常情况下我们无法将多线程中的异常带回主线程，所以也就无法打印线程中的异常，而通过上边学到这些知识，我们可以对线程做如下修改，从而实现捕获线程异常的目的。 以下示例来自weidong的博客文章，稍有修改（见参考链接） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import threadingimport tracebackdef my_func(): raise BaseException(&quot;thread exception&quot;)class ExceptionThread(threading.Thread): def __init__(self, group=None, target=None, name=None, args=(), kwargs=None, verbose=None): &quot;&quot;&quot; Redirect exceptions of thread to an exception handler. &quot;&quot;&quot; threading.Thread.__init__(self, group, target, name, args, kwargs, verbose) if kwargs is None: kwargs = &#123;&#125; self._target = target self._args = args self._kwargs = kwargs self._exc = None def run(self): try: if self._target: self._target() except BaseException as e: import sys self._exc = sys.exc_info() finally: #Avoid a refcycle if the thread is running a function with #an argument that has a member that points to the thread. del self._target, self._args, self._kwargs def join(self): threading.Thread.join(self) if self._exc: msg = &quot;Thread &apos;%s&apos; threw an exception: %s&quot; % (self.getName(), self._exc[1]) new_exc = Exception(msg) raise new_exc.__class__, new_exc, self._exc[2]t = ExceptionThread(target=my_func, name=&apos;my_thread&apos;)t.start()try: t.join()except: traceback.print_exc() 输出如下： 12345678Traceback (most recent call last): File &quot;/data/code/testcode/thread_exc.py&quot;, line 43, in &lt;module&gt; t.join() File &quot;/data/code/testcode/thread_exc.py&quot;, line 23, in run self._target() File &quot;/data/code/testcode/thread_exc.py&quot;, line 5, in my_func raise BaseException(&quot;thread exception&quot;)Exception: Thread &apos;my_thread&apos; threw an exception: thread exception 这样我们就得到了线程中的异常信息。 参考链接 traceback官方文档 weidong's blog","categories":[],"tags":[]},{"title":"Arrow-一个最好用的日期时间Python处理库","slug":"Arrow-一个最好用的日期时间Python处理库","date":"2017-02-01T10:18:18.000Z","updated":"2018-08-12T07:55:20.876Z","comments":true,"path":"2017/02/01/Arrow-一个最好用的日期时间Python处理库/","link":"","permalink":"http://yoursite.com/2017/02/01/Arrow-一个最好用的日期时间Python处理库/","excerpt":"","text":"写过Python程序的人大都知道，Python日期和时间的处理非常繁琐和麻烦，主要有以下几个问题： 有众多的package，类和方法，包括time，datetime，pytz等等 经常需要各种转换，比如时间戳，structtime，时间字符串之间相互转换，localtime和utctime的转换 难以记忆，有违人性的时间格式化字符串%Y %M %m %D %d 基于以上几点，每次做时间处理的时候总是需要翻看以前的代码或者文档，可见此处Python做的有多烂，好了废话不多说，今天给大家介绍的这个arrow极大地解放了我等Python程序员的脑容量。 安装 1pip install arrow 使用 获取当前时间 123456789In [13]: import arrowIn [14]: t = arrow.utcnow()In [15]: tOut[15]: &lt;Arrow [2017-02-01T08:30:37.627622+00:00]&gt;In [19]: arrow.now()Out[19]: &lt;Arrow [2017-02-01T16:32:02.857411+08:00]&gt; 通过utcnow()和now()分别获取了utc时间和local时间，最终获取的是一个Arrow时间对象，通过这个对象我们可以做各种时间转换，后边会看到。 时间形式转换 我们经常需要转换时间对象，比如转换称timestamp，有时需要转换成特定格式的时间字符串。 转换成timestamp 123456In [13]: import arrowIn [14]: t = arrow.utcnow()In [16]: t.timestampOut[16]: 1485937837 转换成时间字符串 1234567In [23]: t = arrow.now()In [24]: t.format()Out[24]: u&apos;2017-02-01 17:00:42+08:00&apos;In [25]: t.format(&quot;YYYY-MM-DD HH:mm&quot;)Out[25]: u&apos;2017-02-01 17:00&apos; 怎么样？是不是感觉很简单，心里感觉轻舒了一口气！这里可以注意到格式化字符串非常人性化便于记忆，对不对？完整的时间格式字符串参见这里 从字符串转换成Arrow对象 12In [20]: arrow.get(&quot;2017-01-20 11:30&quot;, &quot;YYYY-MM-DD HH:mm&quot;)Out[20]: &lt;Arrow [2017-01-20T11:30:00+00:00]&gt; 从时间戳转化为Arrow对象 12345In [26]: arrow.get(&quot;1485937858.659424&quot;)Out[26]: &lt;Arrow [2017-02-01T08:30:58.659424+00:00]&gt;In [27]: arrow.get(1485937858.659424)Out[27]: &lt;Arrow [2017-02-01T08:30:58.659424+00:00]&gt; 注意这里无论传递的是时间戳字符串还是float类型的时间戳都可以进行转化，很人性有木有？ 直接生成Arrow对象 12345In [28]: arrow.Arrow(2017, 2, 1)Out[28]: &lt;Arrow [2017-02-01T00:00:00+00:00]&gt;In [29]: arrow.get(2017, 2, 1)Out[29]: &lt;Arrow [2017-02-01T00:00:00+00:00]&gt; 时间推移 时间推移就是要获取某个时间之前的时间或者之后的时间，比如要获取相对于当前时间前一天的时间。 123456789101112131415In [30]: t = arrow.now()In [31]: tOut[31]: &lt;Arrow [2017-02-01T17:19:19.933507+08:00]&gt;In [33]: t.shift(days=-1) # 前一天Out[33]: &lt;Arrow [2017-01-31T17:19:19.933507+08:00]&gt;In [34]: t.shift(weeks=-1) # 前一周Out[34]: &lt;Arrow [2017-01-25T17:19:19.933507+08:00]&gt;In [35]: t.shift(months=-2) # 前两个月Out[35]: &lt;Arrow [2016-12-01T17:19:19.933507+08:00]&gt;In [37]: t.shift(years=1) # 明年Out[37]: &lt;Arrow [2018-02-01T17:19:19.933507+08:00]&gt; 看是不是很简单，比用timedelta要简单明了多了，是不是？ 更多请参考官方文档和Github 官方文档 Github 顺便说一句点赞是一种美德，Arrow第4000个赞就是我点的，哈哈。如果你觉的我的文章有帮助，也赶紧点个赞吧！","categories":[],"tags":[]},{"title":"为什么要使用IPython","slug":"why_ipython","date":"2016-12-21T10:18:18.000Z","updated":"2018-08-12T10:12:49.574Z","comments":true,"path":"2016/12/21/why_ipython/","link":"","permalink":"http://yoursite.com/2016/12/21/why_ipython/","excerpt":"","text":"IPython提供了改进的交互式Python Shell，我们可以利用IPython来执行Python语句，并能够立刻看到结果，这一点跟Python自带的shell工具没有什么不同，但是IPython额外提供了很多实用的功能是Python自带的shell所没有的，下面就来看看这些实用的功能吧。 &lt;!--more--&gt; ####Tab自动补全 使用过Linux命令行的同学都知道tab自动补全有多实用吧，IPython可以针对之前输入过的变量，对象的方法等进行自动补全。我们只需要输入一部分，就可以看到命名空间中所有相匹配的变量，函数等 这个示例显示了，当输入mylist. 之后按tab键就自动出现的可以使用的method。 tab补全还可以针对文件路径进行补全，例如下面的例子在输入Anaconda之后按tab自动显示路径下的所有可选路径 ####内省 在变量的前面或者后面加问号?就可以查询改对象相关的信息(简要信息)，有的时候对象的描述信息较多时，需要两个问号??来显示全部信息. ####魔术命令 魔术命令(magic commands)是IPython提供一整套命令，用这些命令可以操作IPython本身，以及提供一些系统功能。魔术命令分为两种：一种是基于行的(line-oriented), 命令只针对一行；另一种是基于单元的(cell-oriented)， 命令可以针对多行，均作为其参数。 比如：下面这个%timeit魔术命令就是line-oriented %timeit range(1000) 下面的魔术命令是cell-oriented %%timeit x = numpy.random.rann((100, 100)) numpy.linalg.svd(x) 注：这里timeit只针对svd进行测试，不会针对x变量的赋值做测试。 IPython提供了很多类似的魔术命令，如果你想看都有哪些魔术命令，可以通过%lsmagic来查询，如果想查询某个命令的详细信息，可以通过%cmd?来获取，例如：%run? 另外，默认情况下automagic是ON状态，也就是说对于line-oriented命令我们不需要使用前面的百分号%，直接输入命令即可（例如：timeit range(1000) )，但是对于cell-oriented命令我们必须输入%%，可以通过%automagic来打开/关闭这个automagic功能。 下面仅就常用的魔术命令做下介绍： %run 命令 该命令可以直接执行python脚本，并输出结果，比如我们有一个python文件如下： 123456# test.pydef main(): print &quot;this is a test.&quot;if __name__ == &apos;__main__&apos;: main() 在IPython中执行如下图： %paste 和 %cpaste命令 我们经常会将python文件中的代码粘贴到IPython中执行，以便查看效果，尤其是针对现实图形的代码。这时如果直接拷贝粘贴，IPython可能无法正确的执行，这是因为IPython一旦遇到空行就认为粘贴结束，然后就开始执行了。 例如，我们有如下代码： 12345def add(x, y): result = x + y print resultadd(1, 2) 如果直接拷贝粘贴到IPython当中，就会报错，如下： 这时我们就需要使用%paste或者%cpaste来粘贴了，先拷贝代码，然后在IPython中输入%paste，结果如下： %cpaste也可以粘贴代码，与%paste区别就是它可以持续粘贴(即continue paste)，直到我们按Ctrl+D或者输入双减号&quot;--&quot;以结束粘贴 %pdb 用以打开/关闭自动pdb唤出功能，当我们打开这个功能的时候(通过%pdb on 或者%pdb 1)，程序一旦遇到exception就会自动调用pdb，进入pdb交互界面(如果要关闭该功能可以通过%pdb off 或者%pdb 0) 例如我们有一个test2.py文件如下： 12345678def raise_exception(): var_before_exception = 1 raise Exception(&quot;test&quot;) var_after_exception = 2 print var_before_exception print var_after_exceptionraise_exception() 当我们打开pdb开关后，执行如下图所示： 可以发现pdb在raise exception的地方被唤出了。 %edit 用于启动一个编辑器。在Linux上会启动vim，在Windows上会启动notepad。我们可以在编辑器上编辑代码，保存退出后就会执行相应代码。 ！cmd ！后可以跟一个shell指令，从而在IPython界面就可以直接执行shell指令而不需要再退出IPython了 例如下面执行一条ping命令： 除上述魔术命令外，IPython还提供了很多其它命令，下表列出了常用的一些命令，如果要查看完整列表，请使用%lsmagic 命令 命令说明 %hist 查询输入的历史 %reset 清空namespace %prun 使用Python profiler运行python代码。注：profiler是用于测试代码性能的工具 %time 用于显示Python语句的执行时间，包括cpu time和wall clock time %timeit 用于显示Python语句的执行时间，但是这个命令会多次执行相应的语句（可以指定次数）%timeit只针对一行Python语句，如果有多条语句，需要用分号分隔开，同时%%timeit是cell-oriented魔术命令，紧跟在命令后面的语句是作为setup code的，在cell body中的代码才会被timeit多次执行并计算时间 %bookmark 用于存储常用路径 %cd 进入目录命令 %env 显示系统环境变量 %pushd dir 将当前目录入栈，并进入dir指定的目录 %popd 弹出栈顶目录 matplotlib集成和pylab模式 IPython在使用matplotlib库生成图形的时候，仍然可以在IPython交互界面输入和操作，这是它比Python shell强大的一个重要功能点，使得我们在做科学计算的时候，可以更好的与数据交互并可视化。而要使用此功能，需要在启动IPython的时候，开启pylab模式，如下 $ipython --pylab 输入和输出 IPython的交互界面提示符分为In和Out，In代表输入并在后面的中括号中带有行号，如: In [2]: Out代表输出，同样也有行号。IPython提供了一下几种快捷方式方便我们操作输入和输出： ● _ 和__: 单下划线_代表上一个输出，双下划线代表上上个输出 ● _iX和_X: X代表行号，_iX代表第X行的输入的字符串，_X代表输出的字符串 ● exec：我们可以利用exec命令来执行历史的输入操作 下图展示了如何利用这些快捷方式更方便的进行输入和输出的操作： 键盘快捷键 IPython提供了类似Linux的快捷键操作方式，可以方便我们的命令操作 快捷键 说明 Ctrl + A 光标移到行首 Ctrl + E 光标移到行尾 Ctrl + K 删除从光标开始到行尾的字符 Ctrl + U 删除从光标开始到行首的字符 Ctrl + R 搜索匹配的历史命令 Ctrl + P或上箭头 搜索之前的历史命令 Ctrl + N或下箭头 搜索之后的历史命令 Ctrl + L 清屏 总而言之，IPython为我们提供了很多强大的功能，本篇文章只是简要概述了下IPython的强大之处，还有很多有待大家自己去探索，尤其值得一提的是IPython还提供了notebook功能，也非常强大，由于篇幅所限，大家可以自己在网上搜索相关文章来参考。","categories":[],"tags":[]},{"title":"Python二进制表示和位操作","slug":"python_binary","date":"2016-12-16T10:18:18.000Z","updated":"2018-08-12T10:13:22.978Z","comments":true,"path":"2016/12/16/python_binary/","link":"","permalink":"http://yoursite.com/2016/12/16/python_binary/","excerpt":"","text":"我们都知道在计算机中所有的信息最终都是以二进制的0和1来表示，而有些算法是通过操作bit位来进行运算的，这就需要我们了解Python中如何去表示二进制，又如何是进行位运算的。 &lt;!--more--&gt; 二进制的表示 首先在Python中可以通过以&quot;0b&quot;或者&quot;-0b&quot;开头的字符串来表示二进制，如下所示 1234print 0b101 # 输出5print 0b10 # 输出2print 0b111 # 输出7print -0b101 # 输出-5 由此可知我们用二进制表示的数字在打印之后会变成我们更为熟悉的十进制数，更容易被人理解。 当我们需要看十进制数字的二进制表示时，可以使用bin函数 1bin(5) # 输出0b101 二进制的位操作 首先一点需要明确的是所有的运算（包括位操作）在计算机内部都是通过补码形式来进行运算的，关于补码可以参考文章原码，反码和补码，计算机内部运算示意图如下： 在Python中提供了如下二进制的位操作： 123456&gt;&gt; #右移&lt;&lt; #左移| #位或 &amp; #位与^ #位异或~ #非 位运算法则： 下面我们分别来看下： 左移 12340b11 &lt;&lt; 2 #输出为12, 即0b11005 &lt;&lt; 2 #输出为20, 即0b10100-2 &lt;&lt; 2 #输出为-85 &lt;&lt; 64 #输出为92233720368547758080L 以0b11为例，0b11的补码就是0b11，所以左移就是将所有的0和1的位置进行左移，移位之后将空位补0。 负数的左移相对来说就比较复杂，以-2 &lt;&lt; 2为例，-2的原码是10000000000000000000000000000010（32位系统），其补码为11111111111111111111111111111110，左移之后变为11111111111111111111111111111000，再转化为原码即10000000000000000000000000001000，也就是-8，也就是-2*(2**2)=-8 左移超过32位或者64位（根据系统的不同）自动转化为long类型。 左移操作相当于乘以2**n，以5 &lt;&lt; 3为例，相当于5*(2**3),结果为40。 右移 1230b11 &gt;&gt; 1 #输出为1, 即0b15 &gt;&gt; 1 #输出为2，即0b10-8 &gt;&gt; 3 #输出为-1 在Python中如果符号位为0，则右移后高位补0，如果符号位为1，则高位补1； 同样需要先转化为补码再进行计算，以-8 &gt;&gt; 3为例，-8的原码为10...01000,相应的补码为11...11000,右移后变为1...1,相应的原码为10...01,即-1。 右移操作相当于除以2**n，8 &gt;&gt; 3相当于8/(2**3)=1 或 120b110 | 0b101 #输出7,即0b111-0b001 | 0b101 #输出-1 同样是转化为补码后再进行或运算, 只要有一位有1就为1。 所以或运算常常用于mask技术中的打开开关，即针对某一位把其置为1 比如将某个数字的第三位置为1，我们可以将mask设置为0b100，然后再或运算 12mask = 0b1000b110000 | mask #turn on bit 3 与 10b110 &amp; 0b011 #输出2，即0b010 与运算常常用于mask技术的关闭开关，即针对某一位把其置为0 12mask = 0b100b111111 &amp; mask #turn off bit 2 异或 120b111 ^ 0b111 #输出00b100 ^ 0b111 #输出3 异或常用于将所有的位反转 10b1010 ^ 0b1111 #输出5，即0b0101 非 12~0b101 #输出2，即0b010~-3 #输出2 非运算就是把0变1，1变0，唯一需要注意的是取非时符号位也会变换，比如-3，原码是10...011,补码是11...101,取非后变为00...010,由于符号位为0，所以对应的原码即为其本身，即2。 二进制工具 bitarray 关于bit有一个很有用的Packag叫做bitarray，其中bitarray对象可以帮助我们存储0，1值或者Boolean值，并像list一样进行操作。 123456789101112from bitarray import bitarray#初始化一个有10个bit位的数组，初始值为0a = bitarray(10)#可以像操作list一样操作bitarray对象a[1:8] = 1#bitarray还提供了一些特殊的方法，如：all()#当bitarray中所有的元素都为1时，all()返回为Trueif a.all(): print &quot;all bits are True.&quot; 关于bitarrary的说明详见Github上的bitarray项目 位运算的应用 常见的应用如判断奇偶数 X &amp; 0x1，变换符号位 ~X + 1，数字交换等，详细可以看参考链接中的文章 下面笔者想就实际项目中的一个例子来说明位操作的应用。 下表是一个TS Package header的说明（TS流是流媒体行业常用的传输格式），我们看到为了减少不必要的浪费，包头在定义域的时候都是按位进行定义的，那么我们如果想要取相应的域的值，也就需要使用位操作了。 Packet Header（包头）信息说明 序号 名称 bit数 说明 1 sync_byte 8bits 同步字节 2 transport_error_indicator 1bit 错误指示信息（1：该包至少有1bits传输错误） 3 payload_unit_start_indicator 1bit 负载单元开始标志（packet不满188字节时需填充） 4 transport_priority 1bit 传输优先级标志（1：优先级高） 5 PID 13bits Packet ID号码，唯一的号码对应不同的包 6 transport_scrambling_control 2bits 加密标志（00：未加密；其他表示已加密） 7 adaptation_field_control 2bits 附加区域控制 8 continuity_counter 4bits 包递增计数器 我们以取PID值为例，当我们获取到包头的字节串之后，我们需要如下几个步骤： 需要取到第2个字节，然后忽略第二个字节的高三位（从表中可以看出高三位为其它信息与PID无关）； 将第二个字节的后5位数字左移8位，这样将其移到高位； 移位后与第3个字节的数值相加得到PID的值。 要实现第一步，首先就要用到位操作中常用的mask技术，即通过将对应位为0的数值进行&amp;操作 10b10110111 &amp; 0b00011111 #将高位的3位进行mask关闭操作，使得其值被去除 要实现第二步，就需要用到左移操作，左移操作之后与第三个字节的数值相加就是实际的PID值 完整代码实现如下： 1234def get_package_pid(package): if package is None: raise Exception(&quot;get_package_pid param package is None.&quot;) return ((ord(package[1]) &amp; 0x1f) &lt;&lt; 8) + ord(package[2]) 注: 1, ord()将byte串转化为对应的数字从而进行位运算； 2, 0x1f是十六进制表示，转化为二进制就是0b00011111. 参考链接： https://segmentfault.com/a/1190000003789802 https://www.cnblogs.com/zhangziqiu/archive/2011/03/30/ComputerCode.html https://github.com/wnduan/codecademy-py/blob/master/Bitwise-Operators.md http://developer.51cto.com/art/200808/83641_all.htm","categories":[],"tags":[]},{"title":"使用Flask实现用户登陆认证的详细过程","slug":"flask_login_detail","date":"2016-12-06T03:57:18.000Z","updated":"2018-08-12T10:13:44.781Z","comments":true,"path":"2016/12/06/flask_login_detail/","link":"","permalink":"http://yoursite.com/2016/12/06/flask_login_detail/","excerpt":"","text":"本文旨在详细讲解用户登录的原理和过程，并通过Flask框架来完整实现整个登录过程 &lt;!--more--&gt; 用户认证的原理 在了解使用Flask来实现用户认证之前，我们首先要明白用户认证的原理。假设现在我们要自己去实现用户认证，需要做哪些事情呢？ 首先，用户要能够输入用户名和密码，所以需要网页和表单，用以实现用户输入和提交的过程。 用户提交了用户名和密码，我们就需要比对用户名，密码是否正确，而要想比对，首先我们的系统中就要有存储用户名，密码的地方，大多数后台系统会通过数据库来存储，但是实际上我们也可以简单的存储到文件当中。(为简明起见，本文将用户信息存储到json文件当中) 登录之后，我们需要维持用户登录状态，以便用户在访问特定网页的时候来判断用户是否已经登录，以及是否有权限访问改网页。这就需要有维护一个会话来保存用户的登录状态和用户信息。 从第三步我们也可以看出，如果我们的网页需要权限保护，那么当请求到来的时候，我们就首先要检查用户的信息，比如是否已经登录，是否有权限等，如果检查通过，那么在response的时候就会将相应网页回复给请求的用户，但是如果检查不通过，那么就需要返回错误信息。 在第二步，我们知道要将用户名和密码存储起来，但是如果只是简单的用明文存储用户名和密码，很容易被“有心人”盗取，从而造成用户信息泄露，那么我们实际上应当将用户信息尤其是密码做加密处理之后再存储比较安全。 用户登出 通过Flask以及相应的插件来实现登录过程 接下来讲述如何通过Flask框架以及相应的插件来实现整个登录过程，需要用到的插件如下： flask-wtf wtf werkzeug flask_login 使用flask-wtf和wtf来实现表单功能 flask-wtf对wtf做了一些封装，不过有些东西还是要直接用wtf，比如StringField等。flask-wtf和wtf主要是用于建立html中的元素和Python中的类的对应关系，通过在Python代码中操作对应的类，对象等从而控制html中的元素。我们需要在python代码中使用flask-wtf和wtf来定义前端页面的表单（实际是定义一个表单类），再将对应的表单对象作为render_template函数的参数，传递给相应的template，之后Jinja模板引擎会将相应的template渲染成html文本，再作为http response返回给用户。 定义表单类示例代码： 1234567891011# forms.pyfrom flask_wtf import FlaskFormfrom wtforms import StringField, BooleanField, PasswordFieldfrom wtforms.validators import DataRequired# 定义的表单都需要继承自FlaskFormclass LoginForm(FlaskForm): # 域初始化时，第一个参数是设置label属性的 username = StringField(&apos;User Name&apos;, validators=[DataRequired()]) password = PasswordField(&apos;Password&apos;, validators=[DataRequired()]) remember_me = BooleanField(&apos;remember me&apos;, default=False) 在wtf当中，每个域代表就是html中的元素，比如StringField代表的是&lt;input type=&quot;text&quot;&gt;元素，当然wtf的域还定义了一些特定功能，比如validators，可以通过validators来对这个域的数据做检查，详细请参考wtf教程。 对应的html模板可能如下login.html： 123456789101112131415161718192021222324&#123;% extends &quot;layout.html&quot; %&#125;&lt;html&gt; &lt;head&gt; &lt;title&gt;Login Page&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form action=&quot;&#123;&#123; url_for(&quot;login&quot;) &#125;&#125;&quot; method=&quot;POST&quot;&gt; &lt;p&gt; User Name:&lt;br&gt; &lt;input type=&quot;text&quot; name=&quot;username&quot; /&gt;&lt;br&gt; &lt;/p&gt; &lt;p&gt; Password:&lt;/br&gt; &lt;input type=&quot;password&quot; name=&quot;password&quot; /&gt;&lt;br&gt; &lt;/p&gt; &lt;p&gt; &lt;input type=&quot;checkbox&quot; name=&quot;remember_me&quot;/&gt;Remember Me &lt;/p&gt; &#123;&#123; form.csrf_token &#125;&#125; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt;&#123;% 这里\\&#123;\\&#123; form.csrf_token \\&#125;\\&#125;也可以使用&#123;&#123; form.hidden_tag() &#125;&#125;来替换i%&#125; 同时我们也可以使用form去定义模板，跟直接用html标签去定义效果是相同的，Jinja模板引擎会将对象、属性转化为对应的html标签， 相对应的template，如下login.html： 123456789101112131415161718192021&lt;!-- 模板的语法应当符合Jinja语法 --&gt;&lt;!-- extend from base layout --&gt;&#123;% extends &quot;base.html&quot; %&#125;&#123;% block content %&#125; &lt;h1&gt;Sign In&lt;/h1&gt; &lt;form action=&quot;&#123;&#123; url_for(&quot;login&quot;) &#125;&#125;&quot; method=&quot;post&quot; name=&quot;login&quot;&gt; &#123;&#123; form.csrf_token &#125;&#125; &lt;p&gt; &#123;&#123; form.username.label &#125;&#125;&lt;br&gt; &#123;&#123; form.username(size=80) &#125;&#125;&lt;br&gt; &lt;/p&gt; &lt;p&gt; &#123;&#123; form.password.label &#125;&#125;&lt;br&gt; &lt;!-- 我们可以传递input标签的属性，这里传递的是size属性 --&gt; &#123;&#123; form.password(size=80) &#125;&#125;&lt;br&gt; &lt;/p&gt; &lt;p&gt;&#123;&#123; form.remember_me &#125;&#125; Remember Me&lt;/p&gt; &lt;p&gt;&lt;input type=&quot;submit&quot; value=&quot;Sign In&quot;&gt;&lt;/p&gt; &lt;/form&gt;&#123;% endblock %&#125; 现在我们需要在view中定义相应的路由，并将相应的登录界面展示给用户。 简单起见，将view的相关路由定义放在主程序当中 12345# app.py@app.route(&apos;/login&apos;)def login(): form = LoginForm() return render_template(&apos;login.html&apos;, title=&quot;Sign In&quot;, form=form) 这里简单起见，当用户请求'/login'路由时，直接返回login.html网页，注意这里的html网页是经过Jinja模板引擎将相应的模板转换后的html网页。 至此，如果我们把以上代码整合到flask当中，就应该能够看到相应的登录界面了，那么当用户提交之后，我们应当怎样存储呢？这里我们暂时先不用数据库这样复杂的工具存储，先简单地存为文件。接下来就看下如何去存储。 加密和存储 我们可以首先定义一个User类，用于处理与用户相关的操作，包括存储和验证等。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091# models.pyfrom werkzeug.security import generate_password_hashfrom werkzeug.security import check_password_hashfrom flask_login import UserMixinimport jsonimport uuid# define profile.json constant, the file is used to# save user name and password_hashPROFILE_FILE = &quot;profiles.json&quot;class User(UserMixin): def __init__(self, username): self.username = username self.id = self.get_id() @property def password(self): raise AttributeError(&apos;password is not a readable attribute&apos;) @password.setter def password(self, password): &quot;&quot;&quot;save user name, id and password hash to json file&quot;&quot;&quot; self.password_hash = generate_password_hash(password) with open(PROFILE_FILE, &apos;w+&apos;) as f: try: profiles = json.load(f) except ValueError: profiles = &#123;&#125; profiles[self.username] = [self.password_hash, self.id] f.write(json.dumps(profiles)) def verify_password(self, password): password_hash = self.get_password_hash() if password_hash is None: return False return check_password_hash(self.password_hash, password) def get_password_hash(self): &quot;&quot;&quot;try to get password hash from file. :return password_hash: if the there is corresponding user in the file, return password hash. None: if there is no corresponding user, return None. &quot;&quot;&quot; try: with open(PROFILE_FILE) as f: user_profiles = json.load(f) user_info = user_profiles.get(self.username, None) if user_info is not None: return user_info[0] except IOError: return None except ValueError: return None return None def get_id(self): &quot;&quot;&quot;get user id from profile file, if not exist, it will generate a uuid for the user. &quot;&quot;&quot; if self.username is not None: try: with open(PROFILE_FILE) as f: user_profiles = json.load(f) if self.username in user_profiles: return user_profiles[self.username][1] except IOError: pass except ValueError: pass return unicode(uuid.uuid4()) @staticmethod def get(user_id): &quot;&quot;&quot;try to return user_id corresponding User object. This method is used by load_user callback function &quot;&quot;&quot; if not user_id: return None try: with open(PROFILE_FILE) as f: user_profiles = json.load(f) for user_name, profile in user_profiles.iteritems(): if profile[1] == user_id: return User(user_name) except: return None return None User类需要继承flask-login中的UserMixin类，用于实现相应的用户会话管理。 这里我们是直接存储用户信息到一个json文件&quot;profiles.json&quot; 我们并不直接存储密码，而是存储加密后的hash值，在这里我们使用了werkzeug.security包中的generate_password_hash函数来进行加密，由于此函数默认使用了sha1算法，并添加了长度为8的盐值，所以还是相当安全的。一般用途的话也就够用了。 验证password的时候，我们需要使用werkzeug.security包中的check_password_hash函数来验证密码 get_id是UserMixin类中就有的method，在这我们需要overwrite这个method。在json文件中没有对应的user id时，可以使用uuid.uuid4()生成一个用户唯一id 至此，我们就实现了第二步和第五步，接下来要看第三步，如何去维护一个session 维护用户session 先看下代码，这里把相应代码也放入到app.py当中 123456789101112131415161718192021222324252627282930313233343536373839from forms import LoginFormfrom flask_wtf.csrf import CsrfProtectfrom model import Userfrom flask_login import login_user, login_requiredfrom flask_login import LoginManager, current_userfrom flask_login import logout_userapp = Flask(__name__)app.secret_key = os.urandom(24)# use login manager to manage sessionlogin_manager = LoginManager()login_manager.session_protection = &apos;strong&apos;login_manager.login_view = &apos;login&apos;login_manager.init_app(app=app)# 这个callback函数用于reload User object，根据session中存储的user id@login_manager.user_loaderdef load_user(user_id): return User.get(user_id)# csrf protectioncsrf = CsrfProtect()csrf.init_app(app)@app.route(&apos;/login&apos;)def login(): form = LoginForm() if form.validate_on_submit(): user_name = request.form.get(&apos;username&apos;, None) password = request.form.get(&apos;password&apos;, None) remember_me = request.form.get(&apos;remember_me&apos;, False) user = User(user_name) if user.verify_password(password): login_user(user, remember=remember_me) return redirect(request.args.get(&apos;next&apos;) or url_for(&apos;main&apos;)) return render_template(&apos;login.html&apos;, title=&quot;Sign In&quot;, form=form) 维护用户的会话，关键就在这个LoginManager对象。 必须实现这个load_user callback函数，用以reload user object 当密码验证通过后，使用login_user()函数来登录用户，这时用户在会话中的状态就是登录状态了 受保护网页 保护特定网页，只需要对特定路由加一个装饰器就可以，如下 12345678910# app.py# ...@app.route(&apos;/&apos;)@app.route(&apos;/main&apos;)@login_requireddef main(): return render_template( &apos;main.html&apos;, username=current_user.username)# ... current_user保存的就是当前用户的信息，实质上是一个User对象，所以我们直接调用其属性, 例如这里我们要给模板传一个username的参数，就可以直接用current_user.username 使用@login_required来标识改路由需要登录用户，非登录用户会被重定向到'/login'路由(这个就是由login_manager.login_view = 'login' 语句来指定的) 用户登出 123456789# app.py# ...@app.route(&apos;/logout&apos;)@login_requireddef logout(): logout_user() return redirect(url_for(&apos;login&apos;))# ... 至此，我们就实现了一个完整的登陆和登出的过程。 另外我们可能还需要其它辅助的功能，诸如发送确认邮件，密码重置，权限分级管理等，这些功能都可以通过flask及其插件来完成，这个大家可以自己探索下啦！","categories":[],"tags":[]}]}